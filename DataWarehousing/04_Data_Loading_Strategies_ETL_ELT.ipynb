{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e5ec2853",
   "metadata": {},
   "source": [
    "# Data Warehousing - Part 4: Data Loading Strategies (ETL & ELT)\n",
    "\n",
    "## 1. The Data Flow: Upstream to Downstream\n",
    "\n",
    "In the Data Warehousing ecosystem, we constantly move data between two types of systems:\n",
    "\n",
    "1.  **Upstream (Source):** The operational systems creating data (OLTP, E-commerce sites, CRM).\n",
    "2.  **Downstream (Target):** The analytical systems receiving data (Data Warehouse, Data Marts).\n",
    "\n",
    "### The ETL Process\n",
    "To move data, we use a process called **ETL**:\n",
    "*   **E**xtract: Read data from the Source (e.g., MySQL).\n",
    "*   **T**ransform: Clean, format, and map the data.\n",
    "*   **L**oad: Write data to the Target (e.g., Data Warehouse).\n",
    "\n",
    "### Scenario: The \"Country Code\" Problem\n",
    "As discussed in the lecture, user input in Source systems can be messy. Let's simulate a scenario where users type their country manually.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "\n",
    "# 1. UPSTREAM (Source System - Raw Data)\n",
    "# Users have entered 'India' in various formats.\n",
    "source_data = {\n",
    "    'UserID': [101, 102, 103, 104],\n",
    "    'Name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'User_Input_Country': ['IN', 'IND', 'INDIA', 'AUS'],\n",
    "    'Signup_Date': ['2023-01-01', '2023-01-02', '2023-01-03', '2023-01-04']\n",
    "}\n",
    "\n",
    "df_source = pd.DataFrame(source_data)\n",
    "\n",
    "print(\"--- Upstream (Source) Data ---\")\n",
    "display(df_source)\n",
    "```\n",
    "\n",
    "```python\n",
    "# 2. TRANSFORMATION ( The 'T' in ETL)\n",
    "# We map these variations to a standard ISO code or Name.\n",
    "\n",
    "def transform_country(country_raw):\n",
    "    c = country_raw.upper().strip()\n",
    "    if c in ['IN', 'IND', 'INDIA']:\n",
    "        return 'INDIA'\n",
    "    elif c in ['AUS', 'AUSTRALIA']:\n",
    "        return 'AUSTRALIA'\n",
    "    return 'UNKNOWN'\n",
    "\n",
    "# Apply transformation\n",
    "df_transformed = df_source.copy()\n",
    "df_transformed['Standardized_Country'] = df_transformed['User_Input_Country'].apply(transform_country)\n",
    "\n",
    "# Drop the messy column (optional, depending on requirements)\n",
    "df_transformed = df_transformed.drop(columns=['User_Input_Country'])\n",
    "\n",
    "print(\"--- Transformed Data ---\")\n",
    "display(df_transformed)\n",
    "\n",
    "# 3. LOAD (Downstream)\n",
    "# This clean dataframe is what gets written to the Data Warehouse table.\n",
    "df_warehouse = df_transformed.copy()\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f48b11bd",
   "metadata": {},
   "source": [
    "## 2. ETL vs. ELT\n",
    "\n",
    "While the goal is the same, the **order** of operations changes based on performance requirements.\n",
    "\n",
    "### A. ETL (Extract -> Transform -> Load)\n",
    "*   **Process:** Data is extracted, transformed in a separate processing engine (like an ETL tool or Python script), and *then* loaded into the Warehouse.\n",
    "*   **Use Case:** When the transformation logic is complex or the Data Warehouse compute is expensive.\n",
    "\n",
    "### B. ELT (Extract -> Load -> Transform)\n",
    "*   **Process:** Data is extracted and loaded *immediately* into a temporary/staging table in the Warehouse. The transformation happens **inside the database** (using SQL) before moving to the final table.\n",
    "*   **Advantage:** Leverages the processing power of modern cloud Data Warehouses (Snowflake, Redshift, BigQuery) to do the heavy lifting.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fadedd7",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "## 3. Data Loading Strategies\n",
    "\n",
    "How do we actually move the data? Do we move everything every day? There are two main strategies.\n",
    "\n",
    "### Strategy A: Full Load (Truncate Load)\n",
    "*   **Definition:** Every time the job runs, we delete (**Truncate**) everything in the target table and reload all data from the source.\n",
    "*   **Use Case:** Small reference tables (e.g., a list of States, Categories) or initial data migration.\n",
    "*   **Pros:** Simple to implement. No need to track changes.\n",
    "*   **Cons:** Very slow and expensive for large datasets (e.g., Transaction tables with millions of rows).\n",
    "\n",
    "```python\n",
    "# Simulation of Full Load\n",
    "\n",
    "# Current state of Data Warehouse\n",
    "dw_table = pd.DataFrame({'ID': [1, 2], 'Data': ['A', 'B']})\n",
    "print(\"DW Before Load:\", dw_table.values)\n",
    "\n",
    "# New Data arrives in Source (Everything from scratch)\n",
    "source_data_new = pd.DataFrame({'ID': [1, 2, 3], 'Data': ['A', 'B', 'C']})\n",
    "\n",
    "# --- FULL LOAD PROCESS ---\n",
    "# 1. Truncate (Empty) the target\n",
    "dw_table = pd.DataFrame() \n",
    "\n",
    "# 2. Load everything\n",
    "dw_table = source_data_new.copy()\n",
    "\n",
    "print(\"DW After Full Load:\", dw_table.values)\n",
    "```\n",
    "\n",
    "### Strategy B: Incremental Load (Delta Load)\n",
    "*   **Definition:** We only load the data that has **changed** (inserted or updated) since the last load.\n",
    "*   **Mechanism:** We rely on **Audit Columns** (Watermark columns) like `created_at` or `updated_at`.\n",
    "*   **Use Case:** Large Fact tables (Sales, Orders, Logs).\n",
    "\n",
    "#### Simulation: Identifying the Delta\n",
    "Imagine our Warehouse was last updated on **Jan 10th**. We only want records created *after* Jan 10th.\n",
    "\n",
    "```python\n",
    "# 1. State of Data Warehouse (Last updated Jan 10)\n",
    "warehouse_data = {\n",
    "    'Order_ID': [1, 2],\n",
    "    'Amount': [100, 200],\n",
    "    'Updated_At': [\n",
    "        datetime(2023, 1, 9), \n",
    "        datetime(2023, 1, 10)\n",
    "    ]\n",
    "}\n",
    "df_dw = pd.DataFrame(warehouse_data)\n",
    "\n",
    "# Identify the \"Watermark\" (Max Date in DW)\n",
    "watermark_date = df_dw['Updated_At'].max()\n",
    "print(f\"--- Watermark Date (Last Load): {watermark_date} ---\")\n",
    "\n",
    "# 2. Source System (Contains old AND new data)\n",
    "source_data = {\n",
    "    'Order_ID': [1, 2, 3, 4], # 3 and 4 are new\n",
    "    'Amount': [100, 200, 150, 300],\n",
    "    'Updated_At': [\n",
    "        datetime(2023, 1, 9), \n",
    "        datetime(2023, 1, 10),\n",
    "        datetime(2023, 1, 11), # New\n",
    "        datetime(2023, 1, 12)  # New\n",
    "    ]\n",
    "}\n",
    "df_source_oltp = pd.DataFrame(source_data)\n",
    "\n",
    "# 3. INCREMENTAL LOGIC\n",
    "# Filter Source where date > watermark\n",
    "delta_records = df_source_oltp[df_source_oltp['Updated_At'] > watermark_date]\n",
    "\n",
    "print(\"\\n--- Delta Records (Incremental Feed) ---\")\n",
    "display(delta_records)\n",
    "\n",
    "# 4. Append to Warehouse\n",
    "df_dw_updated = pd.concat([df_dw, delta_records], ignore_index=True)\n",
    "print(\"\\n--- Final Warehouse State ---\")\n",
    "display(df_dw_updated)\n",
    "```\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "630dacb4",
   "metadata": {},
   "source": [
    "## 4. Summary Table\n",
    "\n",
    "| Feature | Full Load | Incremental Load |\n",
    "| :--- | :--- | :--- |\n",
    "| **Method** | Erase and Replace | Find changes & Append/Merge |\n",
    "| **Volume** | Loads 100% of data | Loads ~1-5% of data (Deltas) |\n",
    "| **Performance** | Slower as data grows | Fast, scalable |\n",
    "| **Complexity** | Low | High (Need to handle updates/duplicates) |\n",
    "| **Best For** | Master Data, Lookup Tables | Transactional Data, Big Data |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef8dee96",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "Now that we know how to move data, we need to understand how to structure it. In the next session, we will discuss the building blocks of Data Modeling: **Measures and Attributes**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
