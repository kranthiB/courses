{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0af732a6",
   "metadata": {},
   "source": [
    "# Why Databricks & What is Data Lakehouse?\n",
    "\n",
    "**Objective:** Understand the evolution of data platforms, the challenges with traditional architectures, and how the **Data Lakehouse** architecture (powered by Delta Lake) solves them.\n",
    "\n",
    "## 1. Challenges in Traditional Data Platforms\n",
    "Before Databricks, organizations typically faced three major challenges:\n",
    "\n",
    "### A. Too Many Tool Stacks (Fragmentation)\n",
    "*   **The Problem:** You needed different tools for different tasks:\n",
    "    *   Data Warehousing (e.g., Snowflake, Redshift)\n",
    "    *   ETL Jobs (e.g., Informatica, Talend)\n",
    "    *   Data Lake Storage (e.g., S3, ADLS)\n",
    "    *   Orchestration (e.g., Airflow)\n",
    "    *   AI/ML (e.g., SageMaker, Azure ML)\n",
    "    *   BI/Reporting (e.g., Tableau, PowerBI)\n",
    "*   **The Consequence:** Integrating these tools is difficult. If governance doesn't span across all tools, you get security risks and data leaks.\n",
    "\n",
    "### B. Proprietary Solutions (Vendor Lock-in)\n",
    "*   **The Problem:** Traditional Data Warehouses store data in proprietary formats.\n",
    "*   **The Consequence:** You cannot access your data without using that specific vendor's engine. If you want to move your data or use a different compute engine, you are \"locked in.\"\n",
    "*   **Databricks Solution:** Databricks uses **Open Source** formats (Parquet, CSV, Avro, ORC) stored in your own cloud account.\n",
    "\n",
    "### C. Data Silos (Duplication)\n",
    "*   **The Problem:** Companies maintained two separate systems:\n",
    "    1.  **Data Lake:** For unstructured data, AI, and ML.\n",
    "    2.  **Data Warehouse:** For structured data and BI reporting.\n",
    "*   **The Consequence:** Data had to be copied/moved from Lake to Warehouse. This resulted in:\n",
    "    *   Data Duplication.\n",
    "    *   Stale data (latency in moving data).\n",
    "    *   Different owners for different copies of data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "092f60c5",
   "metadata": {},
   "source": [
    "## 2. The Solution: Data Lakehouse\n",
    "\n",
    "The **Data Lakehouse** is a unified architecture that combines the best elements of a Data Lake and a Data Warehouse.\n",
    "\n",
    "$$ \\text{Data Lakehouse} = \\text{Data Lake (Low Cost, Flexible)} + \\text{Data Warehouse (Performance, ACID, Governance)} $$\n",
    "\n",
    "### How it works:\n",
    "1.  **Storage:** Data remains in your Cloud Storage (ADLS/S3/GCP) in open formats (like Parquet).\n",
    "2.  **Engine:** A transactional layer is added on top. In Databricks, this is **Delta Lake**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408a8fad",
   "metadata": {},
   "source": [
    "## 3. Delta Lake: The Core Engine\n",
    "**Delta Lake** is an open-source storage layer that brings reliability to Data Lakes. It provides:\n",
    "\n",
    "*   **ACID Transactions:** Ensures data integrity (Atomic, Consistent, Isolated, Durable).\n",
    "*   **Versioning:** Keeps history of data changes.\n",
    "*   **Time Travel:** Ability to query older versions of data (restore data).\n",
    "*   **Audit History:** Tracks who changed what and when.\n",
    "*   **DML Operations:** Supports `UPDATE`, `DELETE`, and `MERGE` on your data lake files.\n",
    "\n",
    "*By using Delta Lake, a single copy of data in the Data Lake can serve both AI/ML use cases and BI/Dashboarding use cases.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5690330",
   "metadata": {},
   "source": [
    "## 4. The Databricks Architecture Stack\n",
    "\n",
    "The Databricks \"Data Intelligence Platform\" is structured in layers:\n",
    "\n",
    "| Layer | Component | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Top** | **Personas** | **Data Engineers:** Jobs, Workflows, Notebooks<br>**Data Analysts:** SQL Warehouses, Dashboards<br>**Data Scientists:** MLflow, Model Serving |\n",
    "| **Intelligence** | **Data Intelligence Engine** | Powered by Generative AI (IQ) to understand your data semantics. |\n",
    "| **Governance** | **Unity Catalog** | Unified governance for files, tables, and ML models. |\n",
    "| **Lakehouse** | **Delta Lake** | The engine providing ACID transactions on open formats. |\n",
    "| **Storage** | **Data Lake** | Your raw data (Open formats: Parquet, CSV) |\n",
    "| **Bottom** | **Cloud Provider** | Azure / AWS / GCP |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a49591d",
   "metadata": {},
   "source": [
    "## 5. Summary: Data Intelligence Platform\n",
    "\n",
    "Databricks defines itself as a Data Intelligence Platform.\n",
    "\n",
    "$$ \\text{Data Intelligence Platform} = \\text{Data Lakehouse} + \\text{Generative AI} $$\n",
    "\n",
    "It allows enterprises to get insights from their data using natural language, backed by a unified governance model (Unity Catalog) and an open storage format (Delta Lake)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c16cde",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Practical check:\n",
    "# Even though this is a theory module, let's verify we are running on a cluster\n",
    "# that supports Delta Lake (which is standard in Databricks).\n",
    "\n",
    "try:\n",
    "    from delta.tables import *\n",
    "    print(\"Delta Lake libraries are available.\")\n",
    "    print(\"You are ready to build a Lakehouse!\")\n",
    "except ImportError:\n",
    "    print(\"Delta Lake libraries not found. Please ensure you are running this on a Databricks Runtime.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
