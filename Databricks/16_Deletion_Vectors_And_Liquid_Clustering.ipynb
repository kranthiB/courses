{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9235c2c9",
   "metadata": {},
   "source": [
    "# Deletion Vectors and Liquid Clustering in Delta Lake\n",
    "\n",
    "**Objective:** In this notebook, we will explore two powerful optimization features in Delta Lake that significantly improve performance and reduce I/O overhead.\n",
    "\n",
    "1.  **Deletion Vectors:** An optimization that speeds up `DELETE` and `UPDATE` operations by avoiding full file rewrites.\n",
    "2.  **Liquid Clustering:** A dynamic data layout technique that replaces Hive-style partitioning and Z-Ordering for better query performance.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   For Deletion Vectors: Delta Lake 2.3.0+ or Databricks Runtime (DBR) 12.2 LTS+.\n",
    "*   For Liquid Clustering: Delta Lake 3.1.0+ or DBR 13.3 LTS+."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23734a9a",
   "metadata": {},
   "source": [
    "## 1. Setup Data\n",
    "We will use the standard Databricks dataset `online_retail` for this demonstration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81e50ff3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup: Define file path\n",
    "file_path = \"dbfs:/databricks-datasets/online_retail/data-001/data.csv\"\n",
    "\n",
    "# Verify we can read the data\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "display(df.limit(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35542adf",
   "metadata": {},
   "source": [
    "## 2. Deletion Vectors\n",
    "\n",
    "### What are Deletion Vectors?\n",
    "In standard Delta Lake (CoW - Copy on Write), if you modify a single row in a Parquet file, the entire file must be rewritten. This is I/O intensive.\n",
    "\n",
    "**With Deletion Vectors (MoR - Merge on Read):**\n",
    "Instead of rewriting the file, Delta marks the row as \"deleted\" using a position vector (a small auxiliary file). This makes deletions extremely fast. The physical removal of data happens later during an `OPTIMIZE` job.\n",
    "\n",
    "### Demonstration\n",
    "Let's create a table and explicitly **disable** Deletion Vectors first to see the default behavior (File Rewrite)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c264738",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a schema for our demo\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS dev_bronze\")\n",
    "\n",
    "# Drop table if exists to start fresh\n",
    "spark.sql(\"DROP TABLE IF EXISTS dev_bronze.sales_no_dv\")\n",
    "\n",
    "# Create a Delta Table (CTAS)\n",
    "# We explicitly set deletion vectors to false to demonstrate the \"Old\" way\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE dev_bronze.sales_no_dv\n",
    "    TBLPROPERTIES ('delta.enableDeletionVectors' = false)\n",
    "    AS SELECT * FROM read_files('{file_path}', format => 'csv', header => true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d62a746",
   "metadata": {},
   "source": [
    "### Scenario A: Deleting WITHOUT Deletion Vectors\n",
    "We will delete specific records and check the transaction history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19e657a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Delete specific Invoice Numbers\n",
    "spark.sql(\"DELETE FROM dev_bronze.sales_no_dv WHERE InvoiceNo = '540644'\")\n",
    "\n",
    "# Check History\n",
    "display(spark.sql(\"DESCRIBE HISTORY dev_bronze.sales_no_dv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ff53287",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "If you look at the `operationMetrics` in the history for the DELETE operation above:\n",
    "*   `numFilesAdded`: 1 (or more)\n",
    "*   `numFilesRemoved`: 1 (or more)\n",
    "*   `numDeletionVectorsAdded`: 0\n",
    "\n",
    "This proves that the file was **rewritten**. The old file was removed, and a new file (minus the deleted row) was added."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf79fb0b",
   "metadata": {},
   "source": [
    "### Scenario B: Deleting WITH Deletion Vectors\n",
    "Now, let's enable the feature on the existing table and perform another delete."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "690d70bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Enable Deletion Vectors\n",
    "spark.sql(\"ALTER TABLE dev_bronze.sales_no_dv SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\")\n",
    "\n",
    "# Check properties to confirm\n",
    "display(spark.sql(\"DESCRIBE EXTENDED dev_bronze.sales_no_dv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd590687",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Perform another delete operation on a different Invoice\n",
    "spark.sql(\"DELETE FROM dev_bronze.sales_no_dv WHERE InvoiceNo = '536365'\")\n",
    "\n",
    "# Check History again\n",
    "display(spark.sql(\"DESCRIBE HISTORY dev_bronze.sales_no_dv\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe11084a",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "Look at the latest DELETE operation in history:\n",
    "*   `numFilesAdded`: 0\n",
    "*   `numFilesRemoved`: 0\n",
    "*   `numDeletionVectorsAdded`: 1 (or more)\n",
    "\n",
    "**Conclusion:** The data files were NOT rewritten. Only a small vector file was added to mark the rows as deleted. This is significantly faster for large datasets.\n",
    "\n",
    "### Compacting Deletion Vectors\n",
    "To physically remove the deleted data and merge the vectors, we run `OPTIMIZE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4427850d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"OPTIMIZE dev_bronze.sales_no_dv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4445fddf",
   "metadata": {},
   "source": [
    "---\n",
    "## 3. Liquid Clustering\n",
    "\n",
    "### What is Liquid Clustering?\n",
    "Liquid clustering replaces traditional Hive Partitioning and Z-Ordering. It is a flexible data layout that prevents the \"Small Files\" problem often caused by over-partitioning.\n",
    "\n",
    "**Benefits:**\n",
    "*   Handles high cardinality columns (many unique values).\n",
    "*   Handles skewed data.\n",
    "*   Adapts to changing access patterns without rewriting the whole table.\n",
    "*   Solves the \"Too many partitions\" or \"Too few partitions\" dilemma.\n",
    "\n",
    "### How to use?\n",
    "You use the `CLUSTER BY` clause when creating the table.\n",
    "*Note: Clustering columns must be within the first 32 columns of the table.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5535026f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Drop if exists\n",
    "spark.sql(\"DROP TABLE IF EXISTS dev_bronze.sales_liquid\")\n",
    "\n",
    "# Create Table with Liquid Clustering using CLUSTER BY\n",
    "spark.sql(f\"\"\"\n",
    "    CREATE TABLE dev_bronze.sales_liquid\n",
    "    CLUSTER BY (InvoiceNo)\n",
    "    AS SELECT * FROM read_files('{file_path}', format => 'csv', header => true)\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64edd4cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the Clustering configuration\n",
    "display(spark.sql(\"DESCRIBE EXTENDED dev_bronze.sales_liquid\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3431e",
   "metadata": {},
   "source": [
    "**Observation:**\n",
    "In the `DESCRIBE EXTENDED` output, look for the row **Clustering Columns**. It should list `['InvoiceNo']`.\n",
    "\n",
    "Now, when you query this table filtering by `InvoiceNo`, Databricks will use liquid clustering to skip irrelevant data, making queries much faster.\n",
    "\n",
    "### Modifying Clustering\n",
    "If you want to change the clustering columns or remove them (go back to standard unclustered), you can use `ALTER TABLE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86cd10b5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example: Querying filtered data utilizes the clustering layout\n",
    "result = spark.sql(\"SELECT * FROM dev_bronze.sales_liquid WHERE InvoiceNo = '536365'\")\n",
    "display(result)\n",
    "\n",
    "# To remove clustering (Optional command, just for reference)\n",
    "# spark.sql(\"ALTER TABLE dev_bronze.sales_liquid CLUSTER BY NONE\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a33ec43",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1.  **Deletion Vectors** allow for soft-deletes, preventing expensive file rewrites during DML operations.\n",
    "2.  **Liquid Clustering** simplifies data layout management, solving issues related to rigid partitioning schemes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
