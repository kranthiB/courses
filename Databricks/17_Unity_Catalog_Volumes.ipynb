{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e268b1d",
   "metadata": {},
   "source": [
    "# Unity Catalog: Managed vs. External Volumes\n",
    "\n",
    "**Objective:**\n",
    "In this notebook, we will explore **Databricks Volumes**, a feature of Unity Catalog that allows you to govern, manage, and access unstructured, semi-structured, and structured data files (non-tabular data).\n",
    "\n",
    "We will cover:\n",
    "1.  **Managed Volumes:** Storage governed entirely by Unity Catalog in the managed storage location of the schema.\n",
    "2.  **External Volumes:** Storage governed by Unity Catalog but located in an external cloud storage path (ADLS/S3) that you control.\n",
    "3.  **File Operations:** Copying, reading, and querying files within volumes.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   A Unity Catalog-enabled Databricks workspace.\n",
    "*   `CREATE VOLUME` privileges on the schema.\n",
    "*   For External Volumes: An `EXTERNAL LOCATION` already configured (covered in a previous notebook)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304dba25",
   "metadata": {},
   "source": [
    "## 1. Setup Environment\n",
    "We will use the catalog `dev` and schema `bronze` that we created in previous sessions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d69f90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Select catalog and schema\n",
    "spark.sql(\"USE CATALOG dev\")\n",
    "spark.sql(\"USE SCHEMA bronze\")\n",
    "\n",
    "print(\"Current context: dev.bronze\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d3fae96",
   "metadata": {},
   "source": [
    "## 2. Managed Volumes\n",
    "\n",
    "### What is a Managed Volume?\n",
    "A managed volume is a storage volume created within the default managed storage location of the containing schema. You do not need to specify a path when creating it. Databricks manages the lifecycle of the data files.\n",
    "\n",
    "*   **Dropping a managed volume deletes both the metadata and the actual files.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f16f59f1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a Managed Volume\n",
    "# Syntax: CREATE VOLUME volume_name\n",
    "spark.sql(\"\"\"\n",
    "    CREATE VOLUME IF NOT EXISTS dev.bronze.managed_vol\n",
    "    COMMENT 'This is a managed volume for storing raw files'\n",
    "\"\"\")\n",
    "\n",
    "print(\"Managed Volume 'managed_vol' created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f98bc329",
   "metadata": {},
   "source": [
    "### Inspecting the Volume\n",
    "Let's look at the metadata of the volume we just created. Note the `storage_location` points to the Unity Catalog managed path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49acc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "display(spark.sql(\"DESCRIBE VOLUME dev.bronze.managed_vol\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d78c4c9",
   "metadata": {},
   "source": [
    "### Working with Files in Managed Volumes\n",
    "We can use standard filesystem commands (via `%fs` or `dbutils`) to interact with volumes. The path format is:\n",
    "`/Volumes/<catalog>/<schema>/<volume_name>/<path_to_file>`\n",
    "\n",
    "Let's download a sample CSV file and copy it into our managed volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129caf4c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create a sub-directory inside the volume\n",
    "import os\n",
    "volume_path = \"/Volumes/dev/bronze/managed_vol\"\n",
    "files_dir = f\"{volume_path}/files\"\n",
    "\n",
    "dbutils.fs.mkdirs(files_dir)\n",
    "\n",
    "# 2. Download sample data to local driver and move it to the Volume\n",
    "# Downloading the employee CSV used in previous demos\n",
    "url = \"https://media.githubusercontent.com/media/subhamkharwal/pyspark-zero-to-hero/refs/heads/master/datasets/emp.csv\"\n",
    "local_path = \"/tmp/emp.csv\"\n",
    "\n",
    "# Use shell command to download\n",
    "os.system(f\"wget {url} -O {local_path}\")\n",
    "\n",
    "# Copy from local driver to Volume\n",
    "dbutils.fs.cp(f\"file:{local_path}\", f\"{files_dir}/emp.csv\")\n",
    "\n",
    "# 3. List files in the volume\n",
    "display(dbutils.fs.ls(files_dir))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb112a93",
   "metadata": {},
   "source": [
    "### Querying Data from Volumes\n",
    "We can query files directly from volumes without creating a table first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3335ef1f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read CSV directly from volume path\n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(f\"{files_dir}/emp.csv\")\n",
    "display(df)\n",
    "\n",
    "# Or using SQL\n",
    "display(spark.sql(f\"SELECT * FROM csv.`{files_dir}/emp.csv`\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc55f05b",
   "metadata": {},
   "source": [
    "## 3. External Volumes\n",
    "\n",
    "### What is an External Volume?\n",
    "An external volume is a storage volume created against a specific external location (ADLS/S3/GCP) that you manage. It allows you to bring existing data under Unity Catalog governance without moving it.\n",
    "\n",
    "*   **Dropping an external volume removes the metadata from Unity Catalog but LEAVES the files in your cloud storage untouched.**\n",
    "\n",
    "*Prerequisite Check: Ensure you have an External Location created (e.g., `ext_volume_loc`). If not, refer to the notebook on External Locations.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4343ba91",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define your External Location path (Update this with your actual external location path from previous lessons)\n",
    "# Example: \"abfss://data@<storage_account>.dfs.core.windows.net/ext_vol_path\"\n",
    "external_location_path = \"YOUR_EXTERNAL_LOCATION_PATH_HERE/ext_vol_folder\"\n",
    "\n",
    "# Create External Volume\n",
    "# We must specify the LOCATION\n",
    "try:\n",
    "    spark.sql(f\"\"\"\n",
    "        CREATE EXTERNAL VOLUME IF NOT EXISTS dev.bronze.external_vol\n",
    "        LOCATION '{external_location_path}'\n",
    "        COMMENT 'External volume pointing to my ADLS container'\n",
    "    \"\"\")\n",
    "    print(\"External Volume 'external_vol' created.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error creating volume. Ensure External Location exists. Details: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3553e0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Describe to verify it is EXTERNAL\n",
    "display(spark.sql(\"DESCRIBE VOLUME dev.bronze.external_vol\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fe56a26",
   "metadata": {},
   "source": [
    "### File Operations in External Volume\n",
    "Just like managed volumes, we use the unified path: `/Volumes/dev/bronze/external_vol/...`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97fc1372",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "ext_vol_path = \"/Volumes/dev/bronze/external_vol\"\n",
    "\n",
    "# Copy the same emp.csv to external volume\n",
    "dbutils.fs.cp(f\"file:{local_path}\", f\"{ext_vol_path}/emp_ext.csv\")\n",
    "\n",
    "# Verify file exists\n",
    "display(dbutils.fs.ls(ext_vol_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af280c5",
   "metadata": {},
   "source": [
    "## 4. Cleanup & Behavior Test (DROP)\n",
    "\n",
    "### Dropping Managed Volume\n",
    "This will delete the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbd95e07",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP VOLUME IF EXISTS dev.bronze.managed_vol\")\n",
    "\n",
    "# Verify path is gone (This should throw an error or return empty)\n",
    "try:\n",
    "    dbutils.fs.ls(volume_path)\n",
    "except Exception as e:\n",
    "    print(\"Success: Managed volume path is no longer accessible.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd7350ec",
   "metadata": {},
   "source": [
    "### Dropping External Volume\n",
    "This will only remove the catalog object. The file `emp_ext.csv` will remain in your ADLS/S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd77496b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.sql(\"DROP VOLUME IF EXISTS dev.bronze.external_vol\")\n",
    "print(\"External volume dropped from Unity Catalog metadata.\")\n",
    "\n",
    "# Note: You cannot access it via /Volumes/... anymore, but you could access it via the direct cloud path \n",
    "# if you have direct credentials or mount points set up."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c493f7b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "*   **Volumes** unify file access under `/Volumes/catalog/schema/volume`.\n",
    "*   **Managed Volumes:** Easy setup, UC manages lifecycle, `DROP` deletes data.\n",
    "*   **External Volumes:** Connects to existing cloud storage, `DROP` keeps data safe (metadata only delete)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
