{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6bf574ae",
   "metadata": {},
   "source": [
    "# Parameterizing Notebooks & Building Workflows\n",
    "\n",
    "**Objective:**\n",
    "Learn how to make your Databricks notebooks dynamic by accepting parameters and how to build a simple workflow by calling one notebook from another.\n",
    "\n",
    "**Scenario:**\n",
    "We will create a Parent-Child notebook architecture.\n",
    "1.  **Child Notebook:** Accepts a 'Department' parameter, reads employee data, filters it by that department, and writes it to a specific Delta table.\n",
    "2.  **Parent Notebook:** Calls the Child Notebook multiple times with different department names, effectively running a data pipeline.\n",
    "\n",
    "**Prerequisites:**\n",
    "*   A running Databricks cluster.\n",
    "*   The `emp.csv` file available in your Volume or DBFS (covered in previous videos)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58cf6073",
   "metadata": {},
   "source": [
    "## Part 1: Creating the Child Notebook Logic\n",
    "*Note: In a real scenario, this code would be in a separate notebook file. For demonstration in this single file, we will simulate the logic blocks.*\n",
    "\n",
    "Imagine the code below is inside a notebook named `Child_Notebook_Write_Data`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de3298ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- LOGIC FOR CHILD NOTEBOOK STARTS HERE ---\n",
    "\n",
    "# 1. Create a Text Widget to accept input\n",
    "# Syntax: dbutils.widgets.text(name, defaultValue, label)\n",
    "dbutils.widgets.text(\"dept_param\", \"Sales\", \"Department Name\")\n",
    "\n",
    "# 2. Get the value from the widget\n",
    "current_dept = dbutils.widgets.get(\"dept_param\")\n",
    "print(f\"Processing data for department: {current_dept}\")\n",
    "\n",
    "# 3. Read Data (Using the path from previous lessons)\n",
    "# Update path if your file is elsewhere\n",
    "file_path = \"/Volumes/dev/bronze/managed_vol/files/emp.csv\" \n",
    "df = spark.read.format(\"csv\").option(\"header\", \"true\").load(file_path)\n",
    "\n",
    "# 4. Filter Data based on the parameter\n",
    "# Using PySpark filter with upper() to handle case sensitivity\n",
    "from pyspark.sql.functions import col, upper\n",
    "df_filtered = df.filter(upper(col(\"department\")) == upper(current_dept))\n",
    "\n",
    "# 5. Write Data to a Delta Table specific to the department\n",
    "# Table name will be dynamic: dev.bronze.dept_<dept_name>\n",
    "table_name = f\"dev.bronze.dept_{current_dept.lower()}\"\n",
    "\n",
    "# Check if we have data before writing\n",
    "record_count = df_filtered.count()\n",
    "\n",
    "if record_count > 0:\n",
    "    df_filtered.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "    print(f\"Success: Wrote {record_count} records to table {table_name}\")\n",
    "    # 6. Exit with status\n",
    "    dbutils.notebook.exit(f\"Success: {record_count} records processed for {current_dept}\")\n",
    "else:\n",
    "    print(f\"No data found for department: {current_dept}\")\n",
    "    dbutils.notebook.exit(f\"No Data: 0 records for {current_dept}\")\n",
    "\n",
    "# --- LOGIC FOR CHILD NOTEBOOK ENDS HERE ---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cdefd76",
   "metadata": {},
   "source": [
    "## Part 2: The Parent Notebook Logic (Orchestrator)\n",
    "\n",
    "Now, we will use `dbutils.notebook.run` to call the child notebook. \n",
    "\n",
    "**Important:** Since we cannot actually call *this* current notebook recursively easily in a demo without side effects, imagine the code above is saved as `Child_Notebook`.\n",
    "\n",
    "Below is the command you would use in a separate **Parent Notebook** to trigger the logic above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5e74e16",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Syntax: dbutils.notebook.run(path, timeout_seconds, arguments_dictionary)\n",
    "\n",
    "# Example 1: Run for 'Sales'\n",
    "# Note: This command will fail if 'Child_Notebook' does not actually exist in your workspace.\n",
    "# You need to create the separate notebook file with the code from Cell 3 first.\n",
    "\n",
    "# status_sales = dbutils.notebook.run(\"./Child_Notebook\", 600, {\"dept_param\": \"Sales\"})\n",
    "# print(status_sales)\n",
    "\n",
    "# Example 2: Run for 'IT'\n",
    "# status_it = dbutils.notebook.run(\"./Child_Notebook\", 600, {\"dept_param\": \"IT\"})\n",
    "# print(status_it)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d711570",
   "metadata": {},
   "source": [
    "## 3. Scheduling a Notebook Job\n",
    "You can turn this parameterized notebook into a scheduled job directly from the UI.\n",
    "\n",
    "1.  Click **Schedule** at the top right of the notebook.\n",
    "2.  Click **Add Schedule**.\n",
    "3.  Define the Job Name (e.g., \"Daily Sales Data Load\").\n",
    "4.  Set the Schedule (e.g., Every Day at 8:00 AM).\n",
    "5.  **Parameters:** You can add the key `dept_param` and value `Sales` here. This allows you to reuse the same code for different scheduled jobs (e.g., one job for Sales, one for Marketing)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94ad047",
   "metadata": {},
   "source": [
    "## Summary\n",
    "1.  **Widgets** allow notebooks to accept inputs at runtime (`dbutils.widgets`).\n",
    "2.  **`dbutils.notebook.run`** allows you to chain notebooks, passing parameters and receiving exit values.\n",
    "3.  **`dbutils.notebook.exit`** returns a value to the parent notebook and stops execution of the child notebook.\n",
    "4.  This pattern is fundamental for building modular **ETL Pipelines** in Databricks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
