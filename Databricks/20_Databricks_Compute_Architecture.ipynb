{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8998a969",
   "metadata": {},
   "source": [
    "# Databricks Compute (Clusters) Architecture\n",
    "\n",
    "**Objective:**\n",
    "Understand the computing resources that power Databricks workloads. We will cover types of compute, access modes, runtimes, and how to manage costs.\n",
    "\n",
    "**Agenda:**\n",
    "1.  **What is Compute?** (Drivers & Workers)\n",
    "2.  **All-Purpose vs. Job Compute**\n",
    "3.  **Access Modes** (Single User vs. Shared)\n",
    "4.  **Databricks Runtime (DBR) & Photon**\n",
    "5.  **Cost Management** (Auto-scaling & Termination)\n",
    "6.  **Cluster Permissions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22b54837",
   "metadata": {},
   "source": [
    "## 1. Types of Compute\n",
    "\n",
    "### A. All-Purpose Compute\n",
    "*   **Use Case:** Interactive analysis (Notebooks, Databricks SQL queries).\n",
    "*   **Lifecycle:** You manually create it, start it, and terminate it.\n",
    "*   **Cost:** Generally more expensive than Job compute because it is designed for interactivity.\n",
    "*   **Creation:** Created via the \"Compute\" tab in the UI.\n",
    "\n",
    "### B. Job Compute\n",
    "*   **Use Case:** Running automated jobs (Workflows).\n",
    "*   **Lifecycle:** Created automatically when a job starts and terminates immediately when the job finishes.\n",
    "*   **Cost:** Cheaper than All-Purpose compute.\n",
    "*   **Creation:** Defined within the \"Workflows\" tab when setting up a job task.\n",
    "\n",
    "> **Best Practice:** Always use **Job Compute** for production scheduled workloads to save costs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90ec69f7",
   "metadata": {},
   "source": [
    "## 2. Access Modes (Crucial for Unity Catalog)\n",
    "\n",
    "When creating a cluster, you must select an Access Mode. This dictates how users interact with the cluster and data security.\n",
    "\n",
    "| Access Mode | Unity Catalog Support | Description |\n",
    "| :--- | :--- | :--- |\n",
    "| **Single User** | ✅ Yes | Designed for a single developer. Supports Python, SQL, Scala, R. The user acts as the owner. |\n",
    "| **Shared** | ✅ Yes | Designed for multiple users to share the same cluster securely. Good for concurrent SQL/Python usage. Limits some filesystem operations for security. |\n",
    "| **No Isolation Shared** | ❌ No | Legacy mode. Does not support Unity Catalog governance features. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ad4824c",
   "metadata": {},
   "source": [
    "## 3. Databricks Runtime (DBR) & Photon\n",
    "\n",
    "### Databricks Runtime (DBR)\n",
    "A pre-configured environment that includes Spark, Python, Scala, and standard libraries.\n",
    "*   **LTS (Long Term Support):** Recommended for stability (e.g., 13.3 LTS, 14.3 LTS).\n",
    "*   **ML Variants:** Includes machine learning libraries (TensorFlow, PyTorch) pre-installed.\n",
    "\n",
    "### Photon Acceleration\n",
    "*   A native vectorized execution engine written in C++ (instead of JVM).\n",
    "*   **Benefits:** significantly speeds up SQL and DataFrame operations.\n",
    "*   **Cost:** Slightly higher DBU rate, but often results in lower *total* cost because jobs finish much faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772bc66f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Practical: Check if the current cluster has Photon enabled\n",
    "# We can check the spark configuration for photon\n",
    "is_photon = spark.conf.get(\"spark.databricks.clusterUsageTags.sparkVersion\", \"Unknown\")\n",
    "\n",
    "print(f\"Cluster Runtime Version Tag: {is_photon}\")\n",
    "\n",
    "# Check specifically for photon engine\n",
    "try:\n",
    "    photon_status = spark.conf.get(\"spark.databricks.service.client.photonEnabled\")\n",
    "    print(f\"Photon Enabled: {photon_status}\")\n",
    "except:\n",
    "    print(\"Could not determine exact Photon status from config, likely standard runtime.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ef7c66",
   "metadata": {},
   "source": [
    "## 4. Cost Management\n",
    "\n",
    "### Auto-scaling\n",
    "*   Allows the cluster to resize based on workload.\n",
    "*   **Min Workers:** The cluster will never go below this.\n",
    "*   **Max Workers:** The cluster will never exceed this (budget control).\n",
    "\n",
    "### Auto-termination\n",
    "*   **Critical Feature:** Automatically stops the cluster after `X` minutes of inactivity.\n",
    "*   **Recommendation:** Set this to 15-30 minutes for development clusters to prevent accidental overnight costs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f736d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Practical: Inspect Current Cluster Configuration\n",
    "# We can access cluster tags to see how this specific cluster is configured\n",
    "# Note: This requires access to the context tags\n",
    "\n",
    "try:\n",
    "    tags = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterName\")\n",
    "    print(f\"Current Cluster Name: {tags}\")\n",
    "    \n",
    "    workers = spark.conf.get(\"spark.databricks.clusterUsageTags.clusterWorkers\")\n",
    "    print(f\"Target Workers: {workers}\")\n",
    "except Exception as e:\n",
    "    print(\"Unable to fetch cluster tags.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cfaeb58",
   "metadata": {},
   "source": [
    "## 5. Permissions\n",
    "Who can do what with a cluster?\n",
    "\n",
    "1.  **Can Manage:** Can modify configuration, start, stop, and delete the cluster.\n",
    "2.  **Can Restart:** Can only restart the cluster (useful for clearing cache/state).\n",
    "3.  **Can Attach To:** Can connect notebooks to the cluster to run code, but cannot start/stop it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccfd1cc2",
   "metadata": {},
   "source": [
    "## 6. Monitoring (Spark UI & Logs)\n",
    "*   **Spark UI:** Used for debugging DAGs, tasks, and stages performance.\n",
    "*   **Driver Logs:** `Standard Output` (print statements) and `Log4j` logs are found here. Essential for debugging errors.\n",
    "*   **Metrics:** CPU and Memory utilization charts (Ganglia metrics)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
