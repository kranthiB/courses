{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c71de0a1",
   "metadata": {},
   "source": [
    "# Data Ingestion: COPY INTO\n",
    "## Databricks Zero to Hero\n",
    "\n",
    "**Objective:** Learn how to ingest data into the Data Lakehouse using the SQL `COPY INTO` command. We will explore its capabilities regarding idempotency, schema evolution, and simple data transformations during ingestion.\n",
    "\n",
    "### What is COPY INTO?\n",
    "`COPY INTO` is a SQL command used to load data from a file location into a Delta table.\n",
    "*   **Idempotent:** It tracks which files have already been loaded. If you run it again, it will **not** duplicate data. It provides \"Exactly-Once\" processing.\n",
    "*   **Retriable:** If a job fails, you can safely rerun it.\n",
    "*   **Versatile:** Supports CSV, JSON, Avro, Parquet, ORC, etc.\n",
    "*   **Simple:** Best suited for batch loads containing thousands of files. (For millions of files, **Auto Loader** is preferred)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cb734f9",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "We need a location to stage our raw files. We will create a **Managed Volume** and copy some sample data from Databricks datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e19701c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Create a Managed Volume for landing data\n",
    "%sql\n",
    "CREATE VOLUME IF NOT EXISTS dev.bronze.landing\n",
    "COMMENT \"This is Landing Managed Volume for Raw Data\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c721cfad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Prepare the input directory and sample data\n",
    "# We will copy two CSV files from Databricks datasets to our volume\n",
    "\n",
    "# Create input folder\n",
    "dbutils.fs.mkdirs(\"/Volumes/dev/bronze/landing/input\")\n",
    "\n",
    "# Copy sample Invoice data (Day 1 and Day 2)\n",
    "source_path = \"dbfs:/databricks-datasets/definitive-guide/data/retail-data/by-day/\"\n",
    "target_path = \"/Volumes/dev/bronze/landing/input/\"\n",
    "\n",
    "# Copying 2 specific files\n",
    "dbutils.fs.cp(f\"{source_path}2010-12-01.csv\", f\"{target_path}2010-12-01.csv\")\n",
    "dbutils.fs.cp(f\"{source_path}2010-12-02.csv\", f\"{target_path}2010-12-02.csv\")\n",
    "\n",
    "# Verify files\n",
    "display(dbutils.fs.ls(target_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbe89684",
   "metadata": {},
   "source": [
    "## 2. Basic Ingestion (Schema Evolution)\n",
    "We will create a **placeholder table** (an empty Delta table with no schema defined). We will rely on `COPY INTO` to infer the schema from the source files and evolve the target table structure automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "677b176c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a placeholder table without schema\n",
    "DROP TABLE IF EXISTS dev.bronze.invoice_cp;\n",
    "\n",
    "CREATE TABLE dev.bronze.invoice_cp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79641f6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Load data using COPY INTO\n",
    "-- We use 'mergeSchema' to allow the table to adapt to the CSV columns\n",
    "\n",
    "COPY INTO dev.bronze.invoice_cp\n",
    "FROM '/Volumes/dev/bronze/landing/input'\n",
    "FILEFORMAT = CSV\n",
    "PATTERN = '*.csv'\n",
    "FORMAT_OPTIONS (\n",
    "    'header' = 'true',\n",
    "    'mergeSchema' = 'true' -- Infer schema from file\n",
    ")\n",
    "COPY_OPTIONS (\n",
    "    'mergeSchema' = 'true' -- Update target table schema\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e694d8d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify the loaded data\n",
    "SELECT * FROM dev.bronze.invoice_cp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb129248",
   "metadata": {},
   "source": [
    "## 3. Idempotency Check (Exactly Once)\n",
    "One of the biggest features of `COPY INTO` is that it remembers state. If we run the exact same command again, it should **skip** the files it has already processed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b6b04e4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Rerunning the exact same command\n",
    "COPY INTO dev.bronze.invoice_cp\n",
    "FROM '/Volumes/dev/bronze/landing/input'\n",
    "FILEFORMAT = CSV\n",
    "PATTERN = '*.csv'\n",
    "FORMAT_OPTIONS ('header' = 'true', 'mergeSchema' = 'true')\n",
    "COPY_OPTIONS ('mergeSchema' = 'true');\n",
    "\n",
    "-- RESULT EXPECTATION: num_affected_rows should be 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b3967f",
   "metadata": {},
   "source": [
    "### How does it know?\n",
    "Databricks maintains an internal log within the Delta Log directory called `_copy_into_log`. This metadata tracks processed file names and their states."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3cb075d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check table metadata to find physical location\n",
    "DESCRIBE EXTENDED dev.bronze.invoice_cp;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6edfe13",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's inspect the underlying log (Python)\n",
    "# Replace <path_from_above_command> with the Location you see in the result above\n",
    "# Example: dbfs:/user/hive/warehouse/dev.db/bronze.db/invoice_cp\n",
    "\n",
    "# path = \"<your_table_location>/_delta_log/_copy_into_log\"\n",
    "# display(dbutils.fs.ls(path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3a9b5d",
   "metadata": {},
   "source": [
    "## 4. Transformations & Custom Schema\n",
    "Often, you don't want `SELECT *`. You might want to:\n",
    "1.  Load specific columns.\n",
    "2.  Change data types (e.g., String to Double).\n",
    "3.  Add metadata columns (e.g., Ingestion Timestamp).\n",
    "\n",
    "Let's create a target table with a specific schema and load data with transformations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95865d6d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create a table with defined schema\n",
    "DROP TABLE IF EXISTS dev.bronze.invoice_cp_alt;\n",
    "\n",
    "CREATE TABLE dev.bronze.invoice_cp_alt (\n",
    "    InvoiceNo STRING,\n",
    "    StockCode STRING,\n",
    "    Quantity DOUBLE,\n",
    "    _insert_date TIMESTAMP\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b75ebfba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- COPY INTO with Transformations using SELECT\n",
    "COPY INTO dev.bronze.invoice_cp_alt\n",
    "FROM (\n",
    "    SELECT \n",
    "        InvoiceNo, \n",
    "        StockCode, \n",
    "        cast(Quantity as DOUBLE) as Quantity, -- Casting type\n",
    "        current_timestamp() as _insert_date   -- Adding custom column\n",
    "    FROM '/Volumes/dev/bronze/landing/input'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "PATTERN = '*.csv'\n",
    "FORMAT_OPTIONS (\n",
    "    'header' = 'true',\n",
    "    'mergeSchema' = 'true'\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aebb2d8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify data (Should have 4 columns and correct types)\n",
    "SELECT * FROM dev.bronze.invoice_cp_alt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9939663f",
   "metadata": {},
   "source": [
    "## 5. Incremental Loading\n",
    "Let's verify that `COPY INTO` picks up **new** files automatically. We will add a 3rd file to the source folder and rerun the command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f071793",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Copy a new file (Day 3) to the source folder\n",
    "dbutils.fs.cp(\n",
    "    \"dbfs:/databricks-datasets/definitive-guide/data/retail-data/by-day/2010-12-03.csv\", \n",
    "    \"/Volumes/dev/bronze/landing/input/2010-12-03.csv\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16551cfd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Rerun the COPY INTO command\n",
    "-- It should ONLY load the records from 2010-12-03.csv\n",
    "COPY INTO dev.bronze.invoice_cp_alt\n",
    "FROM (\n",
    "    SELECT \n",
    "        InvoiceNo, \n",
    "        StockCode, \n",
    "        cast(Quantity as DOUBLE) as Quantity, \n",
    "        current_timestamp() as _insert_date\n",
    "    FROM '/Volumes/dev/bronze/landing/input'\n",
    ")\n",
    "FILEFORMAT = CSV\n",
    "PATTERN = '*.csv'\n",
    "FORMAT_OPTIONS ('header' = 'true', 'mergeSchema' = 'true');"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5443a43",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check record counts (Should increase incrementally)\n",
    "SELECT count(*) FROM dev.bronze.invoice_cp_alt;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed270e1a",
   "metadata": {},
   "source": [
    "## Summary\n",
    "*   **COPY INTO** is powerful for simple, batch-based data ingestion.\n",
    "*   It handles **Idempotency** automatically (no duplicates).\n",
    "*   It supports **Schema Inference** and **Evolution**.\n",
    "*   It allows basic **SQL Transformations** (Casting, renaming, adding columns) during load.\n",
    "\n",
    "### Limitation\n",
    "`COPY INTO` lists all files in the directory to detect new ones. If you have **millions of files**, this listing operation becomes slow. In such cases, Databricks recommends using **Auto Loader**, which we will cover in the next session."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
