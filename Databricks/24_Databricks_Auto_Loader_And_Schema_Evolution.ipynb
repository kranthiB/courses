{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdbaf80e",
   "metadata": {},
   "source": [
    "# Auto Loader in Databricks\n",
    "## Efficiently Ingest Data at Scale\n",
    "\n",
    "**Auto Loader** is a Databricks feature designed to incrementally and efficiently process new data files as they arrive in cloud storage. It replaces the need for complex file management logic or scheduling generic file listing jobs.\n",
    "\n",
    "### Key Features:\n",
    "1.  **Incremental Processing:** Only processes new files.\n",
    "2.  **Scalability:** Can handle millions of files per hour.\n",
    "3.  **Schema Evolution:** Automatically handles changing schemas (new columns, type changes).\n",
    "4.  **File Notification & Directory Listing:** Two modes to detect new files.\n",
    "\n",
    "### The `cloudFiles` Source\n",
    "Auto Loader is accessed via the Structured Streaming source called `cloudFiles`.\n",
    "\n",
    "```python\n",
    "spark.readStream.format(\"cloudFiles\") \\\n",
    "     .option(\"cloudFiles.format\", \"csv\") \\\n",
    "     .load(\"/path/to/files\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e227a34",
   "metadata": {},
   "source": [
    "## In this notebook, we will explore:\n",
    "\n",
    "> Setting up Auto Loader for CSV ingestion.\n",
    "\n",
    "> Using Schema Hints for data typing.\n",
    "\n",
    "> Incremental Loading (adding files one by one).\n",
    "\n",
    "> Schema Evolution Modes (addNewColumns, rescue, failOnNewColumns, none)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af71231f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup: Define Paths and Clean up previous runs\n",
    "# We will use a temporary location for this demo to simulate cloud storage\n",
    "\n",
    "import time\n",
    "\n",
    "base_path = \"/tmp/databricks_zero_to_hero/autoloader_demo\"\n",
    "input_path = f\"{base_path}/input\"\n",
    "checkpoint_path = f\"{base_path}/checkpoints\"\n",
    "schema_location = f\"{base_path}/schema_log\"\n",
    "table_name = \"autoloader_demo_table\"\n",
    "\n",
    "# Clean up\n",
    "dbutils.fs.rm(base_path, True)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "\n",
    "print(f\"Environment setup complete. \\nInput Path: {input_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f20e284",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to create dummy CSV data\n",
    "def create_file(file_id, date_str, data_content):\n",
    "    # Simulating a nested directory structure: Year/Month/Day\n",
    "    year, month, day = date_str.split(\"-\")\n",
    "    file_path = f\"{input_path}/{year}/{month}/{day}/file_{file_id}.csv\"\n",
    "    \n",
    "    dbutils.fs.put(file_path, data_content, True)\n",
    "    print(f\"Created file: {file_path}\")\n",
    "\n",
    "# 1. Create Initial Data (Day 1)\n",
    "# Schema: id, name, amount, date\n",
    "data_day_1 = \"\"\"id,name,amount,date\n",
    "1,Alice,100,2023-10-01\n",
    "2,Bob,200,2023-10-01\"\"\"\n",
    "\n",
    "create_file(1, \"2023-10-01\", data_day_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ce6f3e0",
   "metadata": {},
   "source": [
    "## 1. Basic Auto Loader Implementation\n",
    "\n",
    "We will configure Auto Loader to read CSV files.\n",
    "*   **`cloudFiles.format`**: The format of the source files (csv, json, parquet, etc.).\n",
    "*   **`cloudFiles.schemaLocation`**: Where Auto Loader stores the inferred schema state.\n",
    "*   **`pathGlobFilter`**: To select specific files (e.g., `*.csv`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a879d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the Auto Loader Stream Reader\n",
    "# Note: We are NOT providing a schema explicitly. Auto Loader infers it.\n",
    "\n",
    "df_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"cloudFiles.schemaLocation\", schema_location) # Crucial for schema evolution\n",
    "    .option(\"header\", \"true\")\n",
    "    \n",
    "    # Optional: Use hints to enforce specific types for columns if known\n",
    "    .option(\"cloudFiles.schemaHints\", \"id INT, amount DOUBLE\") \n",
    "    \n",
    "    .load(f\"{input_path}/*/*/*\") # Wildcards for nested folders\n",
    ")\n",
    "\n",
    "# Let's inspect the stream object\n",
    "print(\"Is streaming:\", df_stream.isStreaming)\n",
    "df_stream.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a567feda",
   "metadata": {},
   "source": [
    "### Writing to a Delta Table\n",
    "We will use `trigger(availableNow=True)`. This runs the stream as a **Batch** job. It processes all currently available files and then shuts down. This is very cost-effective for periodic ETL."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6bdcbd2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write the stream to a Delta Table\n",
    "query = (df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\") # Allow writing new columns to the Delta table\n",
    "    .outputMode(\"append\")\n",
    "    .table(table_name)\n",
    ")\n",
    "\n",
    "# Wait for the batch to finish\n",
    "query.awaitTermination()\n",
    "\n",
    "# Check the results\n",
    "display(spark.sql(f\"SELECT * FROM {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e935d650",
   "metadata": {},
   "source": [
    "## 2. Incremental Processing\n",
    "Auto Loader tracks processed files in RocksDB (located in the checkpoint directory). If we add a new file, it should pick up **only** the new file in the next run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79f2c5f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Add New Data (Day 2)\n",
    "data_day_2 = \"\"\"id,name,amount,date\n",
    "3,Charlie,150,2023-10-02\n",
    "4,David,300,2023-10-02\"\"\"\n",
    "\n",
    "create_file(2, \"2023-10-02\", data_day_2)\n",
    "\n",
    "# Run the EXACT same stream code again\n",
    "# It uses the checkpoint to know what to read\n",
    "query = (df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\") \n",
    "    .trigger(availableNow=True) # Process only new data\n",
    "    .table(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Verify that we have 4 records now (2 from Day 1 + 2 from Day 2)\n",
    "count = spark.sql(f\"SELECT count(*) FROM {table_name}\").collect()[0][0]\n",
    "print(f\"Total records in table: {count}\")\n",
    "display(spark.sql(f\"SELECT * FROM {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c87476ce",
   "metadata": {},
   "source": [
    "## 3. Schema Evolution Modes\n",
    "\n",
    "What happens if a new file arrives with an **extra column**? Auto Loader has different modes controlled by `cloudFiles.schemaEvolutionMode`.\n",
    "\n",
    "| Mode | Description | behavior |\n",
    "| :--- | :--- | :--- |\n",
    "| **`addNewColumns`** | (Default) The stream updates the schema automatically. | New columns are added to the table. |\n",
    "| **`failOnNewColumns`** | The stream fails. | Useful if schema changes are strictly forbidden. |\n",
    "| **`rescue`** | Schema is never evolved. | New columns are packed into a `_rescued_data` column. Stream does not fail. |\n",
    "| **`none`** | Ignores new columns. | New columns are simply not read. |\n",
    "\n",
    "Let's test the default behavior: **`addNewColumns`**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1c4405",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Add Data with a NEW COLUMN 'region'\n",
    "# Only the schema evolution mode determines if this works. Default is 'addNewColumns'.\n",
    "\n",
    "data_schema_change = \"\"\"id,name,amount,date,region\n",
    "5,Eve,500,2023-10-03,US-East\n",
    "6,Frank,600,2023-10-03,EU-West\"\"\"\n",
    "\n",
    "create_file(3, \"2023-10-03\", data_schema_change)\n",
    "\n",
    "# Run the stream again\n",
    "query = (df_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", checkpoint_path)\n",
    "    .option(\"mergeSchema\", \"true\") # Delta table needs this to accept the new column from Auto Loader\n",
    "    .trigger(availableNow=True)\n",
    "    .table(table_name)\n",
    ")\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "# Check result - 'region' column should exist now, filled with nulls for older records\n",
    "display(spark.sql(f\"SELECT * FROM {table_name}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1759b6ba",
   "metadata": {},
   "source": [
    "## 4. Rescue Mode (`rescue`)\n",
    "\n",
    "Sometimes you don't want to break the schema, but you don't want to lose data either. `rescue` mode puts unexpected data into a special column `_rescued_data`.\n",
    "\n",
    "*Note: To demo this cleanly, we will start a fresh stream with a new checkpoint, as schema evolution behavior is bound to the checkpoint state.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f8db21",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup for Rescue Demo\n",
    "rescue_checkpoint = f\"{base_path}/checkpoint_rescue\"\n",
    "rescue_schema_loc = f\"{base_path}/schema_rescue\"\n",
    "rescue_table = \"autoloader_rescue_demo\"\n",
    "\n",
    "# Create a stream with 'rescue' mode\n",
    "df_rescue_stream = (spark.readStream\n",
    "    .format(\"cloudFiles\")\n",
    "    .option(\"cloudFiles.format\", \"csv\")\n",
    "    .option(\"header\", \"true\")\n",
    "    # Set mode to RESCUE\n",
    "    .option(\"cloudFiles.schemaEvolutionMode\", \"rescue\") \n",
    "    .option(\"cloudFiles.schemaLocation\", rescue_schema_loc)\n",
    "    .load(f\"{input_path}/*/*/*\")\n",
    ")\n",
    "\n",
    "# Write stream\n",
    "query_rescue = (df_rescue_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", rescue_checkpoint)\n",
    "    .option(\"mergeSchema\", \"true\")\n",
    "    .trigger(availableNow=True)\n",
    "    .table(rescue_table)\n",
    ")\n",
    "\n",
    "query_rescue.awaitTermination()\n",
    "\n",
    "print(\"Checking table for _rescued_data column...\")\n",
    "display(spark.sql(f\"SELECT * FROM {rescue_table}\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86b18efd",
   "metadata": {},
   "source": [
    "### Observation on Rescue Mode\n",
    "In the output above, because we started fresh, Auto Loader inferred the schema based on *all* files (including the one with `region`). \n",
    "\n",
    "However, if we now add a file with yet another column (e.g., `status`), and the inferred schema is already locked, `rescue` mode will put `status` into the `_rescued_data` JSON column instead of creating a new top-level column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e660760",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Add file with 'status' column\n",
    "data_rescue_test = \"\"\"id,name,amount,date,region,status\n",
    "7,Grace,700,2023-10-04,US-West,Active\"\"\"\n",
    "\n",
    "create_file(4, \"2023-10-04\", data_rescue_test)\n",
    "\n",
    "# Run Rescue Stream again\n",
    "(df_rescue_stream.writeStream\n",
    "    .format(\"delta\")\n",
    "    .option(\"checkpointLocation\", rescue_checkpoint)\n",
    "    .trigger(availableNow=True)\n",
    "    .table(rescue_table)\n",
    ").awaitTermination()\n",
    "\n",
    "# Review Results\n",
    "# You should see the 'status' data inside the '_rescued_data' column\n",
    "display(spark.sql(f\"SELECT * FROM {rescue_table} WHERE id = 7\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63eb25b7",
   "metadata": {},
   "source": [
    "## 5. File Detection Modes\n",
    "\n",
    "1.  **Directory Listing (Default):**\n",
    "    *   Lists files in the input directory to find new ones.\n",
    "    *   Scales well due to incremental listing, but can be slow for massive historical buckets.\n",
    "    \n",
    "2.  **File Notification:**\n",
    "    *   Uses cloud services (AWS SQS, Azure Event Grid, GCP Pub/Sub).\n",
    "    *   Storage sends a notification when a file lands. Auto Loader reads the queue.\n",
    "    *   **Pros:** Extremely performant for massive directories.\n",
    "    *   **Cons:** Requires elevated cloud permissions to set up the queues/subscriptions.\n",
    "\n",
    "**How to enable File Notification:**\n",
    "```python\n",
    ".option(\"cloudFiles.useNotifications\", \"true\")\n",
    "\n",
    "(This requires specific cloud setup and won't run in standard community edition without cloud config, so we are keeping it as a reference code block)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18853a6c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cleanup\n",
    "# Remove temp files and tables\n",
    "dbutils.fs.rm(base_path, True)\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {table_name}\")\n",
    "spark.sql(f\"DROP TABLE IF EXISTS {rescue_table}\")\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
