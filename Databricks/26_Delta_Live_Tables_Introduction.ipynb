{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "af6abd13",
   "metadata": {},
   "source": [
    "# Delta Live Tables (DLT) Introduction\n",
    "\n",
    "**Delta Live Tables (DLT)** is a declarative framework for building reliable, maintainable, and testable data processing pipelines. Instead of defining your data pipelines using a series of separate Apache Spark tasks, you define the *state* of the data (target tables) and the transformations required to reach that state. DLT handles the orchestration, cluster management, error handling, and data quality checks automatically.\n",
    "\n",
    "### Key Concepts\n",
    "1.  **Declarative Framework**: You tell DLT *what* you want (e.g., \"I want a table X derived from Y\"), and DLT figures out *how* to build and update it.\n",
    "2.  **Continuous vs. Triggered**: Pipelines can run continuously (low latency) or on a schedule (triggered).\n",
    "3.  **Development vs. Production**:\n",
    "    *   **Development**: Reuses clusters to speed up iteration/debugging.\n",
    "    *   **Production**: Restarts clusters for every run (if triggered) to ensure isolation and cost efficiency, retries on failure.\n",
    "\n",
    "### Dataset Types in DLT\n",
    "| Type | Description | Use Case |\n",
    "| :--- | :--- | :--- |\n",
    "| **Streaming Table** | Incremental data processing. Only reads new data from source. | Ingestion from Kafka, Auto Loader, or append-only sources. |\n",
    "| **Materialized View** | Recomputes the entire table from source based on query logic. | Aggregations, joins, complex transformations where state needs to be refreshed. |\n",
    "| **View** | Intermediate logic. Not physically stored. | Reusable transformation logic used by other tables in the pipeline. |\n",
    "\n",
    "---\n",
    "**Pre-requisites:**\n",
    "*   A Databricks Workspace with **Premium** Plan (DLT is a premium feature).\n",
    "*   Unity Catalog enabled (preferred) or Hive Metastore."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3bc7ac1",
   "metadata": {},
   "source": [
    "## 1. Environment Setup (Run this interactively)\n",
    "Before writing the DLT pipeline code, let's prepare some source data. We will use the `samples` catalog provided by Databricks and clone the `TPCH` dataset into a location we can use for our pipeline.\n",
    "\n",
    "*Note: Run this cell on a standard All-Purpose Cluster.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a702b73b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup: Create a schema and clone sample data for our DLT source\n",
    "# This mimics having raw data landing in your bronze layer\n",
    "\n",
    "# 1. Create a schema for our source data\n",
    "spark.sql(\"CREATE SCHEMA IF NOT EXISTS dev.etl_source\")\n",
    "\n",
    "# 2. Deep clone sample tables to our dev environment so we can treat them as raw data\n",
    "# We are using TPCH orders and customer tables\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dev.etl_source.orders_raw \n",
    "    DEEP CLONE samples.tpch.orders\n",
    "\"\"\")\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS dev.etl_source.customer_raw \n",
    "    DEEP CLONE samples.tpch.customer\n",
    "\"\"\")\n",
    "\n",
    "print(\"Source data setup complete in 'dev.etl_source'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c3c13e",
   "metadata": {},
   "source": [
    "## 2. The DLT Pipeline Code\n",
    "The code below represents the logic for a DLT pipeline. \n",
    "\n",
    "**Important:** \n",
    "1.  Do not run the cells below interactively using \"Shift+Enter\" on a standard cluster. It will likely fail or do nothing useful because the `dlt` module is specific to the Pipeline runtime.\n",
    "2.  In a real scenario, this code would exist in a notebook file that you select when creating a **Pipeline** in the Workflows tab.\n",
    "\n",
    "### The Pipeline Logic:\n",
    "1.  **`orders_bronze`**: A **Streaming Table** reading incrementally from the raw orders table.\n",
    "2.  **`customer_bronze`**: A **Materialized View** reading the full customer snapshot (batch).\n",
    "3.  **`joined_view`**: A temporary **View** joining orders and customers (filtering/cleaning).\n",
    "4.  **`joined_silver`**: A **Materialized View** storing the result of the join with added metadata.\n",
    "5.  **`orders_agg_gold`**: A **Materialized View** calculating aggregates (Gold Layer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "594cdadf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# BRONZE LAYER\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 1. Streaming Table for Orders\n",
    "# We use readStream to treat the source as a stream (incremental processing)\n",
    "@dlt.table(\n",
    "    name=\"orders_bronze\",\n",
    "    comment=\"Raw orders data ingested incrementally\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def orders_bronze():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(\"dev.etl_source.orders_raw\")\n",
    "    )\n",
    "\n",
    "# 2. Materialized View for Customers\n",
    "# We assume customer data is relatively small or updates slowly, so we treat it as batch\n",
    "@dlt.table(\n",
    "    name=\"customer_bronze\",\n",
    "    comment=\"Raw customer reference data\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def customer_bronze():\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .table(\"dev.etl_source.customer_raw\")\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SILVER LAYER (Transformations)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 3. Intermediate View (Join Logic)\n",
    "# Views are not materialized to storage; they are computed at runtime\n",
    "@dlt.view(\n",
    "    name=\"joined_view\",\n",
    "    comment=\"Intermediate logic to join orders with customers\"\n",
    ")\n",
    "def joined_view():\n",
    "    # Read from the DLT tables defined above using the \"LIVE\" keyword\n",
    "    df_orders = spark.read.table(\"LIVE.orders_bronze\")\n",
    "    df_customers = spark.read.table(\"LIVE.customer_bronze\")\n",
    "    \n",
    "    # Perform Left Join\n",
    "    # Note: 'o_' and 'c_' are prefixes in TPCH column names\n",
    "    return df_orders.join(\n",
    "        df_customers, \n",
    "        df_orders.o_custkey == df_customers.c_custkey, \n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "# 4. Materialized View (Silver Table)\n",
    "# This persists the joined data\n",
    "@dlt.table(\n",
    "    name=\"joined_silver\",\n",
    "    comment=\"Enriched orders with customer details\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def joined_silver():\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.joined_view\")\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GOLD LAYER (Aggregations)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "# 5. Aggregated Materialized View\n",
    "# Business level aggregations\n",
    "@dlt.table(\n",
    "    name=\"orders_by_segment_gold\",\n",
    "    comment=\"Aggregated order counts by market segment\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def orders_by_segment_gold():\n",
    "    df = spark.read.table(\"LIVE.joined_silver\")\n",
    "    \n",
    "    return (\n",
    "        df.groupBy(\"c_mktsegment\")\n",
    "        .agg(count(\"o_orderkey\").alias(\"total_orders\"))\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ef854c",
   "metadata": {},
   "source": [
    "## 3. How to Execute this Pipeline?\n",
    "\n",
    "Since this is DLT code, you must create a Pipeline in the Databricks UI:\n",
    "\n",
    "1.  **Navigate**: Go to `Workflows` -> `Delta Live Tables` on the left sidebar.\n",
    "2.  **Create Pipeline**: Click **Create Pipeline**.\n",
    "3.  **Configure**:\n",
    "    *   **Pipeline Name**: `01_DLT_Introduction`\n",
    "    *   **Product Edition**: `Core` (or Pro/Advanced if you want Data Quality features later).\n",
    "    *   **Pipeline Mode**: `Triggered` (Batch style) or `Continuous`. For this demo, use **Triggered**.\n",
    "    *   **Source Code**: Browse and select **THIS Notebook**.\n",
    "    *   **Destination**:\n",
    "        *   **Storage options**: Unity Catalog (Recommended) or Hive Metastore.\n",
    "        *   **Catalog**: `dev`\n",
    "        *   **Target Schema**: `etl_dlt_demo` (This is where tables like `orders_bronze` will physically be created).\n",
    "    *   **Compute**: Select a cluster policy or allow it to use the default `Standard_DS3_v2`.\n",
    "4.  **Run**: Click **Start**.\n",
    "\n",
    "### Debugging Tips\n",
    "*   **Development Mode**: When starting the pipeline, select \"Development\". This keeps the cluster running after failure or success, allowing you to iterate quickly without waiting for cluster spin-up every time.\n",
    "*   **Event Log**: If the pipeline fails (e.g., \"count not defined\"), check the Event Log at the bottom of the DLT UI screen. It provides stack traces similar to standard notebooks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53206369",
   "metadata": {},
   "source": [
    "## 4. Validating the Data\n",
    "Once the pipeline run finishes successfully, you can query the created tables in a standard SQL editor or a separate notebook.\n",
    "\n",
    "```sql\n",
    "-- Switch to the target catalog and schema you defined in the pipeline settings\n",
    "USE dev.etl_dlt_demo;\n",
    "\n",
    "-- Check the Bronze Streaming Table\n",
    "SELECT * FROM orders_bronze LIMIT 10;\n",
    "\n",
    "-- Check the Silver Enriched Table\n",
    "SELECT * FROM joined_silver LIMIT 10;\n",
    "\n",
    "-- Check the Gold Aggregated Data\n",
    "SELECT * FROM orders_by_segment_gold;"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
