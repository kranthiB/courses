{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3795a13f",
   "metadata": {},
   "source": [
    "# DLT: Incremental Processing & Schema Evolution\n",
    "\n",
    "In the previous session, we built a basic DLT pipeline. In this session, we will explore:\n",
    "1.  **Incremental Data Loading:** How Streaming Tables process only new data.\n",
    "2.  **Schema Evolution:** How to modify the pipeline logic (add columns, rename tables) and how DLT handles these changes automatically.\n",
    "3.  **DLT Internals:** Understanding where data is stored, checkpoints, and the hidden internal catalogs.\n",
    "4.  **Data Lineage:** Utilizing Unity Catalog to track data flow.\n",
    "\n",
    "---\n",
    "### Workflow for this Notebook\n",
    "1.  **Simulate Data:** We will insert new records into our source table.\n",
    "2.  **Update Pipeline:** We will modify the DLT code to add a new aggregation and rename a table.\n",
    "3.  **Analyze Behavior:** We will observe how DLT handles the new data and code changes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb23f827",
   "metadata": {},
   "source": [
    "## 1. Simulate Incremental Data (Run Interactively)\n",
    "\n",
    "Before running the pipeline again, let's insert **10,000 new records** into the raw source table (`orders_raw`). This will allow us to demonstrate how the **Streaming Table** (`orders_bronze`) picks up *only* the new data, while **Materialized Views** recompute their state.\n",
    "\n",
    "*Note: Run this cell on a standard All-Purpose Cluster, NOT within the DLT pipeline execution.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e24e89d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run this on a standard cluster to simulate incoming data\n",
    "# We are inserting 10k random records from the sample data into our source table\n",
    "\n",
    "spark.sql(\"\"\"\n",
    "    INSERT INTO dev.etl_source.orders_raw\n",
    "    SELECT * FROM samples.tpch.orders\n",
    "    LIMIT 10000\n",
    "\"\"\")\n",
    "\n",
    "print(\"Inserted 10,000 new records into 'dev.etl_source.orders_raw'.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec4f6b7",
   "metadata": {},
   "source": [
    "## 2. Updated DLT Code (Schema Evolution)\n",
    "\n",
    "We are making the following changes to our previous pipeline logic:\n",
    "1.  **Rename Table:** Changing `joined_silver` to `orders_silver`. DLT will handle the creation of the new table and removal of the old one (if not retained).\n",
    "2.  **Schema Evolution (Gold Layer):**\n",
    "    *   Renaming the count column from `total_orders` to `count_orders`.\n",
    "    *   Adding a NEW aggregation column: `sum_totalprice`.\n",
    "\n",
    "**Copy the code below into your DLT Pipeline Notebook (replacing the previous code).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47d7706",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# BRONZE LAYER (No Changes)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"orders_bronze\",\n",
    "    comment=\"Raw orders data ingested incrementally\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def orders_bronze():\n",
    "    # Streaming table: Tracks offsets via checkpoints\n",
    "    # When triggered, this will only process the NEW 10k records inserted above\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .table(\"dev.etl_source.orders_raw\")\n",
    "    )\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"customer_bronze\",\n",
    "    comment=\"Raw customer reference data\",\n",
    "    table_properties={\"quality\": \"bronze\"}\n",
    ")\n",
    "def customer_bronze():\n",
    "    return (\n",
    "        spark.read\n",
    "        .format(\"delta\")\n",
    "        .table(\"dev.etl_source.customer_raw\")\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# SILVER LAYER (Renamed Table)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "@dlt.view(\n",
    "    name=\"joined_view\",\n",
    "    comment=\"Intermediate logic to join orders with customers\"\n",
    ")\n",
    "def joined_view():\n",
    "    df_orders = spark.read.table(\"LIVE.orders_bronze\")\n",
    "    df_customers = spark.read.table(\"LIVE.customer_bronze\")\n",
    "    \n",
    "    return df_orders.join(\n",
    "        df_customers, \n",
    "        df_orders.o_custkey == df_customers.c_custkey, \n",
    "        \"left\"\n",
    "    )\n",
    "\n",
    "# CHANGE 1: Renamed table from 'joined_silver' to 'orders_silver'\n",
    "@dlt.table(\n",
    "    name=\"orders_silver\",\n",
    "    comment=\"Enriched orders with customer details\",\n",
    "    table_properties={\"quality\": \"silver\"}\n",
    ")\n",
    "def orders_silver():\n",
    "    return (\n",
    "        spark.read.table(\"LIVE.joined_view\")\n",
    "        .withColumn(\"processed_timestamp\", current_timestamp())\n",
    "    )\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# GOLD LAYER (Schema Evolution)\n",
    "# ---------------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    name=\"orders_by_segment_gold\",\n",
    "    comment=\"Aggregated order counts and sales by market segment\",\n",
    "    table_properties={\"quality\": \"gold\"}\n",
    ")\n",
    "def orders_by_segment_gold():\n",
    "    # Reading from the NEW silver table name\n",
    "    df = spark.read.table(\"LIVE.orders_silver\")\n",
    "    \n",
    "    # CHANGE 2: Modified Aggregation Logic\n",
    "    return (\n",
    "        df.groupBy(\"c_mktsegment\")\n",
    "        .agg(\n",
    "            count(\"o_orderkey\").alias(\"count_orders\"),      # Renamed Alias\n",
    "            sum(\"o_totalprice\").alias(\"sum_totalprice\")     # New Column Added\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a04e5c53",
   "metadata": {},
   "source": [
    "## 3. Observations & Internals\n",
    "\n",
    "### A. Incremental Processing (Streaming Tables)\n",
    "When you run the pipeline after inserting data:\n",
    "*   **`orders_bronze` (Streaming Table):** You will see it processes **10,000 records** (or however many you inserted). It uses the **checkpoint** mechanism to remember where it left off.\n",
    "*   **`orders_silver` & `Gold` (Materialized Views):** These will recompute based on the full dataset available in the streaming table.\n",
    "\n",
    "### B. Declarative Nature\n",
    "*   You simply renamed `joined_silver` to `orders_silver` in the code.\n",
    "*   **Result:** DLT automatically creates the new table `orders_silver` in the target schema. The old table `joined_silver` is no longer updated (and eventually removed from the pipeline graph).\n",
    "*   **Gold Table:** The new column `sum_totalprice` appears automatically. You didn't run `ALTER TABLE`.\n",
    "\n",
    "### C. DLT Internals (Storage)\n",
    "All DLT tables are Delta Tables backed by a storage location.\n",
    "1.  **Checkpoints:** Streaming tables maintain state in a hidden `checkpoints` directory within the storage location provided in pipeline settings.\n",
    "2.  **Internal Catalog:** If using Unity Catalog, you might see a catalog named `_databricks_internal`. This is an implementation detail where DLT manages the physical state of materialized views before publishing them to your target schema.\n",
    "\n",
    "### D. Data Lineage\n",
    "Because we used Unity Catalog:\n",
    "1.  Go to **Catalog Explorer**.\n",
    "2.  Select the `orders_by_segment_gold` table.\n",
    "3.  Click the **Lineage** tab.\n",
    "4.  You will see a visual graph connecting `orders_raw` -> `orders_bronze` -> `orders_silver` -> `Gold`.\n",
    "5.  **Column Lineage:** You can click on the `count_orders` column to trace exactly which source columns contributed to this calculation."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
