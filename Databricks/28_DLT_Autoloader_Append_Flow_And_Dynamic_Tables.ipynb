{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b369a0d",
   "metadata": {},
   "source": [
    "# Autoloader, Append Flow & Dynamic Tables in DLT\n",
    "\n",
    "**Objective:**\n",
    "In this session, we will advance our Delta Live Tables (DLT) pipeline by:\n",
    "1.  **Ingesting Files with Autoloader:** Reading CSV files from a managed volume using `cloudFiles`.\n",
    "2.  **Implementing Append Flow:** Performing a union of data streams (Delta Table stream + Autoloader stream) into a single table without full re-processing.\n",
    "3.  **Dynamic Table Generation:** Using DLT Pipeline Configurations (Parameters) to dynamically generate Gold tables based on input variables (e.g., Order Status).\n",
    "\n",
    "**Pre-requisites:**\n",
    "*   A Unity Catalog managed volume created at `/Volumes/dev/etl/landing/` (covered in Setup).\n",
    "*   CSV files uploaded to the `files` directory within that volume."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9830a360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import dlt\n",
    "from pyspark.sql.functions import *\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 1. READ DATA FROM DELTA TABLES (Existing Logic)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Reading raw orders data from the Delta Table source\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment = \"Order Bronze Table\",\n",
    "    name = \"orders_bronze_raw\"\n",
    ")\n",
    "def orders_bronze_raw():\n",
    "    return spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# 2. READ DATA USING AUTOLOADER (New Logic)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Reading raw orders data from Files (CSV) using Auto Loader\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment = \"Order Autoloader Table\",\n",
    "    name = \"orders_autoloader_bronze\"\n",
    ")\n",
    "def orders_autoloader_bronze():\n",
    "    return (\n",
    "        spark.readStream.format(\"cloudFiles\")\n",
    "        .option(\"cloudFiles.format\", \"csv\")\n",
    "        # Schema location for Autoloader schema inference and evolution\n",
    "        .option(\"cloudFiles.schemaLocation\", \"/Volumes/dev/etl/landing/autoloader/schemas/1/\")\n",
    "        # We set evolution mode to none for this demo to stick to provided schema\n",
    "        .option(\"cloudFiles.schemaEvolutionMode\", \"none\")\n",
    "        # Providing schema hints to ensure correct data types\n",
    "        .option(\"cloudFiles.schemaHints\", \"\"\"\n",
    "            order_id long, \n",
    "            customer_id long, \n",
    "            order_status string, \n",
    "            total_price decimal(10,2), \n",
    "            order_date date, \n",
    "            order_priority string, \n",
    "            clerk string, \n",
    "            shipping_priority integer, \n",
    "            comment string\n",
    "        \"\"\")\n",
    "        # The path where files are landed\n",
    "        .load(\"/Volumes/dev/etl/landing/files/\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97256d38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 3. UNION STREAMS USING APPEND FLOW\n",
    "# -------------------------------------------------------------------------\n",
    "# Instead of a standard union which might re-read data, we use Append Flow \n",
    "# to incrementally write data from multiple sources into one target table.\n",
    "\n",
    "# Step A: Create the Target Streaming Table\n",
    "dlt.create_streaming_table(\"orders_union_bronze\")\n",
    "\n",
    "# Step B: Append data from the Delta Source\n",
    "@dlt.append_flow(target = \"orders_union_bronze\")\n",
    "def order_delta_append():\n",
    "    return spark.readStream.table(\"LIVE.orders_bronze_raw\")\n",
    "\n",
    "# Step C: Append data from the Autoloader Source\n",
    "@dlt.append_flow(target = \"orders_union_bronze\")\n",
    "def order_autoloader_append():\n",
    "    return spark.readStream.table(\"LIVE.orders_autoloader_bronze\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d94b2f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 4. DOWNSTREAM TRANSFORMATIONS (Silver Layer)\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "# Reading Customer Data\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"bronze\"},\n",
    "    comment = \"Customer Bronze Table\",\n",
    "    name = \"customer_bronze\"\n",
    ")\n",
    "def cust_bronze():\n",
    "    return spark.read.table(\"dev.bronze.customer_raw\")\n",
    "\n",
    "# Creating Joined View (Silver Logic)\n",
    "# We join the UNIONED table with the Customer table\n",
    "@dlt.view(\n",
    "    comment = \"Joined View\"\n",
    ")\n",
    "def joined_vw():\n",
    "    df_orders = spark.readStream.table(\"LIVE.orders_union_bronze\")\n",
    "    df_cust = spark.read.table(\"LIVE.customer_bronze\")\n",
    "    \n",
    "    # Performing Left Outer Join\n",
    "    df_join = df_orders.join(\n",
    "        df_cust, \n",
    "        df_orders.customer_id == df_cust.c_custkey, \n",
    "        \"left_outer\"\n",
    "    ).select(\n",
    "        df_orders[\"*\"], \n",
    "        df_cust[\"c_name\"], \n",
    "        df_cust[\"c_nationkey\"]\n",
    "    )\n",
    "    return df_join\n",
    "\n",
    "# Create Silver Table\n",
    "@dlt.table(\n",
    "    table_properties={\"quality\": \"silver\"},\n",
    "    name = \"orders_silver\"\n",
    ")\n",
    "def orders_silver():\n",
    "    return (\n",
    "        dlt.read(\"joined_vw\")\n",
    "        .withColumn(\"insert_date\", current_timestamp())\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a8746fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# 5. DYNAMIC GOLD TABLES USING PARAMETERS\n",
    "# -------------------------------------------------------------------------\n",
    "# We will read a configuration parameter 'custom.orderStatus' passed \n",
    "# during the Pipeline setup (e.g., value = \"O,F\")\n",
    "\n",
    "# Fetch configuration, default to \"NA\" if not present\n",
    "_order_status_config = spark.conf.get(\"custom.orderStatus\", \"NA\")\n",
    "\n",
    "# Split the string into a list (e.g., ['O', 'F'])\n",
    "_status_list = _order_status_config.split(\",\")\n",
    "\n",
    "# Loop through each status and generate a table dynamically\n",
    "for _status in _status_list:\n",
    "    \n",
    "    # Define table name dynamically\n",
    "    _table_name = f\"orders_agg_{_status}_gold\"\n",
    "    \n",
    "    # Use function closure to capture the loop variable correctly\n",
    "    def create_gold_table(status_val=_status):\n",
    "        @dlt.table(\n",
    "            table_properties={\"quality\": \"gold\"},\n",
    "            comment = f\"Orders Aggregated Table for status {status_val}\",\n",
    "            name = f\"orders_agg_{status_val}_gold\"\n",
    "        )\n",
    "        def orders_agg_gold():\n",
    "            return (\n",
    "                dlt.read(\"orders_silver\")\n",
    "                # Filter based on the dynamic status\n",
    "                .filter(col(\"order_status\") == status_val)\n",
    "                .groupBy(\"c_name\")\n",
    "                .agg(\n",
    "                    count(\"order_id\").alias(\"total_orders\"),\n",
    "                    sum(\"total_price\").alias(\"total_sales\")\n",
    "                )\n",
    "            )\n",
    "    \n",
    "    # Call the function to register the DLT table\n",
    "    create_gold_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e1c3d02",
   "metadata": {},
   "source": [
    "### Configuration Steps for DLT Pipeline\n",
    "\n",
    "To make the Dynamic Tables work, you must add the following **Configuration** in your DLT Pipeline Settings:\n",
    "\n",
    "1.  Open your DLT Pipeline settings.\n",
    "2.  Scroll to the **Advanced** section.\n",
    "3.  Under **Configuration**, add:\n",
    "    *   **Key:** `custom.orderStatus`\n",
    "    *   **Value:** `O,F` (or any comma-separated status codes you wish to filter by)\n",
    "\n",
    "### Key Takeaways\n",
    "*   **Autoloader (`cloudFiles`)** provides an easy, incremental way to ingest files into DLT.\n",
    "*   **`@dlt.append_flow`** allows you to merge multiple streaming sources into a single target table efficiently, preserving incremental processing.\n",
    "*   **Python Logic:** Since DLT is defined via Python, you can use standard Python control flow (loops, variables) to programmatically generate tables based on configurations."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
