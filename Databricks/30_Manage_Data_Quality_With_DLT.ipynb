{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9faaeab1",
   "metadata": {},
   "source": [
    "# Manage Data Quality with Delta Live Tables\n",
    "\n",
    "**Objective:**\n",
    "In this notebook, we will learn how to implement Data Quality constraints in **Delta Live Tables (DLT)** using **Expectations**. We will explore different strategies to handle data violations: Warning, Dropping records, and Failing the pipeline.\n",
    "\n",
    "**Agenda:**\n",
    "1.  Understanding DLT **Expectations**.\n",
    "2.  Types of Expectations (Warn, Drop, Fail).\n",
    "3.  Defining Data Quality Rules using Python Dictionaries.\n",
    "4.  Implementing Expectations in DLT Pipelines.\n",
    "5.  Monitoring Data Quality using DLT Event Logs."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff31fb40",
   "metadata": {},
   "source": [
    "## 1. What are DLT Expectations?\n",
    "\n",
    "Expectations are optional clauses you add to Delta Live Tables datasets. They allow you to define data quality constraints on the contents of a dataset. DLT runs these checks on each record passing through the query.\n",
    "\n",
    "### Types of Expectations (Actions)\n",
    "\n",
    "There are three main actions DLT can take when a record violates a rule:\n",
    "\n",
    "| Action | Decorator | Behavior | Use Case |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Warn (Default)** | `@dlt.expect` / `@dlt.expect_all` | Records are **written** to the target table, but the violation is reported as a metric. The pipeline succeeds. | When you want to monitor data quality but not lose data or stop processing. |\n",
    "| **Drop** | `@dlt.expect_or_drop` / `@dlt.expect_all_or_drop` | Invalid records are **dropped** before writing to the target. The pipeline succeeds. | When valid data is required for downstream analysis, but you can afford to discard bad rows. |\n",
    "| **Fail** | `@dlt.expect_or_fail` / `@dlt.expect_all_or_fail` | The pipeline update **fails** immediately upon detecting an invalid record. | When data integrity is critical, and processing must stop if bad data arrives. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d0f76a6",
   "metadata": {},
   "source": [
    "## 2. Defining Data Quality Rules\n",
    "\n",
    "Instead of hardcoding rules directly into the decorator, it is best practice to define them in a Python dictionary. This allows for reusability and cleaner code.\n",
    "\n",
    "Let's define rules for our **Orders** and **Customer** datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "539473fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import DLT library\n",
    "import dlt\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Define Rules for ORDERS Data\n",
    "# -------------------------------------------------------------------------\n",
    "# Rule 1: Order Status must be one of 'O' (Open), 'F' (Finished), 'P' (Pending)\n",
    "# Rule 2: Order Price (Total Price) must be greater than 0\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "_order_rules = {\n",
    "    \"Valid Order Status\": \"o_orderstatus in ('O', 'F', 'P')\",\n",
    "    \"Valid Order Price\": \"o_totalprice > 0\"\n",
    "}\n",
    "\n",
    "# -------------------------------------------------------------------------\n",
    "# Define Rules for CUSTOMER Data\n",
    "# -------------------------------------------------------------------------\n",
    "# Rule 1: Market Segment must not be null\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "_customer_rules = {\n",
    "    \"Valid Market Segment\": \"c_mktsegment is not null\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a14b2e74",
   "metadata": {},
   "source": [
    "## 3. Implementing Expectations\n",
    "\n",
    "Below are examples of how to apply these rules using the three different actions.\n",
    "\n",
    "### Scenario A: Warning (Track Metrics Only)\n",
    "This is the default behavior. If a rule is violated, the data is still ingested, but the failure count increases in the DLT UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7024923",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario A: Warning\n",
    "# Using @dlt.expect_all to check multiple rules\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Order bronze table with Warning expectations\"\n",
    ")\n",
    "@dlt.expect_all(_order_rules) # <--- Action: Warn (Default)\n",
    "def orders_bronze_warn():\n",
    "    return (\n",
    "        spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "    )\n",
    "\n",
    "# If 'o_totalprice' is negative, the row IS INSERTED, but marked as failed in metrics."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2ea61a",
   "metadata": {},
   "source": [
    "### Scenario B: Fail the Pipeline\n",
    "Use this when strict data quality is required."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d466a63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario B: Fail\n",
    "# Using @dlt.expect_all_or_fail\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Order bronze table with Fail expectations\"\n",
    ")\n",
    "@dlt.expect_all_or_fail(_order_rules) # <--- Action: Fail Pipeline\n",
    "def orders_bronze_fail():\n",
    "    return (\n",
    "        spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "    )\n",
    "\n",
    "# If 'o_totalprice' is negative, the pipeline STOPS with an error."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32c0ffb4",
   "metadata": {},
   "source": [
    "### Scenario C: Drop Invalid Records\n",
    "Use this to clean data on the fly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6915076a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -------------------------------------------------------------------------\n",
    "# Scenario C: Drop\n",
    "# Using @dlt.expect_all_or_drop\n",
    "# -------------------------------------------------------------------------\n",
    "\n",
    "@dlt.table(\n",
    "    comment=\"Order bronze table with Drop expectations\"\n",
    ")\n",
    "@dlt.expect_all_or_drop(_order_rules) # <--- Action: Drop Record\n",
    "def orders_bronze_drop():\n",
    "    return (\n",
    "        spark.readStream.table(\"dev.bronze.orders_raw\")\n",
    "    )\n",
    "\n",
    "# If 'o_totalprice' is negative, the row is SKIPPED (not inserted), and pipeline continues."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43135e9d",
   "metadata": {},
   "source": [
    "## 4. Applying Rules on Views and Joins\n",
    "\n",
    "You can also apply expectations on Views or downstream tables (Silver/Gold) where you join multiple datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed1ca132",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Combining rules from both dictionaries\n",
    "_all_rules = {**_order_rules, **_customer_rules}\n",
    "\n",
    "@dlt.view(\n",
    "    comment=\"Joined view of orders and customers\"\n",
    ")\n",
    "@dlt.expect_all(_all_rules) # Applying checks on the joined result\n",
    "def joined_vw():\n",
    "    # Read streaming tables\n",
    "    orders_df = dlt.read(\"orders_bronze\")\n",
    "    cust_df = dlt.read(\"customer_bronze_vw\")\n",
    "    \n",
    "    # Perform Join\n",
    "    return orders_df.join(cust_df, orders_df.o_custkey == cust_df.c_custkey, \"left\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28cc73de",
   "metadata": {},
   "source": [
    "## 5. Monitoring Data Quality with Event Logs\n",
    "\n",
    "DLT stores detailed logs of every pipeline execution in the `event_log` table. You can query this log to build custom Data Quality Dashboards.\n",
    "\n",
    "The event log contains a JSON column `details` which holds the DQ metrics under `flow_progress.data_quality.expectations`.\n",
    "\n",
    "### SQL Query to Extract DQ Metrics\n",
    "You can run the following SQL query in a Notebook or SQL Editor to parse the JSON and see specific failure counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986b9787",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- Replace <pipeline-id> with your actual DLT Pipeline ID found in the UI\n",
    "\n",
    "WITH event_log_raw AS (\n",
    "  SELECT * FROM event_log('<pipeline-id>') \n",
    "),\n",
    "latest_update AS (\n",
    "  SELECT origin.update_id \n",
    "  FROM event_log_raw \n",
    "  WHERE event_type = 'create_update' \n",
    "  ORDER BY timestamp DESC LIMIT 1\n",
    ")\n",
    "SELECT\n",
    "  row_expectations.dataset as dataset,\n",
    "  row_expectations.name as expectation,\n",
    "  SUM(row_expectations.passed_records) as passing_records,\n",
    "  SUM(row_expectations.failed_records) as failing_records\n",
    "FROM\n",
    "  event_log_raw,\n",
    "  LATERAL VIEW explode(from_json(details:flow_progress:data_quality:expectations, \"array<struct<name:string, dataset:string, passed_records:int, failed_records:int>>\")) AS row_expectations\n",
    "WHERE\n",
    "  event_type = 'flow_progress'\n",
    "  AND origin.update_id = (SELECT update_id FROM latest_update)\n",
    "GROUP BY\n",
    "  row_expectations.dataset,\n",
    "  row_expectations.name;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e5fea82",
   "metadata": {},
   "source": [
    "### Summary\n",
    "\n",
    "1.  **Expectations** allow you to enforce data quality in DLT.\n",
    "2.  **Warn** is great for observability without stopping flows.\n",
    "3.  **Drop** ensures clean data in your target tables.\n",
    "4.  **Fail** prevents bad data ingestion entirely but stops the pipeline.\n",
    "5.  Use the **DLT UI** or **Event Log Queries** to monitor the health and quality of your data over time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
