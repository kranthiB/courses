{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9560943e",
   "metadata": {},
   "source": [
    "# Databricks DLT: Advanced Features & Orchestration\n",
    "## Handling Truncates, Full Refreshes, and File Arrival Triggers\n",
    "\n",
    "**Objective:**\n",
    "In this session, we will wrap up our deep dive into Delta Live Tables (DLT) by addressing complex real-world scenarios and moving our pipeline to production.\n",
    "\n",
    "**Agenda:**\n",
    "1.  **Handling Non-Append-Only Sources:** How to stream from tables that get truncated and reloaded (Truncate & Load).\n",
    "2.  **Full Refresh Management:** How to trigger a full refresh and how to **prevent** specific tables (like SCD Type 2) from losing history during a full refresh.\n",
    "3.  **Orchestration with Workflows:** Scheduling the DLT pipeline using Databricks Workflows.\n",
    "4.  **Event-Driven Architecture:** Setting up **File Arrival Triggers** to run the pipeline automatically when new data lands in a Volume.\n",
    "\n",
    "**Pre-requisites:**\n",
    "*   You should have the DLT Pipeline from the previous sessions (`00_dlt_introduction`) set up.\n",
    "*   A running Compute Cluster for executing the setup commands in this notebook."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efd5a86c",
   "metadata": {},
   "source": [
    "## 1. Handling Truncate & Load Sources in Streaming\n",
    "\n",
    "**The Problem:**\n",
    "Streaming Tables (`readStream`) typically expect the source to be \"Append-Only\". If a source table is truncated (all data deleted) and then reloaded, the default behavior of a Spark Stream is to fail or stop processing because the offsets become invalid.\n",
    "\n",
    "**The Scenario:**\n",
    "Imagine our `orders_raw` table isn't appended to. Instead, an upstream process truncates it daily and inserts the \"new\" incremental batch.\n",
    "\n",
    "**The Solution:**\n",
    "We can use the option `skipChangeCommits` set to `true`. This tells DLT/Spark Structure Streaming to ignore the version commits associated with deletions/updates on the source Delta table and only process the new inserts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c97d1e53",
   "metadata": {},
   "source": [
    "### Step 1.1: Simulate a Truncate & Load\n",
    "Let's modify our source data. We will truncate the `orders_raw` table and insert a small batch of new records.\n",
    "\n",
    "*Note: Run this cell using your standard interactive cluster, not inside the DLT pipeline.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f34094",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Truncate the source table\n",
    "spark.sql(\"TRUNCATE TABLE dev.bronze.orders_raw\")\n",
    "\n",
    "# 2. Insert new incremental records\n",
    "spark.sql(\"\"\"\n",
    "INSERT INTO dev.bronze.orders_raw\n",
    "VALUES\n",
    "(999, 227285, 'O', 162769.66, '1995-10-11', '1-URGENT', 'Clerk#000000412', 0, 'Incremental Record'),\n",
    "(999, 227285, 'O', 100,       '1995-10-11', '1-URGENT', 'Clerk#000000412', 0, 'Incremental Record'),\n",
    "(999, 227285, 'O', 999,       '1995-10-11', '1-URGENT', 'Clerk#000000412', 0, 'Incremental Record')\n",
    "\"\"\")\n",
    "\n",
    "# 3. Verify data\n",
    "display(spark.sql(\"SELECT * FROM dev.bronze.orders_raw\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fc8878c",
   "metadata": {},
   "source": [
    "### Step 1.2: Update DLT Code\n",
    "Go to your DLT pipeline notebook (e.g., `00_dlt_introduction`) and update the `orders_bronze` definition.\n",
    "\n",
    "**Code Change:**\n",
    "Add `.option(\"skipChangeCommits\", \"true\")` to the read stream.\n",
    "\n",
    "```python\n",
    "@dlt.table(\n",
    "    comment=\"Order bronze table\"\n",
    ")\n",
    "def orders_bronze():\n",
    "    return (\n",
    "        spark.readStream\n",
    "        .format(\"delta\")\n",
    "        .option(\"skipChangeCommits\", \"true\") # <--- ADD THIS OPTION\n",
    "        .table(\"dev.bronze.orders_raw\")\n",
    "    )\n",
    "\n",
    "\n",
    "Action:\n",
    "1. Update the code in your DLT notebook.\n",
    "2. Click Start on your DLT Pipeline.\n",
    "3. Verify that the pipeline succeeds and processes the 3 new records, despite the source being truncated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ffea612",
   "metadata": {},
   "source": [
    "## 2. Managing Full Refreshes\n",
    "\n",
    "**The Concept:**\n",
    "A **Full Refresh** clears all state and data from the DLT tables and rebuilds them from scratch (re-reading all source data). This is useful if schema changes drastically or logic is updated significantly.\n",
    "\n",
    "**The Problem with SCD Type 2:**\n",
    "If you have a Slowly Changing Dimension (SCD) Type 2 table (which tracks history), a Full Refresh is dangerous. It will wipe out your historical tracking and reset the table to the current state of the source, effectively deleting your historical insights.\n",
    "\n",
    "**The Solution:**\n",
    "We can use the table property `pipelines.reset.allowed` set to `false`. This prevents the specific table from being refreshed even when \"Full Refresh All\" is clicked."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2110998",
   "metadata": {},
   "source": [
    "### Step 2.1: Protect SCD Table\n",
    "In your DLT pipeline notebook, locate your SCD Type 2 table definition (e.g., `customers_scd2`) and add the table property.\n",
    "\n",
    "**Code Change:**\n",
    "\n",
    "```python\n",
    "dlt.create_streaming_table(\n",
    "    name=\"customer_scd2_bronze\",\n",
    "    table_properties={\n",
    "        \"quality\": \"bronze\",\n",
    "        \"pipelines.reset.allowed\": \"false\" # <--- ADD THIS PROPERTY\n",
    "    }\n",
    ")\n",
    "# ... rest of the SCD logic ...\n",
    "\n",
    "\n",
    "Action:\n",
    "1. Update the code in your DLT notebook.\n",
    "2. If you were to click the arrow next to \"Start\" and select \"Full Refresh all\", this specific table would generally be skipped or protected (depending on DLT runtime version behavior, it ensures history isn't accidentally wiped via standard refresh mechanisms)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ed9630f",
   "metadata": {},
   "source": [
    "## 3. Orchestration with Workflows & File Arrival Triggers\n",
    "\n",
    "Running pipelines manually via the \"Start\" button is for development. For production, we use **Databricks Workflows**.\n",
    "\n",
    "We will set up a specific type of trigger: **File Arrival**. This runs the pipeline only when a file lands in a specific Storage Location (or Volume).\n",
    "\n",
    "### Step 3.1: Create the Workflow\n",
    "1. Navigate to **Workflows** on the left sidebar.\n",
    "2. Click **Create Job**.\n",
    "3. **Name:** `DLT_Pipeline_File_Trigger`.\n",
    "4. **Task Name:** `dlt_task`.\n",
    "5. **Type:** Select **Delta Live Tables pipeline**.\n",
    "6. **Pipeline:** Select your `00_dlt_introduction` pipeline.\n",
    "7. **Create Task**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b779bf5",
   "metadata": {},
   "source": [
    "### Step 3.2: Configure File Arrival Trigger\n",
    "1. On the Job configuration page, look for the **Trigger** section on the right.\n",
    "2. Click **Add Trigger**.\n",
    "3. **Trigger Type:** Select **File Arrival**.\n",
    "4. **Storage Location:** Enter the path to your Volume/Landing zone where the Auto Loader expects files.\n",
    "   * *Example:* `/Volumes/dev/etl/landing/files/`\n",
    "5. **Configuration:**\n",
    "   * **Minimum time between triggers:** (Optional, e.g., 0 seconds).\n",
    "   * **Wait after last change:** (Optional).\n",
    "6. Click **Save**.\n",
    "\n",
    "*Note: Ensure the DLT Pipeline mode is set to **\"Production\"** in the DLT settings. This ensures the cluster terminates automatically after the job finishes, saving costs.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b0e15dd",
   "metadata": {},
   "source": [
    "### Step 3.3: Test the Trigger\n",
    "To test this, we don't click \"Run Now\". Instead, we upload a file to the volume. The File Arrival trigger is constantly polling (or using event grid) to detect new files.\n",
    "\n",
    "Run the cell below to upload a dummy file to your landing zone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d3f5548",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper to write a dummy CSV file to the Volume being watched by the Workflow\n",
    "import pandas as pd\n",
    "import time\n",
    "\n",
    "# Define path (Update this to match your Volume path used in the Trigger configuration)\n",
    "volume_path = \"/Volumes/dev/etl/landing/files/\"\n",
    "filename = f\"orders_trigger_test_{int(time.time())}.csv\"\n",
    "full_path = volume_path + filename\n",
    "\n",
    "# Create dummy data\n",
    "data = {\n",
    "    \"order_id\": [1001],\n",
    "    \"customer_id\": [227285],\n",
    "    \"order_status\": [\"O\"],\n",
    "    \"total_price\": [50.0],\n",
    "    \"order_date\": [\"2024-01-01\"],\n",
    "    \"order_priority\": [\"1-URGENT\"],\n",
    "    \"clerk\": [\"Clerk#001\"],\n",
    "    \"ship_priority\": [0],\n",
    "    \"comment\": [\"Triggered by File Arrival\"]\n",
    "}\n",
    "\n",
    "# Write file\n",
    "df = pd.DataFrame(data)\n",
    "# Note: In a real notebook, you might use dbutils.fs.put or standard python write\n",
    "# writing to /Volumes works like a standard file system\n",
    "df.to_csv(full_path, index=False)\n",
    "\n",
    "print(f\"File uploaded to: {full_path}\")\n",
    "print(\"Go to the Workflows UI and check the 'Runs' tab. The job should start automatically within a minute.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9eb951d2",
   "metadata": {},
   "source": [
    "## 4. Summary & SQL Alternative\n",
    "\n",
    "### DLT with SQL\n",
    "While this series focused on **Python** for DLT (which offers flexibility for metaprogramming), Databricks fully supports **SQL** for DLT.\n",
    "The concepts remain identical:\n",
    "*   `CREATE STREAMING LIVE TABLE` instead of `@dlt.table`\n",
    "*   `APPLY CHANGES INTO` for SCD Type 1/2 logic.\n",
    "\n",
    "### What we accomplished:\n",
    "1.  **Robust Ingestion:** Handled `skipChangeCommits` for non-append sources.\n",
    "2.  **Safety:** Protected historical tables from accidental Full Refreshes.\n",
    "3.  **Automation:** Deployed the pipeline using Workflows.\n",
    "4.  **Event-Driven:** Configured the pipeline to run automatically upon data arrival.\n",
    "\n",
    "This concludes the Delta Live Tables module of the Zero to Hero series!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
