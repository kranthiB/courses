{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4d3778e9",
   "metadata": {},
   "source": [
    "# Databricks SQL & SQL Warehouses\n",
    "## Data Analyst Persona\n",
    "\n",
    "Up until now, we have largely focused on the **Data Engineering** persona (working with Spark Clusters, Jobs, DLT, and Notebooks). Today, we shift gears to the **Data Analyst** persona.\n",
    "\n",
    "### Objectives\n",
    "1.  Understand **Databricks SQL (DBSQL)** and how it enables Data Warehousing on the Lakehouse.\n",
    "2.  Learn about **SQL Warehouses** (Serverless, Pro, Classic) and their features.\n",
    "3.  Explore the **SQL Editor** for writing and managing queries.\n",
    "4.  Deep dive into **Query Profiling**, Monitoring, and Debugging (Spark UI).\n",
    "5.  Create **Visualizations** and **Dashboards** directly from queries.\n",
    "6.  Connect **BI Tools** (PowerBI, Tableau) to Databricks.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f95d858b",
   "metadata": {},
   "source": [
    "## 1. What is Databricks SQL (DBSQL)?\n",
    "\n",
    "Databricks SQL provides a dedicated interface for data analysts who want to run SQL queries without worrying about Spark configurations, cluster management, or Python code.\n",
    "\n",
    "### The \"Warehouse\" on the \"Lake\"\n",
    "Traditionally, companies moved data from a Data Lake to a Data Warehouse (like Snowflake or Redshift) for high-performance SQL analysis.\n",
    "*   **Databricks Solution:** With the **Lakehouse** architecture (powered by Delta Lake), you can perform Data Warehousing operations directly on your data lake storage.\n",
    "*   **Benefit:** No data movement, single source of truth, lower latency, and lower TCO (Total Cost of Ownership).\n",
    "\n",
    "### SQL Warehouse Types\n",
    "To run SQL queries in DBSQL, you need a compute resource called a **SQL Warehouse**.\n",
    "\n",
    "| Feature | **Serverless** (Recommended) | **Pro** | **Classic** |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Startup Time** | Instant (Seconds) | Minutes | Minutes |\n",
    "| **Compute Management** | Managed by Databricks (in their account) | In your Cloud Account | In your Cloud Account |\n",
    "| **Performance** | **Intelligent Workload Management**, Predictive I/O | Photon Engine | Standard |\n",
    "| **Cost** | Optimizes cost by scaling down instantly | Standard scaling | Slower scaling |\n",
    "\n",
    "**Key Features:**\n",
    "*   **Photon Engine:** A vectorized query engine written in C++ for extreme speed.\n",
    "*   **Predictive I/O:** AI-driven optimization to scan data faster.\n",
    "*   **Concurrency:** Warehouses automatically scale (add more clusters) based on the number of concurrent queries."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9fbb633",
   "metadata": {},
   "source": [
    "## 2. Creating a SQL Warehouse (Hands-on)\n",
    "\n",
    "*Note: This is a GUI operation, but the steps are documented here for reference.*\n",
    "\n",
    "1.  Navigate to **Compute** -> **SQL Warehouses** tab.\n",
    "2.  Click **Create SQL Warehouse**.\n",
    "3.  **Name:** E.g., `dev_warehouse`.\n",
    "4.  **Cluster Size:** Uses \"T-Shirt\" sizing (2X-Small, Small, Medium, etc.).\n",
    "    *   *Tip:* Start small (2X-Small) and scale up if query latency is high.\n",
    "5.  **Scaling:** Set Min and Max clusters.\n",
    "    *   *Example:* Min 1, Max 5. Databricks will spin up new clusters if too many users run queries simultaneously to prevent queuing.\n",
    "6.  **Type:** Select **Serverless** for the best experience.\n",
    "7.  **Unity Catalog:** Enabled by default.\n",
    "\n",
    "### Connection Details (BI Tools)\n",
    "Once created, go to the **Connection Details** tab. You will find:\n",
    "*   **Server Hostname**\n",
    "*   **HTTP Path**\n",
    "*   **JDBC/ODBC Connection String**\n",
    "*   *Direct Connectors:* Buttons to download connection files for PowerBI, Tableau, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a0352dd",
   "metadata": {},
   "source": [
    "## 3. The SQL Editor\n",
    "\n",
    "The SQL Editor is the IDE for Data Analysts. It allows you to:\n",
    "*   Browse the **Catalog Explorer** (Catalogs, Schemas, Tables) on the left.\n",
    "*   Write generic ANSI SQL queries.\n",
    "*   Save, Share, and Schedule queries.\n",
    "\n",
    "### Example Query Scenario\n",
    "We will join our `orders` table with the `customers` (SCD Type 2) table to calculate total orders by market segment.\n",
    "\n",
    "**Key Logic:**\n",
    "*   Join `orders_silver` (Fact) with `customers_silver` (Dimension).\n",
    "*   Filter for active customer records (`_END_AT IS NULL`) because it is an SCD Type 2 table.\n",
    "*   Group by `market_segment`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebd3eee9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "-- This creates the SQL query logic demonstrated in the video.\n",
    "-- You can run this in a Notebook attached to a SQL Warehouse or in the SQL Editor.\n",
    "\n",
    "SELECT\n",
    "    c.c_mktsegment,\n",
    "    COUNT(o.o_orderkey) AS total_orders\n",
    "FROM\n",
    "    dev.etl.orders_silver o\n",
    "    LEFT JOIN dev.etl.customer_scd2_bronze c\n",
    "        ON o.o_custkey = c.c_custkey\n",
    "WHERE\n",
    "    -- SCD Type 2 Logic: Only take the current active record for the customer\n",
    "    c.__END_AT IS NULL\n",
    "GROUP BY\n",
    "    c.c_mktsegment;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d04de73",
   "metadata": {},
   "source": [
    "## 4. Query Profiling & Debugging\n",
    "\n",
    "One of the most powerful features of DBSQL is the **Query Profile**.\n",
    "\n",
    "### How to access:\n",
    "1.  Run a query in the SQL Editor.\n",
    "2.  Click on the time duration at the bottom of the editor (e.g., \"Time in Photon\").\n",
    "3.  Click **See Query Profile**.\n",
    "\n",
    "### What to look for:\n",
    "*   **Visual DAG:** A flowchart of how the query was executed.\n",
    "*   **Time Spent:** See exactly which step took the most time (e.g., Scanning data vs. Aggregating vs. Shuffling).\n",
    "*   **Rows Processed:** Hover over lines to see if a join caused a data explosion (or if partition pruning worked efficiently).\n",
    "*   **Photon Usage:** Yellow boxes indicate the query is running on the high-performance Photon engine.\n",
    "\n",
    "### Advanced Debugging (Spark UI)\n",
    "For deep engineering analysis, you can click the **Spark UI** link within the Query Profile to see the underlying Spark DAG, stages, and tasks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb97969d",
   "metadata": {},
   "source": [
    "## 5. Visualizations & Scheduling\n",
    "\n",
    "### Visualizations\n",
    "You don't always need an external BI tool.\n",
    "1.  In the SQL Editor results pane, click the **+ (Plus)** icon -> **Visualization**.\n",
    "2.  **Type:** Select Bar, Line, Pie, etc.\n",
    "3.  **Configuration:** Drag and drop columns (e.g., X-axis: `c_mktsegment`, Y-axis: `total_orders`).\n",
    "4.  These visualizations can be added to **Databricks Dashboards**.\n",
    "\n",
    "### Scheduling\n",
    "You can schedule a SQL query to run periodically (e.g., to refresh a report or check data quality).\n",
    "1.  Click **Schedule** at the top right of the editor.\n",
    "2.  Set frequency (e.g., Every 12 hours).\n",
    "3.  Choose the SQL Warehouse to run the schedule."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d572bda0",
   "metadata": {},
   "source": [
    "## 6. Notebooks on SQL Warehouses\n",
    "\n",
    "A relatively new and powerful feature is the ability to run **Jupyter/Databricks Notebooks** directly on a **SQL Warehouse**.\n",
    "\n",
    "*   **Why?** You get the narrative power of a notebook with the startup speed and cost-efficiency of a Serverless SQL Warehouse.\n",
    "*   **Limitation:** You can primarily run SQL. Python support on SQL Warehouses is limited or non-existent depending on the specific warehouse configuration (designed for SQL workloads).\n",
    "*   **How:** In the notebook dropdown for \"Connect\", select your **SQL Warehouse** instead of a standard All-Purpose Cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3807317",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Note: If this notebook is attached to a SQL Warehouse,\n",
    "# Only SQL cells (like Cell 5) will execute successfully.\n",
    "# Python code cells might not run or have limited functionality.\n",
    "\n",
    "print(\"To test the SQL Warehouse integration:\")\n",
    "print(\"1. Click the 'Connect' dropdown at the top right.\")\n",
    "print(\"2. Select 'SQL Warehouses'.\")\n",
    "print(\"3. Choose the warehouse created in step 2 (e.g., dev_warehouse).\")\n",
    "print(\"4. Run the SQL cell above.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
