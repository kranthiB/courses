{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "97c7867c",
   "metadata": {},
   "source": [
    "# Streaming Tables & Materialized Views in Databricks SQL\n",
    "\n",
    "## Objectives\n",
    "In this notebook, we will explore advanced Databricks SQL capabilities that bridge the gap between Data Warehousing and Data Engineering:\n",
    "\n",
    "1.  **Streaming Tables (ST):** How to ingest data incrementally from cloud storage (S3/ADLS/Volumes) using SQL.\n",
    "2.  **Materialized Views (MV):** How to create pre-computed aggregation tables that update incrementally.\n",
    "3.  **The Backend:** Understand how DBSQL uses Serverless Delta Live Tables (DLT) pipelines to manage these objects.\n",
    "4.  **Scheduling:** How to automate the refresh of these tables using SQL syntax.\n",
    "\n",
    "## Prerequisites\n",
    "*   **Unity Catalog** must be enabled.\n",
    "*   You must use a **Serverless SQL Warehouse** or a **Pro SQL Warehouse** to execute these specific SQL commands (`CREATE STREAMING TABLE`, `MATERIALIZED VIEW`).\n",
    "*   *Note:* While we are running this in a Notebook, these commands are native to the SQL Editor/DBSQL environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf0bbca2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. SETUP: Create Dummy Data\n",
    "# We need some files in a Volume to simulate an ingestion source.\n",
    "# Let's create a Volume and write a CSV file to it.\n",
    "\n",
    "import os\n",
    "\n",
    "catalog = \"dev\"\n",
    "schema = \"bronze\"\n",
    "volume_name = \"external_vol\"\n",
    "file_path = f\"/Volumes/{catalog}/{schema}/{volume_name}/files/\"\n",
    "\n",
    "# Create directory if strictly local (for demo purposes) or verify volume exists\n",
    "# dbutils.fs.mkdirs(file_path) # Uncomment if needed\n",
    "\n",
    "# Define dummy data for Batch 1\n",
    "data_batch_1 = \"\"\"order_id,order_date,order_status,total_price\n",
    "101,2024-01-01,O,150.00\n",
    "102,2024-01-01,F,200.50\n",
    "103,2024-01-02,O,120.00\n",
    "104,2024-01-02,F,300.00\n",
    "\"\"\"\n",
    "\n",
    "# Write orders_1.csv\n",
    "dbutils.fs.put(f\"{file_path}orders_1.csv\", data_batch_1, True)\n",
    "\n",
    "print(f\"Created orders_1.csv at {file_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e43f998",
   "metadata": {},
   "source": [
    "## 1. Creating a Streaming Table (ST)\n",
    "\n",
    "A **Streaming Table** is a Delta table with extra support for streaming or incremental data processing.\n",
    "\n",
    "We will use the function `read_files()` (a Table Valued Function) to act as a streaming source. It automatically detects new files added to the source directory.\n",
    "\n",
    "**Key Syntax:**\n",
    "```sql\n",
    "CREATE OR REFRESH STREAMING TABLE <table_name> AS\n",
    "SELECT * FROM STREAM read_files('<path>', format => 'csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b8dcaea",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Create the Streaming Table\n",
    "-- Note: Replace the path below with your specific Volume path if different\n",
    "CREATE OR REFRESH STREAMING TABLE dev.bronze.orders_st\n",
    "AS\n",
    "SELECT *\n",
    "FROM STREAM read_files(\n",
    "  '/Volumes/dev/bronze/external_vol/files/',\n",
    "  format => 'csv',\n",
    "  header => 'true',\n",
    "  schema => 'order_id INT, order_date DATE, order_status STRING, total_price DOUBLE'\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ad112ef",
   "metadata": {},
   "source": [
    "### Verify Data Load\n",
    "The table should now contain data from `orders_1.csv`.\n",
    "**Note on Architecture:** When you ran the command above, Databricks actually kicked off a serverless DLT update in the background to load this data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a769d918",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM dev.bronze.orders_st;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe19ca94",
   "metadata": {},
   "source": [
    "## 2. Incremental Loading (The \"Streaming\" part)\n",
    "\n",
    "The power of a Streaming Table is that it tracks state. It knows which files it has already ingested. If we add a new file, running `REFRESH` will only process the **new** file.\n",
    "\n",
    "Let's simulate arriving data by adding `orders_2.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e876408",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Batch 2 Data\n",
    "data_batch_2 = \"\"\"order_id,order_date,order_status,total_price\n",
    "105,2024-01-03,O,550.00\n",
    "106,2024-01-03,F,100.00\n",
    "\"\"\"\n",
    "\n",
    "# Write orders_2.csv to the same folder\n",
    "dbutils.fs.put(f\"{file_path}orders_2.csv\", data_batch_2, True)\n",
    "\n",
    "print(f\"Created orders_2.csv at {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "660a15bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Refresh the streaming table\n",
    "-- This acts like a Trigger: AvailableNow in Spark Structured Streaming\n",
    "REFRESH STREAMING TABLE dev.bronze.orders_st;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc366f78",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Verify that new records (105, 106) are added\n",
    "SELECT * FROM dev.bronze.orders_st;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae81d6f8",
   "metadata": {},
   "source": [
    "## 3. Creating a Materialized View (MV)\n",
    "\n",
    "A **Materialized View** computes and stores the result of a query. Unlike a standard View (which runs the query every time you call it), an MV stores the physical data.\n",
    "\n",
    "**Key Benefit:** Performance. Complex aggregations are pre-computed.\n",
    "**Incremental Refresh:** When the source (our Streaming Table) updates, the MV attempts to update *incrementally* rather than re-computing the whole table.\n",
    "\n",
    "Let's create an MV to calculate total sales by status."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498cf001",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE MATERIALIZED VIEW dev.bronze.orders_mv\n",
    "AS\n",
    "SELECT\n",
    "  order_status,\n",
    "  SUM(total_price) as agg_total_price,\n",
    "  COUNT(*) as order_count\n",
    "FROM dev.bronze.orders_st\n",
    "GROUP BY order_status;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f6c1f2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Check the aggregated results\n",
    "SELECT * FROM dev.bronze.orders_mv;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d280caa",
   "metadata": {},
   "source": [
    "## 4. End-to-End Flow & Scheduling\n",
    "\n",
    "Let's see the full power of the \"Lakehouse\" architecture.\n",
    "1. We will add `orders_3.csv`.\n",
    "2. We will `REFRESH` the Materialized View.\n",
    "   * *Note:* Refreshing the MV automatically ensures upstream dependencies (the Streaming Table) are fresh. It cascades the update!\n",
    "\n",
    "We can also add a `SCHEDULE` clause to automate this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89b072fb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Batch 3 Data\n",
    "data_batch_3 = \"\"\"order_id,order_date,order_status,total_price\n",
    "107,2024-01-04,O,90.00\n",
    "\"\"\"\n",
    "\n",
    "# Write orders_3.csv\n",
    "dbutils.fs.put(f\"{file_path}orders_3.csv\", data_batch_3, True)\n",
    "print(f\"Created orders_3.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a8c38b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- We can manually refresh the MV.\n",
    "-- Because MV depends on ST, Databricks checks ST for new data (orders_3),\n",
    "-- ingests it, and then updates the MV aggregation incrementally.\n",
    "\n",
    "REFRESH MATERIALIZED VIEW dev.bronze.orders_mv;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87c3bbe0",
   "metadata": {},
   "source": [
    "### Scheduling Syntax\n",
    "To make this run automatically (e.g., every hour), you modify the Create statement or use `ALTER`.\n",
    "\n",
    "```sql\n",
    "CREATE OR REPLACE MATERIALIZED VIEW dev.bronze.orders_mv\n",
    "SCHEDULE CRON '0 0 * * * ?' -- Run every day at midnight (or use EVERY 1 HOUR)\n",
    "AS\n",
    "SELECT ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4b388c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "-- Example of altering schedule\n",
    "-- This will ensure the table refreshes automatically every 4 hours\n",
    "ALTER MATERIALIZED VIEW dev.bronze.orders_mv\n",
    "SET SCHEDULE EVERY 4 HOURS;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c98b384c",
   "metadata": {},
   "source": [
    "## 5. Under the Hood: Delta Live Tables (DLT)\n",
    "\n",
    "If you navigate to **Query History** in your Databricks Workspace after running these cells:\n",
    "1.  Look for the `REFRESH` commands.\n",
    "2.  You will see a link to a **Job/Pipeline**.\n",
    "3.  Clicking that link takes you to a **Delta Live Tables** pipeline interface.\n",
    "\n",
    "**Insight:**\n",
    "Databricks SQL uses a \"Serverless DLT\" backend to manage the state, checkpoints, and incremental processing logic for Streaming Tables and Materialized Views.\n",
    "*   **No-Op:** If no new files are found, the pipeline creates a \"No-Op\" (No Operation) event, saving compute cost.\n",
    "*   **Group Aggregate:** For MVs, it uses stateful aggregation to only update the changed groups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf3a841a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Clean up (Optional)\n",
    "# dbutils.fs.rm(file_path, True)\n",
    "# spark.sql(\"DROP MATERIALIZED VIEW IF EXISTS dev.bronze.orders_mv\")\n",
    "# spark.sql(\"DROP TABLE IF EXISTS dev.bronze.orders_st\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
