{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6e5f93f0",
   "metadata": {},
   "source": [
    "# Databricks Asset Bundles (DABs) - Part 1\n",
    "## Configuration & Dev Deployment\n",
    "\n",
    "**Databricks Asset Bundles (DABs)** allow you to express your Databricks data, analytics, and ML projects as code (Infrastructure as Code). They provide a unified interface to manage your project's resources (Jobs, Pipelines, Notebooks) and deploy them across different environments (Dev, Staging, Prod) using CI/CD.\n",
    "\n",
    "### Learning Objectives\n",
    "1.  Understand the structure of a DABs project.\n",
    "2.  Configure `databricks.yml` manually from scratch.\n",
    "3.  Define resources (Jobs, DLT Pipelines, Schemas) using YAML.\n",
    "4.  Understand **Targets**, **Variables**, and **Presets**.\n",
    "5.  Deploy bundles using the **UI** and **Databricks CLI**.\n",
    "6.  Understand **Source-Linked Deployment** vs. **Bundle Deployment**.\n",
    "\n",
    "### Prerequisites\n",
    "*   Databricks Workspace (with Serverless enabled for UI deployment features).\n",
    "*   Databricks CLI installed (v0.218.0 or higher recommended).\n",
    "*   Knowledge of Git and basic YAML syntax."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cd27bd8",
   "metadata": {},
   "source": [
    "## 1. Project Structure & Initialization\n",
    "\n",
    "While you can use `databricks bundle init` to generate a template, understanding the manual structure is crucial for customization.\n",
    "\n",
    "We will create a project structure within Databricks Repos (Git Folders) as follows:\n",
    "\n",
    "```text\n",
    "my_dab_project/\n",
    "├── databricks.yml          # Main configuration file\n",
    "├── resources/              # Folder for resource definitions\n",
    "│   ├── jobs/               # YAML files for Jobs\n",
    "│   ├── pipelines/          # YAML files for DLT Pipelines\n",
    "│   ├── schemas/            # YAML files for Schemas\n",
    "│   └── notebooks/          # Source code notebooks\n",
    "└── src/                    # Source code folder\n",
    "    └── env/\n",
    "        └── variables.yml   # Environment variables\n",
    "```\n",
    "\n",
    "**Note:** The root must contain the databricks.yml file for Databricks to recognize it as a Bundle project."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "776e529c",
   "metadata": {},
   "source": [
    "## 2. Configuring `databricks.yml`\n",
    "\n",
    "The `databricks.yml` is the heart of your bundle. It defines the bundle name, includes other configurations, permissions, and deployment targets.\n",
    "\n",
    "### Basic Configuration Structure\n",
    "\n",
    "```yaml\n",
    "bundle:\n",
    "  name: dab_demo  # Unique name for the bundle\n",
    "\n",
    "include:\n",
    "  - src/env/*.yml           # Include variable files\n",
    "  - resources/jobs/*.yml    # Include job definitions\n",
    "  - resources/pipelines/*.yml # Include pipeline definitions\n",
    "  - resources/schemas/*.yml   # Include schema definitions\n",
    "\n",
    "permissions:\n",
    "  - group_name: de_grp      # Grant permissions to specific groups\n",
    "    level: CAN_MANAGE\n",
    "\n",
    "targets:\n",
    "  dev:\n",
    "    mode: development       # optimized for interactive dev loops\n",
    "    default: true\n",
    "    workspace:\n",
    "      host: https://<your-databricks-instance-url>\n",
    "    \n",
    "  qa:\n",
    "    workspace:\n",
    "      host: https://<your-qa-databricks-instance-url>\n",
    "```\n",
    "\n",
    "### Key Concepts:\n",
    "1. include: Allows you to split your configuration into multiple files for better organization.\n",
    "2. targets: Defines the environments (e.g., dev, qa, prod).\n",
    "3. mode: development:\n",
    "\n",
    "    Adds a prefix (e.g., [dev <user>]) to resource names to avoid collisions.\n",
    "\n",
    "    Enables **Source-Linked Deployment** (files are read directly from the Repo/IDE, skipping the upload step for faster iteration)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78372df4",
   "metadata": {},
   "source": [
    "## 3. Defining Resources (YAML)\n",
    "\n",
    "Instead of clicking through the UI, we define resources in YAML files under the `resources/` folder.\n",
    "\n",
    "### A. Defining a Job (`resources/jobs/demo_job.yml`)\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  jobs:\n",
    "    dab_demo_job:\n",
    "      name: dab_demo_job\n",
    "      tasks:\n",
    "        - task_key: demo_notebook\n",
    "          notebook_task:\n",
    "            notebook_path: ../notebooks/demo_repo_notebook.py # Relative path\n",
    "            source: WORKSPACE\n",
    "      job_clusters:\n",
    "        - job_cluster_key: job_cluster\n",
    "          new_cluster:\n",
    "            spark_version: 15.4.x-scala2.12\n",
    "            node_type_id: Standard_D4ds_v5\n",
    "            num_workers: 1\n",
    "```\n",
    "\n",
    "### B. Defining a DLT Pipeline (resources/pipelines/demo_pipeline.yml)\n",
    "We can use variables (like ${var.catalog}) to make configurations dynamic across environments.\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  pipelines:\n",
    "    dlt_orders_pipeline:\n",
    "      name: dlt_orders_pipeline\n",
    "      target: ${var.catalog}  # Dynamic catalog name\n",
    "      libraries:\n",
    "        - notebook:\n",
    "            path: ../notebooks/dlt_dab_demo_orders.sql\n",
    "      configuration:\n",
    "        my_param: value\n",
    "      development: true\n",
    "```\n",
    "\n",
    "### C. Defining a Schema (resources/schemas/bronze_schema.yml)\n",
    "We can also manage Unity Catalog schemas via DABs.\n",
    "\n",
    "```yaml\n",
    "resources:\n",
    "  schemas:\n",
    "    bronze_schema:\n",
    "      name: bronze\n",
    "      catalog_name: ${var.catalog}\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "124d962f",
   "metadata": {},
   "source": [
    "## 4. Variables and Environment Specifics\n",
    "\n",
    "To handle differences between environments (like Catalog names), we use variables.\n",
    "\n",
    "**1. Define Variables (`src/env/variables.yml`)**\n",
    "```yaml\n",
    "variables:\n",
    "  catalog:\n",
    "    description: The target catalog for the environment\n",
    "    default: dev  # Default value for dev target\n",
    "```\n",
    "\n",
    "**2. Override in databricks.yml Targets**\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    # uses default catalog: dev\n",
    "  \n",
    "  qa:\n",
    "    variables:\n",
    "      catalog: qa_catalog  # Overrides catalog for QA\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6afaad",
   "metadata": {},
   "source": [
    "## 5. Development Workflow: Presets & Source Linking\n",
    "\n",
    "In a shared development environment, multiple developers might work on the same bundle. To prevent conflicts, we use **Presets**.\n",
    "\n",
    "Add this to your `databricks.yml` under `targets: dev`:\n",
    "\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    presets:\n",
    "      name_prefix: \"dev_${workspace.current_user.short_name}\"\n",
    "```\n",
    "\n",
    "#### What this does:\n",
    "When you deploy to dev, the job dab_demo_job will actually be created as dev_username_dab_demo_job. This allows every developer to have their own copy of the resources.\n",
    "\n",
    "### Source-Linked Deployment\n",
    "By default, mode: development enables source-linked deployment.\n",
    "\n",
    "1. True (Default): Jobs reference the notebook files directly in your Workspace/Rep folder. Changes to code take effect immediately without redeploying the bundle.\n",
    "2. False: The CLI uploads the files to a hidden .bundle folder. Jobs reference these isolated files.\n",
    "\n",
    "To force isolation (simulating production behavior in dev), set:\n",
    "\n",
    "```yaml\n",
    "targets:\n",
    "  dev:\n",
    "    presets:\n",
    "      source_linked_deployment: false\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803cfb53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# CLI Commands Reference\n",
    "# You can run these commands in the Databricks Web Terminal or your local terminal.\n",
    "\n",
    "# 1. Validate the bundle configuration (checks for syntax errors)\n",
    "# !databricks bundle validate -t dev\n",
    "\n",
    "# 2. Deploy the bundle to the 'dev' target\n",
    "# !databricks bundle deploy -t dev\n",
    "\n",
    "# 3. View the summary of what is deployed\n",
    "# !databricks bundle summary -t dev\n",
    "\n",
    "# 4. Run a specific job defined in the bundle\n",
    "# !databricks bundle run dab_demo_job -t dev\n",
    "\n",
    "# 5. Destroy/Cleanup resources created by the bundle\n",
    "# !databricks bundle destroy -t dev"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3154f39e",
   "metadata": {},
   "source": [
    "## 6. Deployment via UI\n",
    "\n",
    "If **\"Collaborate on Databricks Asset Bundles\"** is enabled in your Workspace Preview settings, you will see a **Deployments** tab (Rocket icon) in the left sidebar when inside a Bundle folder.\n",
    "\n",
    "1.  Click the **Deployments** icon.\n",
    "2.  Select the **Target** (e.g., `dev`).\n",
    "3.  Click **Deploy**.\n",
    "4.  You can also inspect variables and override them directly in the UI before deployment.\n",
    "\n",
    "### Note on Gitignore\n",
    "Ensure you add the hidden `.databricks` folder to your `.gitignore` file. This folder contains local state information about deployments and should not be committed to the repository.\n",
    "\n",
    "```text\n",
    "# .gitignore\n",
    ".databricks/\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9bb51f",
   "metadata": {},
   "source": [
    "## 7. Summary & Next Steps\n",
    "\n",
    "We have successfully:\n",
    "1.  Created a DABs project structure manually.\n",
    "2.  Configured `databricks.yml` with targets and permissions.\n",
    "3.  Defined Jobs, Pipelines, and Schemas as code.\n",
    "4.  Used Variables for environment-agnostic configuration.\n",
    "5.  Deployed to a `dev` environment using both source-linking and isolated bundle deployment.\n",
    "\n",
    "In the **next notebook**, we will take this to the next level by configuring an **Azure DevOps Pipeline** to automate the deployment of this bundle to a **QA environment** using CI/CD."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
