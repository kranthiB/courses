{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a79fb931",
   "metadata": {},
   "source": [
    "# Data Warehousing with PySpark: Project Overview\n",
    "\n",
    "## 1. Introduction\n",
    "In this project series, we are going to design and implement a Data Warehouse using **PySpark**. The goal is to simulate a real-world scenario where we move from raw data ingestion to analytics reporting.\n",
    "\n",
    "### Prerequisites\n",
    "*   Basic understanding of Data Warehousing concepts.\n",
    "*   Basic knowledge of Python and Apache Spark.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Problem Statement\n",
    "\n",
    "We are acting as data engineers for a **Retail Pet Food Company**.\n",
    "\n",
    "*   **Context:** The company has stores at multiple locations in India.\n",
    "*   **Current State:** The company is growing rapidly in size.\n",
    "*   **Business Requirement:** They want to determine specific Key Performance Indicators (KPIs) to understand their business performance better.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Business Requirements (KPIs)\n",
    "\n",
    "To address the business needs, our data warehouse pipeline must be able to answer the following questions:\n",
    "\n",
    "1.  **Sales Analysis:** Calculate sales per store, per day, and per month.\n",
    "2.  **Top Performers:** Identify the top-selling products per store in various geographic regions.\n",
    "3.  **Under-performers:** Identify the least-selling products per store in various geographic regions.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Technical Solution & Architecture\n",
    "\n",
    "### Data Source (Ingestion)\n",
    "The individual stores export their CRM data as flat files. These files are dumped into a shared location in a Data Lake.\n",
    "*   **Storage:** AWS S3 (Simple Storage Service).\n",
    "*   **Data Types:** \n",
    "    *   Orders Data\n",
    "    *   Customer Data\n",
    "    *   Store Data\n",
    "    *   Product Data\n",
    "\n",
    "### Data Processing (ETL)\n",
    "We will use **Apache Spark (PySpark)** to read the data from the Data Lake.\n",
    "*   **Transformation:** We will perform necessary cleaning, joining, and aggregation logic to calculate the KPIs.\n",
    "*   **Loading:** The processed data will be loaded into the Data Warehouse schema.\n",
    "\n",
    "### Architecture Flow\n",
    "1.  **Source:** Stores generate CSV/JSON files.\n",
    "2.  **Landing Zone:** Files land in AWS S3.\n",
    "3.  **Processing Layer:** PySpark reads from S3 -> Processes Data.\n",
    "4.  **Serving Layer:** Data is written to the Data Warehouse for Analytics Reporting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "227c9ed7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Environment Setup \n",
    "# Since this is the initialization notebook, let's verify our Spark environment is ready for the upcoming tasks.\n",
    "\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def init_spark():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"PetFood_DataWarehouse_Project\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    spark = init_spark()\n",
    "    print(f\"Spark Version: {spark.version}\")\n",
    "    print(\"Project Environment Initialized successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2077ef92",
   "metadata": {},
   "source": [
    "## 5. Next Steps\n",
    "\n",
    "In the upcoming notebooks, we will:\n",
    "1.  Set up the folder structure (mocking AWS S3 locally or connecting to actual S3).\n",
    "2.  Ingest the raw CRM data files (Orders, Customers, etc.).\n",
    "3.  Build the transformation logic to satisfy the KPIs defined above."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
