{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19f1ceab",
   "metadata": {},
   "source": [
    "# Local Environment Setup: PySpark with Docker\n",
    "\n",
    "## 1. Objective\n",
    "In this notebook, we will set up a robust Data Engineering environment on our local machine using **Docker**. This setup includes:\n",
    "*   **Jupyter Lab:** For writing and executing code.\n",
    "*   **Apache Spark:** For data processing.\n",
    "*   **Cluster Mode:** Setting up a Master node and Worker nodes to simulate a real distributed environment.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Prerequisites: Docker Desktop\n",
    "\n",
    "Before proceeding, ensure you have Docker Desktop installed.\n",
    "\n",
    "1.  Go to [Docker Hub](https://hub.docker.com/).\n",
    "2.  Search for **Docker Desktop**.\n",
    "3.  Download the version appropriate for your OS (Windows, Mac Intel, or Mac Apple Silicon).\n",
    "4.  Install and launch Docker Desktop.\n",
    "\n",
    "---\n",
    "\n",
    "## 3. Option A: Standalone Setup (Quick Start)\n",
    "\n",
    "If you want a simple, single-container setup without a cluster, follow these steps in your **Terminal/Command Prompt**:\n",
    "\n",
    "**1. Pull the Docker Image:**\n",
    "We will use the pre-configured image `self/pyspark-jupyter-lab-old` which contains Spark 3.3.0 and Python 3.7.\n",
    "\n",
    "```bash\n",
    "docker pull self/pyspark-jupyter-lab-old:latest\n",
    "```\n",
    "\n",
    "**2. Run the Container:**\n",
    "Map the necessary ports:\n",
    "\n",
    "> 4040: Spark UI\n",
    "\n",
    "> 8888: Jupyter Lab\n",
    "\n",
    "```bash\n",
    "docker run -p 4040:4040 -p 8888:8888 self/pyspark-jupyter-lab-old:latest\n",
    "```\n",
    "\n",
    "**3. Access Jupyter Lab:**\n",
    "\n",
    "> Look at the terminal logs for a URL containing a token (e.g., http://127.0.0.1:8888/lab?token=...).\n",
    "\n",
    "> Open that URL in your browser.\n",
    "\n",
    "---\n",
    "\n",
    "## 4. Option B: Cluster Setup (Recommended)\n",
    "\n",
    "To simulate a real production environment with a Master and Workers, we will use docker-compose.\n",
    "\n",
    "**1. Clone the Repository:**\n",
    "We will use the configuration provided in the ease-with-data GitHub repository.\n",
    "\n",
    "```bash\n",
    "# Clone the repo\n",
    "git clone https://github.com/subhamkharwal/docker-images.git\n",
    "\n",
    "# Navigate to the specific project folder\n",
    "cd docker-images/pyspark-cluster-with-jupyter\n",
    "```\n",
    "\n",
    "**2. Start the Cluster:**\n",
    "Run the following command to download images and start the containers (Master, Workers, History Server, Jupyter).\n",
    "\n",
    "```bash\n",
    "docker-compose up\n",
    "```\n",
    "\n",
    "**3. Access Points:**\n",
    "\n",
    "> Jupyter Lab: localhost:8888\n",
    "\n",
    "> Spark Master UI: localhost:8080\n",
    "\n",
    "> Spark Worker UI: localhost:8081\n",
    "\n",
    "**Note on Data Persistence:**\n",
    "To read local files in Cluster Mode, ensure your data is placed in the mounted volume folder. By default, the configuration maps your local directory to /data inside the container.\n",
    "\n",
    "---\n",
    "\n",
    "## 5. Environment Verification\n",
    "\n",
    "Once your Docker container is running and you have opened this notebook inside Jupyter Lab, run the following code cells to verify the installation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9afca223",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 2. Create Spark Session\n",
    "# If running in Cluster mode via the provided docker-compose, \n",
    "# the master URL is usually handled by the environment, \n",
    "# but for verification, 'local[*]' or the specific master URL can be used.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Setup_Verification\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67cd3907",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. Validation: Run a simple Spark Job\n",
    "# We will create a small DataFrame and display it to ensure the engine is working.\n",
    "\n",
    "try:\n",
    "    df = spark.range(10)\n",
    "    print(\"DataFrame created with 10 rows.\")\n",
    "    df.show()\n",
    "    print(\"Test Passed: PySpark is functioning correctly.\")\n",
    "except Exception as e:\n",
    "    print(f\"Test Failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5be3c1f",
   "metadata": {},
   "source": [
    "## 6. Check Spark UI\n",
    "\n",
    "1.  While the session above is active, open a new browser tab.\n",
    "2.  Go to [http://localhost:4040](http://localhost:4040).\n",
    "3.  You should see the Spark UI showing the job we just ran (`showString`).\n",
    "\n",
    "## 7. Clean Up\n",
    "\n",
    "Always stop your Spark session when done to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f7f7d29",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "spark.stop()\n",
    "print(\"Spark Session Stopped.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
