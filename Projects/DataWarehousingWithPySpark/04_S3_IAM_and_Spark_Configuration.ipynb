{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6f27167",
   "metadata": {},
   "source": [
    "# AWS S3, IAM, and Spark Configuration\n",
    "\n",
    "## 1. Overview\n",
    "In this notebook, we will configure the connectivity between our local PySpark environment (running in Docker) and the AWS Data Lake (S3).\n",
    "\n",
    "## 2. AWS Setup (Manual Steps)\n",
    "\n",
    "Before running the scripts below, you must perform the following actions in your AWS Console:\n",
    "\n",
    "### A. Create S3 Bucket & Folders\n",
    "1.  Go to **S3 Console**.\n",
    "2.  Create a bucket (e.g., `self-yourname`). *Bucket names must be globally unique.*\n",
    "3.  Inside the bucket, create a folder named `dw-with-pyspark`.\n",
    "4.  Inside `dw-with-pyspark`, create the following sub-folders:\n",
    "    *   `landing`\n",
    "    *   `archive`\n",
    "    *   `warehouse`\n",
    "\n",
    "### B. Configure IAM User\n",
    "1.  Go to **IAM Console**.\n",
    "2.  **Create Group:** Create a group (e.g., `S3DemoAccess`) and attach the policy **`AmazonS3FullAccess`**.\n",
    "3.  **Create User:** Create a user (e.g., `s3_pyspark_user`).\n",
    "4.  **Add to Group:** Add this user to the `S3DemoAccess` group.\n",
    "5.  **Generate Keys:**\n",
    "    *   Go to the user's **Security Credentials**.\n",
    "    *   Create an **Access Key** (select Command Line Interface usage).\n",
    "    *   **IMPORTANT:** Copy the `Access Key ID` and `Secret Access Key`. We will use them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "412b5003",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# ==========================================\n",
    "# INPUT YOUR AWS CONFIGURATION HERE\n",
    "# ==========================================\n",
    "aws_access_key = \"YOUR_ACCESS_KEY_ID\"\n",
    "aws_secret_key = \"YOUR_SECRET_ACCESS_KEY\"\n",
    "s3_bucket_name = \"YOUR_BUCKET_NAME\" \n",
    "\n",
    "# Define paths (Based on the Docker container structure)\n",
    "# Usually Spark is located at /spark or ~/spark inside these images\n",
    "spark_home = os.environ.get('SPARK_HOME', '/spark')\n",
    "spark_conf_dir = os.path.join(spark_home, 'conf')\n",
    "\n",
    "print(f\"Configuring for Bucket: {s3_bucket_name}\")\n",
    "print(f\"Spark Config Directory: {spark_conf_dir}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2db33c",
   "metadata": {},
   "source": [
    "## 3. Configure AWS Credentials\n",
    "We will create the `~/.aws/credentials` file. This allows libraries like `boto3` (and potentially Spark's Hadoop AWS module) to authenticate with AWS."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9030e6da",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create .aws directory\n",
    "aws_dir = os.path.expanduser('~/.aws')\n",
    "os.makedirs(aws_dir, exist_ok=True)\n",
    "\n",
    "# Define credentials content\n",
    "credentials_content = f\"\"\"[default]\n",
    "aws_access_key_id = {aws_access_key}\n",
    "aws_secret_access_key = {aws_secret_key}\n",
    "\"\"\"\n",
    "\n",
    "# Write file\n",
    "with open(os.path.join(aws_dir, 'credentials'), 'w') as f:\n",
    "    f.write(credentials_content)\n",
    "\n",
    "print(f\"AWS Credentials file created at {aws_dir}/credentials\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6f6557c",
   "metadata": {},
   "source": [
    "## 4. Configure `spark-defaults.conf`\n",
    "We need to tell Spark to:\n",
    "1.  Load the Delta Lake and Hadoop AWS libraries.\n",
    "2.  Use the Delta Catalog.\n",
    "3.  Use S3A FileSystem for `s3a://` schemes.\n",
    "4.  Set the default Data Warehouse location to our S3 bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446fedac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define the warehouse path\n",
    "warehouse_dir = f\"s3a://{s3_bucket_name}/dw-with-pyspark/warehouse\"\n",
    "\n",
    "spark_defaults_content = f\"\"\"\n",
    "# Default system properties included when running spark-submit.\n",
    "# This is useful for setting default environmental settings.\n",
    "\n",
    "spark.master                            local[*]\n",
    "spark.driver.memory                     4g\n",
    "spark.executor.memory                   4g\n",
    "\n",
    "# --- Packages ---\n",
    "# Note: Ensure these versions are compatible with the Spark version in your Docker image.\n",
    "# The video uses specific versions, but we generally use these for Spark 3.x:\n",
    "spark.jars.packages                     io.delta:delta-core_2.12:2.1.0,org.apache.hadoop:hadoop-aws:3.3.1\n",
    "\n",
    "# --- Delta Lake Configuration ---\n",
    "spark.sql.extensions                    io.delta.sql.DeltaSparkSessionExtension\n",
    "spark.sql.catalog.spark_catalog         org.apache.spark.sql.delta.catalog.DeltaCatalog\n",
    "\n",
    "# --- S3 Configuration ---\n",
    "spark.sql.warehouse.dir                 {warehouse_dir}\n",
    "spark.hadoop.fs.s3a.impl                org.apache.hadoop.fs.s3a.S3AFileSystem\n",
    "spark.hadoop.fs.s3a.access.key          {aws_access_key}\n",
    "spark.hadoop.fs.s3a.secret.key          {aws_secret_key}\n",
    "spark.hadoop.fs.s3a.endpoint            s3.amazonaws.com\n",
    "\"\"\"\n",
    "\n",
    "# Write the file\n",
    "with open(os.path.join(spark_conf_dir, 'spark-defaults.conf'), 'w') as f:\n",
    "    f.write(spark_defaults_content)\n",
    "\n",
    "print(\"spark-defaults.conf updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "468a7623",
   "metadata": {},
   "source": [
    "## 5. Configure `hive-site.xml`\n",
    "This file configures the Metastore. We are using a local Derby database for the metastore, but we want it to point to our S3 warehouse location."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89abd2fa",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "hive_site_content = f\"\"\"<configuration>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionURL</name>\n",
    "        <value>jdbc:derby:;databaseName=/home/jovyan/metastore_db;create=true</value>\n",
    "        <description>JDBC connect string for a JDBC metastore</description>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>javax.jdo.option.ConnectionDriverName</name>\n",
    "        <value>org.apache.derby.jdbc.EmbeddedDriver</value>\n",
    "        <description>Driver class name for a JDBC metastore</description>\n",
    "    </property>\n",
    "    <property>\n",
    "        <name>hive.metastore.warehouse.dir</name>\n",
    "        <value>{warehouse_dir}</value>\n",
    "        <description>location of default database for the warehouse</description>\n",
    "    </property>\n",
    "</configuration>\n",
    "\"\"\"\n",
    "\n",
    "# Write the file\n",
    "with open(os.path.join(spark_conf_dir, 'hive-site.xml'), 'w') as f:\n",
    "    f.write(hive_site_content)\n",
    "\n",
    "print(\"hive-site.xml created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66831877",
   "metadata": {},
   "source": [
    "## 6. Restart Kernel\n",
    "For these configurations to take effect, you **MUST restart your Spark Session**.\n",
    "1.  Stop the Kernel (Kernel -> Shut Down Kernel).\n",
    "2.  Refresh the page or start a new notebook to initialize a fresh SparkContext."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b638727c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 7. Verification\n",
    "# After restarting the kernel, run this cell to verify S3 access.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"S3 Connectivity Test\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created.\")\n",
    "\n",
    "# Test S3 Access\n",
    "# We will try to list files in the bucket (it might be empty, but it shouldn't error out)\n",
    "try:\n",
    "    sc = spark.sparkContext\n",
    "    # Using Hadoop FileSystem API via Spark\n",
    "    fs = sc._jvm.org.apache.hadoop.fs.FileSystem.get(sc._jsc.hadoopConfiguration())\n",
    "    path = sc._jvm.org.apache.hadoop.fs.Path(f\"s3a://{s3_bucket_name}/\")\n",
    "    exists = fs.exists(path)\n",
    "    print(f\"Connection Successful! Bucket '{s3_bucket_name}' exists: {exists}\")\n",
    "except Exception as e:\n",
    "    print(\"Error connecting to S3:\")\n",
    "    print(e)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
