{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "75ef3439",
   "metadata": {},
   "source": [
    "# ETL Architecture and Loading Strategy\n",
    "\n",
    "## 1. Introduction\n",
    "In this notebook, we define the architecture for our Data Warehouse. Based on the project design, we will implement a **Multi-Hop (Medallion) Architecture** consisting of three distinct layers. This structure ensures data quality, allows for restarting pipelines in case of failure (fault tolerance), and separates concerns between raw data ingestion and business logic.\n",
    "\n",
    "## 2. Architecture Overview\n",
    "\n",
    "The data will flow through the following layers:\n",
    "\n",
    "### Layer 1: Landing (Bronze)\n",
    "*   **Database Name:** `edw_ld`\n",
    "*   **Source:** Raw files from AWS S3 Data Lake.\n",
    "*   **Load Strategy:** **Append Mode**.\n",
    "*   **Data Transformation:** \n",
    "    *   No transformations or calculations.\n",
    "    *   No Joins.\n",
    "    *   **Schema:** All columns are read as **STRING** to prevent schema mismatch errors during ingestion.\n",
    "*   **Goal:** To keep a faithful copy of the source data as it arrived.\n",
    "\n",
    "### Layer 2: Staging (Silver)\n",
    "*   **Database Name:** `edw_stg`\n",
    "*   **Source:** Data from Landing Layer (`edw_ld`).\n",
    "*   **Load Strategy:** **Overwrite/Truncate Mode** (for each batch).\n",
    "*   **Data Transformation:**\n",
    "    *   Apply Schema (Cast columns to Integer, Date, etc.).\n",
    "    *   Perform **all major calculations** and business logic.\n",
    "    *   Perform basic joins required for transformation.\n",
    "*   **Goal:** To prepare clean, transformed data ready for the warehouse.\n",
    "\n",
    "### Layer 3: Data Warehouse (Gold)\n",
    "*   **Database Name:** `edw`\n",
    "*   **Source:** Data from Staging Layer (`edw_stg`).\n",
    "*   **Load Strategy:** **Upsert (Merge)** or Append.\n",
    "    *   Handles Slowly Changing Dimensions (SCD Type 1 or 2).\n",
    "*   **Data Transformation:**\n",
    "    *   Minimal calculations.\n",
    "    *   Joins to populate Surrogate Keys.\n",
    "*   **Goal:** Optimized tables for Analytics and Reporting.\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Job Control Mechanism\n",
    "To manage this flow, we will use a **Job Control Table**. This table will:\n",
    "*   Log the status of every pipeline run.\n",
    "*   Manage **Incremental Loading** (store the last processed timestamp or file ID).\n",
    "*   Allow the pipeline to determine if it needs to run a Full Load or Incremental Load.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4e7c5e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setup Spark Session\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark_session():\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(\"05_Architecture_Setup\") \\\n",
    "        .master(\"local[*]\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark Version: {spark.version}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbff8a52",
   "metadata": {},
   "source": [
    "## 4. Setting up the Data Lakehouse Layers\n",
    "\n",
    "We will now create the three databases in our Spark environment that correspond to the layers defined in our strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c75ab037",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define Database Names based on strategy\n",
    "databases = {\n",
    "    \"landing\": \"edw_ld\",\n",
    "    \"staging\": \"edw_stg\",\n",
    "    \"warehouse\": \"edw\"\n",
    "}\n",
    "\n",
    "# Create Databases\n",
    "def create_databases(spark, db_dict):\n",
    "    for layer, db_name in db_dict.items():\n",
    "        print(f\"Creating {layer} layer database: {db_name}...\")\n",
    "        spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "        print(f\"Database {db_name} created (or already exists).\")\n",
    "\n",
    "create_databases(spark, databases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc90645e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify Database Creation\n",
    "print(\"Listing all databases in the Spark Catalog:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee748d5f",
   "metadata": {},
   "source": [
    "## 5. Configuration Strategy\n",
    "\n",
    "To support the loading strategy mentioned (reading strings in landing, casting in staging), we will define a configuration structure in future notebooks. \n",
    "\n",
    "The flow for the upcoming implementation will be:\n",
    "1.  **Read Config:** Load file paths and schema definitions.\n",
    "2.  **Check Job Control:** Determine which files to read.\n",
    "3.  **Run Landing Job:** Ingest raw data to `edw_ld`.\n",
    "4.  **Run Staging Job:** Read `edw_ld`, transform, write to `edw_stg`.\n",
    "5.  **Run Warehouse Job:** Merge `edw_stg` into `edw`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
