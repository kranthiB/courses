{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3d3fe207",
   "metadata": {},
   "source": [
    "# Database & Table Initialization\n",
    "\n",
    "## 1. Overview\n",
    "In this notebook, we will lay the foundation for our Data Lakehouse. We will:\n",
    "1.  Initialize the **Spark Session** with Delta Lake support.\n",
    "2.  Create the necessary **Databases** (Layers).\n",
    "    *   `edw_ld`: Landing Layer (Raw Data)\n",
    "    *   `edw_stg`: Staging Layer (Intermediate Processing)\n",
    "    *   `edw`: Enterprise Data Warehouse (Final Consumption)\n",
    "3.  Create the **Dimension** and **Fact** tables using Delta format.\n",
    "4.  Create a **Job Control** table to track our pipeline executions.\n",
    "\n",
    "## 2. Spark Session Setup\n",
    "We need to ensure our Spark Session is configured to use the **Delta Lake** engine and Hive catalog support."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2debcfa9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "import os\n",
    "\n",
    "def get_spark_session(app_name=\"Init_Database\"):\n",
    "    \"\"\"\n",
    "    Creates a Spark Session with Delta Lake configurations.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .enableHiveSupport() \\\n",
    "        .getOrCreate()\n",
    "    return spark\n",
    "\n",
    "# Initialize Spark\n",
    "spark = get_spark_session()\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff955a21",
   "metadata": {},
   "source": [
    "## 3. Create Database Layers\n",
    "\n",
    "We will create three distinct databases to organize our data lifecycle:\n",
    "1.  **Landing (`edw_ld`):** Where raw files are mapped initially.\n",
    "2.  **Staging (`edw_stg`):** Temporary storage for transformations.\n",
    "3.  **EDW (`edw`):** The final modeled data (Star Schema)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af9945a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Databases\n",
    "databases = [\"edw\", \"edw_stg\", \"edw_ld\"]\n",
    "\n",
    "for db in databases:\n",
    "    print(f\"Creating Database: {db}\")\n",
    "    spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db}\")\n",
    "\n",
    "# Verify creation\n",
    "spark.sql(\"SHOW DATABASES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84605a6c",
   "metadata": {},
   "source": [
    "## 4. Create Dimension Tables (`edw` layer)\n",
    "\n",
    "We will now create the tables in the `edw` database. These tables are **Managed Tables** stored in Delta format.\n",
    "\n",
    "### A. Store Dimension (`dim_store`)\n",
    "Contains details about the retail store locations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1ac1808",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Store Dimension\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.dim_store (\n",
    "    row_wid STRING,\n",
    "    store_id STRING,\n",
    "    store_name STRING,\n",
    "    address STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    zip_code STRING,\n",
    "    phone_number STRING,\n",
    "    manager_name STRING,\n",
    "    insert_dt TIMESTAMP,\n",
    "    update_dt TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"dim_store created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82715ea5",
   "metadata": {},
   "source": [
    "### B. Product Dimension (`dim_product`)\n",
    "Contains product catalog information. We will use **SCD Type 2** logic here (implied by `effective_start_dt`, `effective_end_dt`, and `active_flg`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a001568f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Product Dimension\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.dim_product (\n",
    "    row_wid STRING,\n",
    "    product_id STRING,\n",
    "    product_name STRING,\n",
    "    brand STRING,\n",
    "    category STRING,\n",
    "    unit_price DOUBLE,\n",
    "    size STRING,\n",
    "    uom STRING,\n",
    "    image_url STRING,\n",
    "    effective_start_dt TIMESTAMP,\n",
    "    effective_end_dt TIMESTAMP,\n",
    "    active_flg INT,\n",
    "    insert_dt TIMESTAMP,\n",
    "    update_dt TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"dim_product created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd77bcb0",
   "metadata": {},
   "source": [
    "### C. Customer Dimension (`dim_customer`)\n",
    "Contains customer details. This is also modeled for **SCD Type 2** to track address or email changes over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c4f488b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Customer Dimension\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.dim_customer (\n",
    "    row_wid STRING,\n",
    "    customer_id STRING,\n",
    "    first_name STRING,\n",
    "    last_name STRING,\n",
    "    address STRING,\n",
    "    city STRING,\n",
    "    state STRING,\n",
    "    zip_code STRING,\n",
    "    phone_number STRING,\n",
    "    email STRING,\n",
    "    plan_type_id STRING,\n",
    "    effective_start_dt TIMESTAMP,\n",
    "    effective_end_dt TIMESTAMP,\n",
    "    active_flg INT,\n",
    "    insert_dt TIMESTAMP,\n",
    "    update_dt TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"dim_customer created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f27662b",
   "metadata": {},
   "source": [
    "## 5. Create Fact Table\n",
    "\n",
    "### Sales Fact (`fact_sales`)\n",
    "This table holds the transactional data. It links to dimensions via the `_wid` (Warehouse ID) columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0adb0ad2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Sales Fact Table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.fact_sales (\n",
    "    row_wid STRING,\n",
    "    date_id STRING,\n",
    "    store_wid STRING,\n",
    "    product_wid STRING,\n",
    "    customer_wid STRING,\n",
    "    order_id STRING,\n",
    "    invoice_num STRING,\n",
    "    qty INT,\n",
    "    tax DOUBLE,\n",
    "    line_total DOUBLE,\n",
    "    insert_dt TIMESTAMP,\n",
    "    update_dt TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"fact_sales created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b3c89a0",
   "metadata": {},
   "source": [
    "## 6. Infrastructure Tables\n",
    "\n",
    "### Job Control Table (`job_control`)\n",
    "This table is crucial for our ETL pipelines. It tracks the status of data loads (e.g., \"Success\", \"Failed\") and the high-water mark (`max_timestamp`) to support incremental loading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdcadf63",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create Job Control Table\n",
    "spark.sql(\"\"\"\n",
    "CREATE TABLE IF NOT EXISTS edw.job_control (\n",
    "    job_id STRING,\n",
    "    job_name STRING,\n",
    "    status STRING,\n",
    "    max_timestamp TIMESTAMP,\n",
    "    rundate STRING,\n",
    "    insert_dt TIMESTAMP\n",
    ") USING DELTA\n",
    "\"\"\")\n",
    "\n",
    "print(\"job_control created successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac59d737",
   "metadata": {},
   "source": [
    "## 7. Verification\n",
    "\n",
    "Let's list all the tables in our `edw` database to confirm everything is set up correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "496732eb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify Tables\n",
    "print(\"Tables in EDW:\")\n",
    "spark.sql(\"SHOW TABLES IN edw\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
