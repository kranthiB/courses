{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b9da994a",
   "metadata": {},
   "source": [
    "# Utility Functions & Project Libraries\n",
    "\n",
    "## 1. Overview\n",
    "Before we begin ingesting data, we need to set up a framework to handle:\n",
    "1.  **Job Control:** Tracking which data has been loaded (Full vs. Incremental load) using a MySQL/Delta table log.\n",
    "2.  **Configuration:** Managing the \"Run Date\" (Batch Date).\n",
    "3.  **Common Utilities:** Generating date data, casting columns, etc.\n",
    "4.  **AWS S3 Management:** Moving files from \"Landing\" to \"Archive\" after processing.\n",
    "\n",
    "In this notebook, we will create the necessary Python library files in a `lib` folder.\n",
    "\n",
    "---\n",
    "\n",
    "## 2. Setup Library Folder\n",
    "First, let's create a directory to store our helper scripts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e119562d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a 'lib' directory if it doesn't exist\n",
    "if not os.path.exists('lib'):\n",
    "    os.makedirs('lib')\n",
    "    print(\"Created 'lib' directory.\")\n",
    "else:\n",
    "    print(\"'lib' directory already exists.\")\n",
    "    \n",
    "# Create an empty __init__.py to make it a package\n",
    "with open('lib/__init__.py', 'w') as f:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c60ef",
   "metadata": {},
   "source": [
    "## 3. Configuration (Run Date)\n",
    "\n",
    "We need a way to tell our pipeline what \"Business Date\" or \"Batch Date\" we are processing. We will use a simple text file `run_config.txt` (or JSON) to store this.\n",
    "\n",
    "*   **Format:** `YYYYMMDD`\n",
    "*   **Initial Full Load Date:** `20220101`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9bb2488",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lib/run_config.txt\n",
    "{\"rundate\": \"20220101\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f55d5aa",
   "metadata": {},
   "source": [
    "## 4. General Utilities (`utils.py`)\n",
    "\n",
    "This script will contain general helper functions:\n",
    "1.  **`get_rundate`**: Reads the config file created above.\n",
    "2.  **`get_string_cols`**: A helper to cast all columns in a DataFrame to String (useful for raw ingestion).\n",
    "3.  **`date_data`**: A specific function to generate Date Dimension data (since we don't have a source file for dates)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09273da6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lib/utils.py\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.functions import col, lit, concat\n",
    "import json\n",
    "\n",
    "def get_rundate():\n",
    "    \"\"\"Reads the Run Date from the config file\"\"\"\n",
    "    try:\n",
    "        with open(\"lib/run_config.txt\", \"r\") as f:\n",
    "            config = json.load(f)\n",
    "            return config.get(\"rundate\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading config: {e}\")\n",
    "        return None\n",
    "\n",
    "def get_string_cols(spark, df):\n",
    "    \"\"\"Casts all columns to String type\"\"\"\n",
    "    return [col(c).cast(\"string\").alias(c) for c in df.columns]\n",
    "\n",
    "def date_data(spark, start_run_dt, num_years=1):\n",
    "    \"\"\"\n",
    "    Generates a sequence of dates for the Date Dimension.\n",
    "    \"\"\"\n",
    "    data = []\n",
    "    start_date = datetime.strptime(str(start_run_dt), \"%Y%m%d\")\n",
    "    \n",
    "    # Generate data for X years\n",
    "    for i in range(0, 365 * num_years):\n",
    "        next_date = start_date + timedelta(days=i)\n",
    "        data.append((\n",
    "            next_date.strftime(\"%Y-%m-%d\"), # date\n",
    "            next_date.strftime(\"%Y%m%d\"),   # date_key\n",
    "            next_date.strftime(\"%Y\"),       # year\n",
    "            next_date.strftime(\"%m\"),       # month\n",
    "            next_date.strftime(\"%d\")        # day\n",
    "        ))\n",
    "\n",
    "    schema = [\"full_date\", \"date_key\", \"year\", \"month\", \"day\"]\n",
    "    return spark.createDataFrame(data, schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc5d50",
   "metadata": {},
   "source": [
    "## 5. Job Control (`job_control.py`)\n",
    "\n",
    "This is the most critical logic for Data Warehousing. This script manages the **Job Control Table**.\n",
    "\n",
    "**Table Schema:**\n",
    "*   `schema_name`: Database name.\n",
    "*   `table_name`: Table name.\n",
    "*   `max_timestamp`: The maximum timestamp of data loaded.\n",
    "*   `rundate`: The batch date.\n",
    "\n",
    "**Key Logic:**\n",
    "*   **`get_max_timestamp`**: Checks the log table. \n",
    "    *   If a record exists, return the max timestamp (Incremental Load).\n",
    "    *   If **NO** record exists, return `1900-01-01 00:00:00` (Default High Watermark for Full Load).\n",
    "*   **`insert_log`**: Updates the table after a successful load."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72c6c3d7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lib/job_control.py\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import max, lit, current_timestamp\n",
    "\n",
    "def get_max_timestamp(spark, schema_name, table_name):\n",
    "    \"\"\"\n",
    "    Gets the max timestamp for a specific table to determine incremental logic.\n",
    "    Returns '1900-01-01' if no history exists.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Read the Job Control Table (We will create this table in later videos, assuming it exists or handling error)\n",
    "        # Note: In a real scenario, handle the \"Table not found\" exception for the very first run\n",
    "        df = spark.read.format(\"delta\").load(f\"spark-warehouse/job_control\")\n",
    "        \n",
    "        # Filter for specific table\n",
    "        df_filtered = df.filter((df.schema_name == schema_name) & (df.table_name == table_name))\n",
    "        \n",
    "        if df_filtered.count() > 0:\n",
    "            max_ts = df_filtered.agg(max(\"max_timestamp\")).collect()[0][0]\n",
    "            return str(max_ts)\n",
    "        else:\n",
    "            return \"1900-01-01 00:00:00\"\n",
    "            \n",
    "    except Exception as e:\n",
    "        # If table doesn't exist yet (First run ever), return default\n",
    "        return \"1900-01-01 00:00:00\"\n",
    "\n",
    "def insert_log(spark, schema_name, table_name, max_timestamp, rundate):\n",
    "    \"\"\"Logs the execution status\"\"\"\n",
    "    try:\n",
    "        data = [{\n",
    "            \"schema_name\": schema_name,\n",
    "            \"table_name\": table_name,\n",
    "            \"max_timestamp\": str(max_timestamp),\n",
    "            \"rundate\": str(rundate),\n",
    "            \"insert_dt\": current_timestamp()\n",
    "        }]\n",
    "        \n",
    "        # Determine schema implicitly or define explicit schema\n",
    "        df = spark.createDataFrame(data)\n",
    "        \n",
    "        # Write to Job Control table\n",
    "        df.write.format(\"delta\").mode(\"append\").save(f\"spark-warehouse/job_control\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error logging job: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b84445b",
   "metadata": {},
   "source": [
    "## 6. AWS S3 Utilities (`aws_s3.py`)\n",
    "\n",
    "This script handles file movements. After we process a CSV file from the \"Landing\" zone, we don't want to process it again. We move it to an \"Archive\" folder.\n",
    "\n",
    "*   **Prerequisite:** Requires `boto3` library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fd16f90",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lib/aws_s3.py\n",
    "import boto3\n",
    "\n",
    "def archive_landing_object(bucket_name, file_key):\n",
    "    \"\"\"\n",
    "    Moves a file from Landing to Archive within S3.\n",
    "    Structure: s3://bucket/landing/file.csv -> s3://bucket/archive/file.csv\n",
    "    \"\"\"\n",
    "    s3 = boto3.client('s3')\n",
    "    \n",
    "    try:\n",
    "        # Define Copy Source\n",
    "        copy_source = {'Bucket': bucket_name, 'Key': file_key}\n",
    "        \n",
    "        # Define New Key (Replace 'landing' with 'archive')\n",
    "        new_key = file_key.replace(\"landing\", \"archive\")\n",
    "        \n",
    "        # Copy Object\n",
    "        s3.copy_object(Bucket=bucket_name, CopySource=copy_source, Key=new_key)\n",
    "        \n",
    "        # Delete Original Object\n",
    "        s3.delete_object(Bucket=bucket_name, Key=file_key)\n",
    "        \n",
    "        print(f\"Archived: {file_key}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error archiving file {file_key}: {e}\")\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f19d6f2",
   "metadata": {},
   "source": [
    "## 7. Spark Session Wrapper (`spark_session.py`)\n",
    "\n",
    "A simple utility to ensure we always create the Spark Session with the necessary configurations (like Delta Lake support and S3 connectivity) consistent across all notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a450d3b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile lib/spark_session.py\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "def get_spark_session(app_name=\"PetFood_DW\"):\n",
    "    spark = SparkSession.builder \\\n",
    "        .appName(app_name) \\\n",
    "        .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.2.0,org.apache.hadoop:hadoop-aws:3.3.2\") \\\n",
    "        .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "        .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "        .getOrCreate()\n",
    "    return spark"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe9d7bd7",
   "metadata": {},
   "source": [
    "## 8. Verification\n",
    "\n",
    "Let's verify that our files were created successfully in the `lib` folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87a11643",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "print(\"Files in 'lib' directory:\")\n",
    "print(os.listdir('lib'))"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
