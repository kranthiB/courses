{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "27a8b207",
   "metadata": {},
   "source": [
    "# Date Dimension - Landing Data Load\n",
    "\n",
    "## Overview\n",
    "In this notebook, we perform the **Landing Data Load** for the Date Dimension. \n",
    "\n",
    "### The Landing Layer Pattern\n",
    "The Landing Layer (or Raw Layer) serves as the entry point for data into the Data Lakehouse. The standard process involves:\n",
    "1.  **Reading Source:** Reading incremental files or generating data.\n",
    "2.  **Casting to String:** Converting all columns to String type. This prevents job failures due to data type mismatches from the source, allowing us to handle type casting in the Staging layer.\n",
    "3.  **Audit Columns:** Adding `insert_dt` (timestamp) and `rundate` to track lineage.\n",
    "4.  **Write Strategy:** Writing data in **Append Mode** (usually) to maintain history, or Overwrite for full reloads.\n",
    "5.  **Job Control:** Updating the control table with the status.\n",
    "\n",
    "### Specifics for Date Dimension\n",
    "Unlike other tables that read from files, the **Date Dimension** is generated programmatically using a utility function. \n",
    "*   **Target Table:** `dim_date_ld`\n",
    "*   **Run Date:** `20220101` (Full Load scenario)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79af330d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "# Assuming we have our project utility modules (as per the video context)\n",
    "# In a real setup, these would be in a separate .py file available in the path\n",
    "from lib.utils import get_spark_session, get_run_date, spark_create_date_data\n",
    "from lib.job_control import insert_log, get_max_timestamp\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Date Landing Load\")\n",
    "\n",
    "# 1. Set Job Parameters\n",
    "# For this tutorial, we simulate a Full Load with a specific run date\n",
    "# In production, this might come from a config file or airflow argument\n",
    "run_date = \"20220101\" \n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2264ccfe",
   "metadata": {},
   "source": [
    "## 1. Generate Data\n",
    "We use the utility function `spark_create_date_data` to generate 2 years of date data based on the `run_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f58d8af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define columns required for Date Landing\n",
    "# Note: In Landing Layer, we treat everything as String initially\n",
    "date_schema_cols = [\"date\", \"day\", \"month\", \"year\", \"day_of_week\"]\n",
    "\n",
    "# Generate Data\n",
    "# The utility function generates date records. \n",
    "# We ask for 2 years of data starting from the run_date.\n",
    "df_raw = spark_create_date_data(spark, run_date, 2)\n",
    "\n",
    "print(\"Raw Data Schema:\")\n",
    "df_raw.printSchema()\n",
    "\n",
    "print(f\"Raw Data Count: {df_raw.count()}\")\n",
    "df_raw.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a4f4b2",
   "metadata": {},
   "source": [
    "## 2. Transformation: Cast to String & Add Audit Columns\n",
    "For the Landing Layer, we standardize the schema by casting all business columns to String. We then append the audit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4af5b1b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Cast all columns to String\n",
    "# This is a defensive programming practice for the raw layer\n",
    "df_casted = df_raw.select([col(c).cast(\"string\") for c in df_raw.columns])\n",
    "\n",
    "# 2. Add Audit Columns\n",
    "# insert_dt: Current timestamp of load\n",
    "# rundate: The business date for which the job is running\n",
    "df_final = df_casted.withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "                    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "print(\"Final Landing Data Schema:\")\n",
    "df_final.printSchema()\n",
    "df_final.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "153fb34c",
   "metadata": {},
   "source": [
    "## 3. Write Data to Landing Layer\n",
    "We implement a check using the **Job Control Table**.\n",
    "*   **First Run:** If the max timestamp is the default low value (`1900-01-01`), it implies the table is empty. We use **Overwrite** mode.\n",
    "*   **Subsequent Runs:** If data exists, we use **Append** mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb9cdea7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Table Configuration\n",
    "schema_name = \"pyspark_warehouse\" # Database name\n",
    "table_name = \"dim_date_ld\"\n",
    "table_full_name = f\"{schema_name}.{table_name}\"\n",
    "landing_path = f\"s3a://warehouse/landing/{table_name}\" # Example Path\n",
    "\n",
    "# Check Job Control to decide Write Mode\n",
    "# get_max_timestamp returns \"1900-01-01 00:00:00\" if no entry exists\n",
    "max_timestamp = get_max_timestamp(spark, schema_name, table_name)\n",
    "\n",
    "print(f\"Max Timestamp in Job Control: {max_timestamp}\")\n",
    "\n",
    "if max_timestamp == \"1900-01-01 00:00:00.000000\":\n",
    "    print(\"No previous load found. Mode: OVERWRITE\")\n",
    "    write_mode = \"overwrite\"\n",
    "else:\n",
    "    print(\"Previous load found. Mode: APPEND\")\n",
    "    write_mode = \"append\"\n",
    "\n",
    "# Write Data\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(write_mode) \\\n",
    "    .saveAsTable(table_full_name)\n",
    "\n",
    "print(f\"Data successfully written to {table_full_name} in {write_mode} mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d832d7e8",
   "metadata": {},
   "source": [
    "## 4. Update Job Control & Generate Manifest\n",
    "Finally, we log the success of the job and generate the Symlink Manifest file (required if we want to query Delta Lake tables via AWS Athena)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de4ec1ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Update Job Control Table\n",
    "# We log the schema, table, max insert_dt, and record counts\n",
    "insert_log(spark, schema_name, table_name, df_final.count(), run_date)\n",
    "\n",
    "print(\"Job Control Table updated.\")\n",
    "\n",
    "# 2. Generate Symlink Manifest\n",
    "# This allows external engines like Athena/Presto to read the Delta table\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {table_full_name}\")\n",
    "\n",
    "print(\"Symlink Manifest generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfc1bd58",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Validation\n",
    "# Check if data is readable\n",
    "spark.sql(f\"SELECT * FROM {table_full_name} LIMIT 5\").show()\n",
    "\n",
    "# Verify Job Control Entry\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT * FROM {schema_name}.job_control \n",
    "    WHERE table_name = '{table_name}' \n",
    "    ORDER BY insert_dt DESC LIMIT 1\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
