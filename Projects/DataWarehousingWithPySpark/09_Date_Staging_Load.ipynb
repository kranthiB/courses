{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea04612c",
   "metadata": {},
   "source": [
    "# Date Dimension - Staging Data Load\n",
    "\n",
    "## Overview\n",
    "In this notebook, we move data from the **Landing Layer** to the **Staging Layer**.\n",
    "\n",
    "### The Staging Layer Pattern\n",
    "The Staging Layer serves as an intermediate processing zone where data is cleaned, de-duplicated, and typed before entering the Data Warehouse (Dimension/Fact) tables.\n",
    "\n",
    "### Steps involved:\n",
    "1.  **Read Incremental Data:** We query the **Job Control Table** to find the `max_timestamp` of the previous load. We then read only the records from the Landing Layer that were inserted *after* this timestamp.\n",
    "2.  **De-duplication:** We handle potential duplicate data arrival. We group by the **Natural Key** (in this case, `date`) and keep the record with the latest `insert_dt`.\n",
    "3.  **Type Casting:** In Landing, all columns were strings. Here, we cast them to their correct types (Date, Integer, etc.).\n",
    "4.  **Audit Columns:** We add `insert_dt` and `update_dt` to track when records are processed in this layer.\n",
    "5.  **Write Strategy:** We write to the Staging table in **Overwrite** mode."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51bba34f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Assuming utility modules are available\n",
    "from lib.utils import get_spark_session, get_run_date\n",
    "from lib.job_control import insert_log, get_max_timestamp\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Date Staging Load\")\n",
    "\n",
    "# 1. Job Parameters\n",
    "run_date = \"20220101\" \n",
    "schema_name = \"pyspark_warehouse\"\n",
    "\n",
    "# Source (Landing) and Target (Staging) configuration\n",
    "src_table = f\"{schema_name}.dim_date_ld\"\n",
    "tgt_table = \"dim_date_stg\"\n",
    "tgt_table_full = f\"{schema_name}.{tgt_table}\"\n",
    "\n",
    "print(f\"Reading from: {src_table}\")\n",
    "print(f\"Writing to:   {tgt_table_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae0cc9e",
   "metadata": {},
   "source": [
    "## 1. Determine High Watermark\n",
    "We check the `job_control` table to see when the `dim_date_stg` table was last loaded. This helps us define the cut-off point for reading from the Landing layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b50c5b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Get the Max Timestamp (High Watermark)\n",
    "# If first run, returns 1900-01-01\n",
    "max_timestamp = get_max_timestamp(spark, schema_name, tgt_table)\n",
    "\n",
    "print(f\"High Watermark (Max Timestamp): {max_timestamp}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ccf80f3",
   "metadata": {},
   "source": [
    "## 2. Read Incremental Data\n",
    "We read data from the Landing Layer where the `insert_dt` is greater than our High Watermark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ca9e88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read from Landing Table\n",
    "df_landing = spark.read.table(src_table)\n",
    "\n",
    "# Filter for incremental data\n",
    "df_incremental = df_landing.filter(col(\"insert_dt\") > max_timestamp)\n",
    "\n",
    "print(f\"Records fetched from Landing: {df_incremental.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd21c204",
   "metadata": {},
   "source": [
    "## 3. De-duplication Logic\n",
    "To ensure data quality, we remove duplicates based on the Natural Key.\n",
    "*   **Partition By:** `date` (Natural Key)\n",
    "*   **Order By:** `insert_dt DESC` (Prioritize latest data)\n",
    "*   **Logic:** Assign a `row_number` and keep only `rn == 1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b6d7af4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Define Window Spec\n",
    "window_spec = Window.partitionBy(\"date\").orderBy(col(\"insert_dt\").desc())\n",
    "\n",
    "# Calculate Row Number\n",
    "df_deduped_prep = df_incremental.withColumn(\"rn\", row_number().over(window_spec))\n",
    "\n",
    "# Filter only the latest record per key\n",
    "df_deduped = df_deduped_prep.filter(col(\"rn\") == 1).drop(\"rn\")\n",
    "\n",
    "print(f\"Count after De-duplication: {df_deduped.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17c4b0cc",
   "metadata": {},
   "source": [
    "## 4. Transformation & Type Casting\n",
    "Convert the raw string columns to proper business data types and add audit information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "576ce7e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Transformations\n",
    "df_stg = df_deduped \\\n",
    "    .withColumn(\"date\", to_date(col(\"date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"day\", col(\"day\").cast(\"integer\")) \\\n",
    "    .withColumn(\"month\", col(\"month\").cast(\"integer\")) \\\n",
    "    .withColumn(\"year\", col(\"year\").cast(\"integer\")) \\\n",
    "    .withColumn(\"day_of_week\", col(\"day_of_week\").cast(\"string\")) \\\n",
    "    .withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"update_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "# Select specific columns to ensure schema order\n",
    "final_cols = [\"date\", \"day\", \"month\", \"year\", \"day_of_week\", \n",
    "              \"insert_dt\", \"update_dt\", \"rundate\"]\n",
    "\n",
    "df_final = df_stg.select(final_cols)\n",
    "\n",
    "print(\"Staging Schema:\")\n",
    "df_final.printSchema()\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9f285c",
   "metadata": {},
   "source": [
    "## 5. Write to Staging\n",
    "We write to the staging table in **Overwrite** mode. The Staging layer usually holds the \"delta\" or the current batch of data being processed before it merges into the final Dimension/Fact."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ae35ea0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write Data\n",
    "df_final.write \\\n",
    "    .format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(tgt_table_full)\n",
    "\n",
    "print(f\"Data written to {tgt_table_full} in OVERWRITE mode.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "202f9c40",
   "metadata": {},
   "source": [
    "## 6. Job Logging & Manifest\n",
    "Update the control table and generate the manifest for Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd3cc6f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Update Job Control\n",
    "insert_log(spark, schema_name, tgt_table, df_final.count(), run_date)\n",
    "print(\"Job Control updated.\")\n",
    "\n",
    "# 2. Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {tgt_table_full}\")\n",
    "print(\"Manifest generated.\")\n",
    "\n",
    "# 3. Validation\n",
    "spark.sql(f\"SELECT * FROM {tgt_table_full} LIMIT 5\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
