{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5eeb341b",
   "metadata": {},
   "source": [
    "# Date Dimension - Data Warehouse Load (SCD Type 1)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we move data from the **Staging Layer** to the final **Data Warehouse (Dimension)** layer.\n",
    "\n",
    "### The Dimension Load Pattern (SCD Type 1)\n",
    "We are implementing a Slowly Changing Dimension Type 1 (Overwriting old history with new state) strategy.\n",
    "\n",
    "### Steps involved:\n",
    "1.  **Read Staging Data:** Read all data from the Staging table (as it acts as a queue/buffer for the latest batch).\n",
    "2.  **Surrogate Key Generation:** Create a unique identifier (`row_wid`) for the dimension. For the Date dimension, we will generate a \"Smart Key\" based on the date format (e.g., 20220101).\n",
    "3.  **Transformation:** Ensure data types align with the final schema.\n",
    "4.  **Write Strategy (Upsert/Merge):** \n",
    "    *   **Check Load Type:** If this is the very first run (Full Load), we truncate the target table to ensure a clean slate.\n",
    "    *   **Merge:** Use Delta Lake's `MERGE INTO` command to match records based on the Natural Key (`date`). \n",
    "        *   **Matched:** Update the record.\n",
    "        *   **Not Matched:** Insert the record.\n",
    "5.  **Job Control & Manifest:** Update logs and generate the manifest file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7de0720",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Assuming utility modules are available\n",
    "from lib.utils import get_spark_session, get_run_date\n",
    "from lib.job_control import insert_log, get_max_timestamp\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = get_spark_session(\"Date Dimension Load\")\n",
    "\n",
    "# 1. Job Parameters\n",
    "# For this tutorial, we act as if it is a Full Load\n",
    "run_date = \"20220101\" \n",
    "schema_name = \"pyspark_warehouse\"\n",
    "\n",
    "# Source (Staging) and Target (Dimension) configuration\n",
    "src_table = f\"{schema_name}.dim_date_stg\"\n",
    "tgt_table = \"dim_date\"\n",
    "tgt_table_full = f\"{schema_name}.{tgt_table}\"\n",
    "\n",
    "print(f\"Reading from: {src_table}\")\n",
    "print(f\"Writing to:   {tgt_table_full}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b35da33",
   "metadata": {},
   "source": [
    "## 1. Read Staging Data\n",
    "We read the data prepared in the previous step. Since Staging is always truncated/overwritten in our pipeline, we read the whole table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fff389ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read from Staging\n",
    "df_stg = spark.read.table(src_table)\n",
    "\n",
    "print(f\"Staging Data Count: {df_stg.count()}\")\n",
    "df_stg.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90789492",
   "metadata": {},
   "source": [
    "## 2. Generate Surrogate Keys & Transform\n",
    "For a Date Dimension, a common practice for the Surrogate Key (`row_wid`) is to use the integer representation of the date (e.g., `20220101`).\n",
    "\n",
    "We also ensure the column names and types match the target table definition."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd558ef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Generate Surrogate Key (row_wid)\n",
    "# We cast the date column to format 'yyyyMMdd' and then to Integer/Long\n",
    "df_dim_temp = df_stg.withColumn(\"row_wid\", date_format(col(\"date\"), \"yyyyMMdd\").cast(\"long\"))\n",
    "\n",
    "# Select and Reorder columns to match Target Schema\n",
    "final_cols = [\n",
    "    \"row_wid\", \n",
    "    \"date\", \n",
    "    \"day\", \n",
    "    \"month\", \n",
    "    \"year\", \n",
    "    \"day_of_week\", \n",
    "    \"rundate\", \n",
    "    \"insert_dt\", \n",
    "    \"update_dt\"\n",
    "]\n",
    "\n",
    "df_final = df_dim_temp.select(final_cols)\n",
    "\n",
    "print(\"Final Dimension Data Preview:\")\n",
    "df_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3085dc89",
   "metadata": {},
   "source": [
    "## 3. Handling Full Load vs Incremental\n",
    "We check the **Job Control Table**. \n",
    "*   If `max_timestamp` is the default low value, it implies the dimension is empty or this is a forced Full Load.\n",
    "*   **Action:** If Full Load, we `TRUNCATE` the target Delta table and run `VACUUM` to clean up old files immediately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b75126f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check High Watermark\n",
    "max_timestamp = get_max_timestamp(spark, schema_name, tgt_table)\n",
    "print(f\"Max Timestamp: {max_timestamp}\")\n",
    "\n",
    "# Logic for Initial Load\n",
    "if max_timestamp == \"1900-01-01 00:00:00.000000\":\n",
    "    print(\"Full Load Detected. Truncating Target Table...\")\n",
    "    \n",
    "    # Initialize DeltaTable object\n",
    "    dt_target = DeltaTable.forName(spark, tgt_table_full)\n",
    "    \n",
    "    # Truncate (Delete all rows)\n",
    "    dt_target.delete() \n",
    "    \n",
    "    # Vacuum to remove physical files (retention check disabled for immediate effect)\n",
    "    spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "    dt_target.vacuum(0) \n",
    "    \n",
    "    print(\"Target Table Truncated and Vacuumed.\")\n",
    "else:\n",
    "    print(\"Incremental Load Detected.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0fec7eb7",
   "metadata": {},
   "source": [
    "## 4. Upsert (Merge) Data\n",
    "We use the **Delta Merge** operation to implement SCD Type 1.\n",
    "*   **Join Condition:** `target.date = source.date` (Natural Key)\n",
    "*   **When Matched:** Update all columns.\n",
    "*   **When Not Matched:** Insert the new row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec530bd9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize DeltaTable for Target\n",
    "dt_target = DeltaTable.forName(spark, tgt_table_full)\n",
    "\n",
    "# Define the Merge\n",
    "dt_target.alias(\"target\") \\\n",
    "    .merge(\n",
    "        df_final.alias(\"source\"),\n",
    "        \"target.date = source.date\" # Natural Key Join\n",
    "    ) \\\n",
    "    .whenMatchedUpdateAll() \\\n",
    "    .whenNotMatchedInsertAll() \\\n",
    "    .execute()\n",
    "\n",
    "print(\"Merge (Upsert) completed successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a383ed",
   "metadata": {},
   "source": [
    "## 5. Job Logging & Manifest\n",
    "Record the execution stats and update the Athena manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f1f3151",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Update Job Control\n",
    "# Note: For Merge operations, obtaining the exact count of inserted/updated rows \n",
    "# requires parsing Merge metrics. For simplicity here, we log the source count.\n",
    "insert_log(spark, schema_name, tgt_table, df_final.count(), run_date)\n",
    "print(\"Job Control updated.\")\n",
    "\n",
    "# 2. Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {tgt_table_full}\")\n",
    "print(\"Manifest generated.\")\n",
    "\n",
    "# 3. Final Validation\n",
    "print(\"Data in Final Dimension:\")\n",
    "spark.sql(f\"SELECT * FROM {tgt_table_full} ORDER BY date LIMIT 5\").show()\n",
    "\n",
    "print(\"Job Control Status:\")\n",
    "spark.sql(f\"SELECT * FROM {schema_name}.job_control WHERE table_name='{tgt_table}' ORDER BY insert_dt DESC LIMIT 1\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
