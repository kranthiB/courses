{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "41018af3",
   "metadata": {},
   "source": [
    "# AWS Athena Integration: Symlink Manifest\n",
    "\n",
    "## Overview\n",
    "In the previous notebooks, we loaded data into Landing, Staging, and the Data Warehouse using Delta Lake. However, Delta Lake manages data using a transaction log and multiple Parquet files across different versions. \n",
    "\n",
    "**The Challenge:**\n",
    "Standard query engines like AWS Athena (Presto) simply look at a folder and read all files. If they read a Delta Lake folder directly, they will read **all versions** of the data (including deleted or stale parquet files), resulting in incorrect or duplicated data.\n",
    "\n",
    "**The Solution: Symlink Manifest**\n",
    "To allow Athena to read Delta tables correctly, we generate a **Symlink Manifest**. This is a text file that contains the list of *only* the valid parquet files for the current snapshot of the table.\n",
    "\n",
    "## Objective\n",
    "In this notebook, we will:\n",
    "1.  Define the SQL DDLs required to create tables in Athena that point to these manifests.\n",
    "2.  Use Python (`boto3`) to automate the creation of the Databases and Tables in the AWS Glue Catalog."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93721132",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import boto3\n",
    "import time\n",
    "\n",
    "# Configuration\n",
    "# Ensure you have your AWS Credentials configured in your environment \n",
    "# (e.g., ~/.aws/credentials or via environment variables)\n",
    "aws_region = \"ap-south-1\" # Change to your region\n",
    "athena_output_loc = \"s3://warehouse/target/athena_output/\" # Created in video\n",
    "\n",
    "# Initialize Athena Client\n",
    "athena_client = boto3.client('athena', region_name=aws_region)\n",
    "\n",
    "print(f\"Athena Client Initialized for region: {aws_region}\")\n",
    "print(f\"Query Output Location: {athena_output_loc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91ef833",
   "metadata": {},
   "source": [
    "## 1. SQL DDL Definitions\n",
    "Below are the SQL commands derived from the video. Note the specific configuration for Delta Lake compatibility:\n",
    "*   **Row Format:** `ParquetHiveSerDe`\n",
    "*   **Input Format:** `SymlinkTextInputFormat`\n",
    "*   **Location:** Points specifically to the `_symlink_format_manifest` directory inside the table folder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42106952",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Database Creation DDLs\n",
    "create_db_sqls = [\n",
    "    \"CREATE DATABASE IF NOT EXISTS edw_ld\",\n",
    "    \"CREATE DATABASE IF NOT EXISTS edw_stg\",\n",
    "    \"CREATE DATABASE IF NOT EXISTS edw\"\n",
    "]\n",
    "\n",
    "# 2. Table Creation DDLs\n",
    "\n",
    "# Landing Table\n",
    "ddl_landing = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS edw_ld.dim_date_ld (\n",
    "    `date` string,\n",
    "    `day` string,\n",
    "    `month` string,\n",
    "    `year` string,\n",
    "    `day_of_week` string,\n",
    "    `insert_dt` timestamp,\n",
    "    `rundate` string \n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://warehouse/landing/dim_date_ld/_symlink_format_manifest/'\n",
    "\"\"\"\n",
    "\n",
    "# Staging Table\n",
    "ddl_staging = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS edw_stg.dim_date_stg (\n",
    "    `date` date,\n",
    "    `day` int,\n",
    "    `month` int,\n",
    "    `year` int,\n",
    "    `day_of_week` string,\n",
    "    `insert_dt` timestamp,\n",
    "    `update_dt` timestamp,\n",
    "    `rundate` string\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://warehouse/staging/dim_date_stg/_symlink_format_manifest/'\n",
    "\"\"\"\n",
    "\n",
    "# Dimension (DW) Table\n",
    "ddl_dw = \"\"\"\n",
    "CREATE EXTERNAL TABLE IF NOT EXISTS edw.dim_date (\n",
    "    `row_wid` bigint,\n",
    "    `date` date,\n",
    "    `day` int,\n",
    "    `month` int,\n",
    "    `year` int,\n",
    "    `day_of_week` string,\n",
    "    `rundate` string,\n",
    "    `insert_dt` timestamp,\n",
    "    `update_dt` timestamp\n",
    ")\n",
    "ROW FORMAT SERDE 'org.apache.hadoop.hive.ql.io.parquet.serde.ParquetHiveSerDe'\n",
    "STORED AS INPUTFORMAT 'org.apache.hadoop.hive.ql.io.SymlinkTextInputFormat'\n",
    "OUTPUTFORMAT 'org.apache.hadoop.hive.ql.io.HiveIgnoreKeyTextOutputFormat'\n",
    "LOCATION 's3://warehouse/edw/dim_date/_symlink_format_manifest/'\n",
    "\"\"\"\n",
    "\n",
    "table_sqls = [ddl_landing, ddl_staging, ddl_dw]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6798176",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to execute Athena Queries\n",
    "def run_athena_query(query, database, output_location):\n",
    "    try:\n",
    "        response = athena_client.start_query_execution(\n",
    "            QueryString=query,\n",
    "            QueryExecutionContext={'Database': database},\n",
    "            ResultConfiguration={'OutputLocation': output_location}\n",
    "        )\n",
    "        query_execution_id = response['QueryExecutionId']\n",
    "        \n",
    "        # Wait for query to complete\n",
    "        state = 'RUNNING'\n",
    "        while state in ['RUNNING', 'QUEUED']:\n",
    "            time.sleep(1)\n",
    "            res = athena_client.get_query_execution(QueryExecutionId=query_execution_id)\n",
    "            state = res['QueryExecution']['Status']['State']\n",
    "            \n",
    "        if state == 'SUCCEEDED':\n",
    "            print(f\"Query Succeeded: {query[:50]}...\")\n",
    "        else:\n",
    "            print(f\"Query Failed: {res['QueryExecution']['Status']['StateChangeReason']}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error executing query: {e}\")\n",
    "\n",
    "# 1. Create Databases\n",
    "print(\"--- Creating Databases ---\")\n",
    "for sql in create_db_sqls:\n",
    "    run_athena_query(sql, 'default', athena_output_loc)\n",
    "\n",
    "# 2. Create Tables\n",
    "print(\"\\n--- Creating Tables ---\")\n",
    "# Note: We pass 'default' as context, but the table names in DDL have schema prefixes (e.g., edw_ld.table)\n",
    "for sql in table_sqls:\n",
    "    run_athena_query(sql, 'default', athena_output_loc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3396f9d",
   "metadata": {},
   "source": [
    "## 2. Verification\n",
    "Now that the tables are created, we can query the Data Warehouse table (`edw.dim_date`) to confirm that Athena can read the data correctly via the manifest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe3be648",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Query the final Dimension table\n",
    "verify_sql = \"SELECT * FROM edw.dim_date LIMIT 10\"\n",
    "\n",
    "print(\"\\n--- Verifying Data Access ---\")\n",
    "try:\n",
    "    # Run the select query\n",
    "    response = athena_client.start_query_execution(\n",
    "        QueryString=verify_sql,\n",
    "        QueryExecutionContext={'Database': 'edw'},\n",
    "        ResultConfiguration={'OutputLocation': athena_output_loc}\n",
    "    )\n",
    "    query_id = response['QueryExecutionId']\n",
    "    \n",
    "    # Wait for completion\n",
    "    time.sleep(3) \n",
    "    \n",
    "    # Get Results\n",
    "    results = athena_client.get_query_results(QueryExecutionId=query_id)\n",
    "    \n",
    "    # Simple print of result rows\n",
    "    rows = results['ResultSet']['Rows']\n",
    "    for row in rows:\n",
    "        data = [col.get('VarCharValue', 'NULL') for col in row['Data']]\n",
    "        print(\"\\t\".join(data))\n",
    "        \n",
    "except Exception as e:\n",
    "    print(f\"Verification failed. Ensure AWS credentials are set and tables were created. Error: {e}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
