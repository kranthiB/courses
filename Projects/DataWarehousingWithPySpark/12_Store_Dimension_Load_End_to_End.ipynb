{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "284d44ff",
   "metadata": {},
   "source": [
    "# Store Dimension - End-to-End Pipeline\n",
    "\n",
    "## Overview\n",
    "In this notebook, we implement the complete data pipeline for the **Store Dimension**. Unlike the Date dimension, this pipeline ingests data from external **CSV files**.\n",
    "\n",
    "## Architecture Flow\n",
    "1.  **Source:** `store_{rundate}.csv` file in the Landing Bucket.\n",
    "2.  **Landing Layer:** \n",
    "    *   Read CSV.\n",
    "    *   Cast to String.\n",
    "    *   Add Audit Columns.\n",
    "    *   Write to Delta Table (`dim_store_ld`).\n",
    "    *   Archive the source file.\n",
    "3.  **Staging Layer:**\n",
    "    *   Read incremental data from Landing.\n",
    "    *   De-duplication based on Natural Key (`store_id`).\n",
    "    *   Type casting.\n",
    "    *   Write to Delta Table (`dim_store_stg`).\n",
    "4.  **Dimension Layer (SCD Type 1):**\n",
    "    *   Read from Staging.\n",
    "    *   Generate Surrogate Keys (`row_wid`) using UUID.\n",
    "    *   **Upsert (Merge):** Update existing stores, Insert new stores.\n",
    "    *   Generate Symlink Manifest for Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41ab4cb8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Store Dimension Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "landing_file_name = f\"store_{run_date}.csv\"\n",
    "\n",
    "# Define Paths (Simulated for this notebook)\n",
    "base_path = \"s3a://warehouse/\" # Update with your bucket\n",
    "source_path = f\"{base_path}landing/source/store/{landing_file_name}\"\n",
    "archive_path = f\"{base_path}archive/store/\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "466b38f0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULATION: Create Dummy Source CSV File ---\n",
    "# In a real scenario, this file arrives from the source system.\n",
    "# We create it here to make this notebook runnable.\n",
    "\n",
    "data = [\n",
    "    (\"5001\", \"Pet House KA\", \"123 Main St\", \"Anytown\", \"KA\", \"12345\", \"91-99999-00001\"),\n",
    "    (\"5002\", \"Pet House MH\", \"456 Elm St\", \"Othertown\", \"MH\", \"67890\", \"91-99999-00002\"),\n",
    "    (\"5003\", \"Pet House TN\", \"789 Oak Ave\", \"BigCity\", \"TN\", \"11223\", \"91-99999-00003\")\n",
    "]\n",
    "columns = [\"store_id\", \"store_name\", \"address\", \"city\", \"state\", \"zip\", \"phone\"]\n",
    "\n",
    "df_source = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write as CSV to simulate the landing file\n",
    "# Note: In local execution, this writes to a local folder. \n",
    "# Ensure your Spark setup supports the scheme used in 'source_path'\n",
    "df_source.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(source_path.replace(landing_file_name, \"\"))\n",
    "\n",
    "print(f\"Simulated Source File Created at: {source_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e17eca35",
   "metadata": {},
   "source": [
    "## 1. Landing Layer Load\n",
    "We read the specific CSV file for the `run_date`, cast columns to String to prevent schema breaks, and add audit details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "717ae976",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- LANDING LOAD ---\n",
    "\n",
    "# 1. Read CSV\n",
    "df_raw = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"delimiter\", \",\") \\\n",
    "    .csv(source_path)\n",
    "\n",
    "# 2. Cast to String & Add Audit Cols\n",
    "# It is best practice to cast raw input to String in the Landing layer\n",
    "df_landing = df_raw.select([col(c).cast(\"string\") for c in df_raw.columns]) \\\n",
    "    .withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "print(\"Landing Data Preview:\")\n",
    "df_landing.show(truncate=False)\n",
    "\n",
    "# 3. Write to Landing Delta Table\n",
    "landing_table = f\"{schema_name}.dim_store_ld\"\n",
    "df_landing.write.format(\"delta\").mode(\"append\").saveAsTable(landing_table)\n",
    "\n",
    "print(f\"Data loaded to {landing_table}\")\n",
    "\n",
    "# 4. Archival (Logic only - requires boto3 for actual S3 move)\n",
    "print(f\"TODO: Move {source_path} to {archive_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea4e8d9",
   "metadata": {},
   "source": [
    "## 2. Staging Layer Load\n",
    "We read from the Landing table. In a production incremental run, we would filter by `insert_dt > max_timestamp`. Here, we perform deduplication on the `store_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0190f315",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- STAGING LOAD ---\n",
    "\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# 1. Read from Landing\n",
    "df_ld = spark.read.table(landing_table)\n",
    "\n",
    "# 2. Deduplication Logic (Natural Key: store_id)\n",
    "# We keep the latest record based on insert_dt\n",
    "window_spec = Window.partitionBy(\"store_id\").orderBy(col(\"insert_dt\").desc())\n",
    "\n",
    "df_deduped = df_ld.withColumn(\"rn\", row_number().over(window_spec)) \\\n",
    "                  .filter(col(\"rn\") == 1) \\\n",
    "                  .drop(\"rn\")\n",
    "\n",
    "# 3. Type Casting\n",
    "# Convert strings to appropriate types\n",
    "df_stg = df_deduped \\\n",
    "    .withColumn(\"store_id\", col(\"store_id\").cast(\"integer\")) \\\n",
    "    .withColumn(\"zip_code\", col(\"zip\").cast(\"string\")) \\\n",
    "    .withColumn(\"update_dt\", current_timestamp()) \\\n",
    "    .drop(\"zip\") # Renamed to zip_code\n",
    "\n",
    "# Select specific columns\n",
    "stg_cols = [\"store_id\", \"store_name\", \"address\", \"city\", \"state\", \"zip_code\", \"phone\", \"insert_dt\", \"update_dt\", \"rundate\"]\n",
    "df_stg_final = df_stg.select(stg_cols)\n",
    "\n",
    "# 4. Write to Staging (Overwrite)\n",
    "staging_table = f\"{schema_name}.dim_store_stg\"\n",
    "df_stg_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_table)\n",
    "\n",
    "print(f\"Data loaded to {staging_table}\")\n",
    "df_stg_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f312f9",
   "metadata": {},
   "source": [
    "## 3. Dimension Load (SCD Type 1)\n",
    "We move data from Staging to the final Dimension.\n",
    "*   **Surrogate Key:** Generated using a Python UUID function.\n",
    "*   **Merge:** Upsert logic based on `store_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e96bf97c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- DIMENSION LOAD ---\n",
    "\n",
    "# 1. Define UDF for UUID (Surrogate Key)\n",
    "# Note: In standard Spark, creating a monotonically increasing ID is often preferred for performance,\n",
    "# but for random unique IDs, UUID is used.\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "# 2. Read Staging\n",
    "df_stage_data = spark.read.table(staging_table)\n",
    "\n",
    "# 3. Generate Surrogate Key\n",
    "df_dim_prep = df_stage_data.withColumn(\"row_wid\", uuid_udf())\n",
    "\n",
    "# 4. Target Table Definition\n",
    "dim_table = f\"{schema_name}.dim_store\"\n",
    "\n",
    "# 5. SCD Type 1 Merge (Upsert)\n",
    "if DeltaTable.isDeltaTable(spark, f\"/user/hive/warehouse/{dim_table}\"): # Simplified check path\n",
    "    delta_target = DeltaTable.forName(spark, dim_table)\n",
    "    \n",
    "    delta_target.alias(\"tgt\").merge(\n",
    "        df_dim_prep.alias(\"src\"),\n",
    "        \"tgt.store_id = src.store_id\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"store_name\": col(\"src.store_name\"),\n",
    "        \"address\": col(\"src.address\"),\n",
    "        \"city\": col(\"src.city\"),\n",
    "        \"state\": col(\"src.state\"),\n",
    "        \"zip_code\": col(\"src.zip_code\"),\n",
    "        \"phone\": col(\"src.phone\"),\n",
    "        \"update_dt\": col(\"src.update_dt\"),\n",
    "        \"rundate\": col(\"src.rundate\")\n",
    "    }).whenNotMatchedInsert(values={\n",
    "        \"row_wid\": col(\"src.row_wid\"),\n",
    "        \"store_id\": col(\"src.store_id\"),\n",
    "        \"store_name\": col(\"src.store_name\"),\n",
    "        \"address\": col(\"src.address\"),\n",
    "        \"city\": col(\"src.city\"),\n",
    "        \"state\": col(\"src.state\"),\n",
    "        \"zip_code\": col(\"src.zip_code\"),\n",
    "        \"phone\": col(\"src.phone\"),\n",
    "        \"insert_dt\": col(\"src.insert_dt\"),\n",
    "        \"update_dt\": col(\"src.update_dt\"),\n",
    "        \"rundate\": col(\"src.rundate\")\n",
    "    }).execute()\n",
    "    print(\"Merge Completed.\")\n",
    "else:\n",
    "    # First Run: Create table\n",
    "    print(\"Table does not exist. Creating new table...\")\n",
    "    df_dim_prep.write.format(\"delta\").saveAsTable(dim_table)\n",
    "\n",
    "# 6. Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {dim_table}\")\n",
    "print(\"Manifest Generated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c1fe4ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Final Validation\n",
    "spark.sql(f\"SELECT * FROM {dim_table}\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
