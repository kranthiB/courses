{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4e5340eb",
   "metadata": {},
   "source": [
    "# Customer Dimension - End-to-End Pipeline (SCD Type 2)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we implement the pipeline for the **Customer Dimension**.\n",
    "This dimension requires **SCD Type 2** logic. This means if a customer's details change (e.g., they move to a new address), we:\n",
    "1.  **Expire** the old record (Set `active_flg` to 'N', set `end_date`).\n",
    "2.  **Insert** a new record with the new details (Set `active_flg` to 'Y', set `start_date`).\n",
    "\n",
    "## Architecture Flow\n",
    "1.  **Source:** `customer_{rundate}.csv` file.\n",
    "2.  **Landing Layer:** Ingest raw CSV to Delta.\n",
    "3.  **Staging Layer:** \n",
    "    *   Split `name` into `first_name` and `last_name`.\n",
    "    *   Handle NULLs in `plan_type`.\n",
    "    *   Standardize dates.\n",
    "4.  **Dimension Layer (SCD Type 2):**\n",
    "    *   **Full Load:** Truncate and Load.\n",
    "    *   **Incremental (SCD2):** \n",
    "        *   Identify changes between Staging and Dimension.\n",
    "        *   **Merge (Update):** Close historical records.\n",
    "        *   **Insert:** Append new active records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71b45c92",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Customer Dimension Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "landing_file_name = f\"customer_{run_date}.csv\"\n",
    "\n",
    "# Paths\n",
    "base_path = \"s3a://warehouse/\" # Update as needed\n",
    "source_path = f\"{base_path}landing/source/customer/{landing_file_name}\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e33587c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULATION: Create Dummy Source CSV ---\n",
    "data = [\n",
    "    (\"C001\", \"Ramesh Kumar\", \"123 Main St\", \"Anytown\", \"KA\", \"12345\", \"9999900001\", \"ramesh@email.com\", \"1985-05-20\", \"Gold\"),\n",
    "    (\"C002\", \"Sita Sharma\", \"456 Elm St\", \"Othertown\", \"MH\", \"67890\", \"9999900002\", \"sita@email.com\", \"1990-08-15\", None), # Null plan\n",
    "    (\"C003\", \"John Doe\", \"789 Oak Ave\", \"BigCity\", \"TN\", \"11223\", \"9999900003\", \"john@email.com\", \"1982-12-10\", \"Silver\")\n",
    "]\n",
    "columns = [\"customer_id\", \"name\", \"address\", \"city\", \"state\", \"zip_code\", \"phone_number\", \"email\", \"date_of_birth\", \"plan_type\"]\n",
    "\n",
    "df_source = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write to Source Path\n",
    "df_source.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(source_path.replace(landing_file_name, \"\"))\n",
    "print(\"Source file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efda4bf9",
   "metadata": {},
   "source": [
    "## 1. Landing Load\n",
    "Read the CSV, cast to String, and write to the Landing Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f0d82a4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- LANDING ---\n",
    "df_raw = spark.read.option(\"header\", \"true\").csv(source_path)\n",
    "\n",
    "# Cast to String & Audit\n",
    "df_landing = df_raw.select([col(c).cast(\"string\") for c in df_raw.columns]) \\\n",
    "    .withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "# Write\n",
    "landing_table = f\"{schema_name}.dim_customer_ld\"\n",
    "df_landing.write.format(\"delta\").mode(\"append\").saveAsTable(landing_table)\n",
    "print(f\"Loaded to {landing_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cceaccdb",
   "metadata": {},
   "source": [
    "## 2. Staging Load\n",
    "We apply specific transformations here:\n",
    "1.  **Name Split:** Convert `name` -> `first_name`, `last_name`.\n",
    "2.  **Date Casting:** `date_of_birth` to Date type.\n",
    "3.  **Null Handling:** Coalesce `plan_type` to \"NA\".\n",
    "4.  **Deduplication:** Based on `customer_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9573798e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- STAGING ---\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_ld = spark.read.table(landing_table)\n",
    "\n",
    "# Dedupe\n",
    "window_spec = Window.partitionBy(\"customer_id\").orderBy(col(\"insert_dt\").desc())\n",
    "df_deduped = df_ld.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "# Transformations\n",
    "df_stg = df_deduped \\\n",
    "    .withColumn(\"first_name\", split(col(\"name\"), \" \")[0]) \\\n",
    "    .withColumn(\"last_name\", split(col(\"name\"), \" \")[1]) \\\n",
    "    .withColumn(\"date_of_birth\", to_date(col(\"date_of_birth\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"plan_type\", coalesce(col(\"plan_type\"), lit(\"NA\"))) \\\n",
    "    .withColumn(\"update_dt\", current_timestamp())\n",
    "\n",
    "# Select Columns\n",
    "stg_cols = [\"customer_id\", \"first_name\", \"last_name\", \"address\", \"city\", \"state\", \n",
    "            \"zip_code\", \"phone_number\", \"email\", \"date_of_birth\", \"plan_type\", \n",
    "            \"insert_dt\", \"update_dt\", \"rundate\"]\n",
    "\n",
    "df_stg_final = df_stg.select(stg_cols)\n",
    "\n",
    "# Write\n",
    "staging_table = f\"{schema_name}.dim_customer_stg\"\n",
    "df_stg_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_table)\n",
    "print(f\"Loaded to {staging_table}\")\n",
    "df_stg_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4f63a12",
   "metadata": {},
   "source": [
    "## 3. Dimension Load (SCD Type 2)\n",
    "\n",
    "### The SCD2 Logic\n",
    "1.  **Surrogate Key:** Generate `row_wid`.\n",
    "2.  **SCD Columns:**\n",
    "    *   `effective_start_dt`: Current Timestamp (for new rows).\n",
    "    *   `effective_end_dt`: High Date (9999-12-31) for active rows.\n",
    "    *   `active_flg`: 'Y' for active rows.\n",
    "3.  **Update (Merge):** Identify records in the Target table that match the incoming Staging data (by `customer_id`) AND are currently active (`active_flg='Y'`). For these records, update `active_flg='N'` and set `effective_end_dt` to now.\n",
    "4.  **Insert:** Insert the new records from Staging as active."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caaf16f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- DIMENSION (SCD2) ---\n",
    "\n",
    "# UUID UDF\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "# Read Staging\n",
    "df_stage = spark.read.table(staging_table)\n",
    "\n",
    "# Prepare Data for Insertion (New Records)\n",
    "# Add SCD2 specific columns for new records\n",
    "df_new_records = df_stage \\\n",
    "    .withColumn(\"row_wid\", uuid_udf()) \\\n",
    "    .withColumn(\"effective_start_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_dt\", to_timestamp(lit(\"9999-12-31 00:00:00\"))) \\\n",
    "    .withColumn(\"active_flg\", lit(\"Y\"))\n",
    "\n",
    "dim_table = f\"{schema_name}.dim_customer\"\n",
    "\n",
    "# --- SCD 2 IMPLEMENTATION ---\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, f\"/user/hive/warehouse/{dim_table}\"):\n",
    "    # FIRST RUN: Just write the data\n",
    "    print(\"Table not found. Creating Initial Load.\")\n",
    "    df_new_records.write.format(\"delta\").saveAsTable(dim_table)\n",
    "else:\n",
    "    print(\"Incremental Load: executing SCD2 Logic.\")\n",
    "    delta_target = DeltaTable.forName(spark, dim_table)\n",
    "    \n",
    "    # 1. UPDATE (Close History)\n",
    "    # We join Target and Staging on customer_id. \n",
    "    # If match found, we expire the Target record.\n",
    "    # Note: In a strict SCD2, we compare hash/columns to see if data actually changed. \n",
    "    # Here, we assume incoming data implies a change or new version.\n",
    "    \n",
    "    # We use a temporary merge logic to update the target\n",
    "    # Ideally, SCD2 involves: \n",
    "    #   1. Update old records \n",
    "    #   2. Insert new records\n",
    "    \n",
    "    # Step A: Update existing active records to expire them\n",
    "    delta_target.alias(\"tgt\").merge(\n",
    "        df_stage.alias(\"src\"),\n",
    "        \"tgt.customer_id = src.customer_id AND tgt.active_flg = 'Y'\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"active_flg\": lit(\"N\"),\n",
    "        \"effective_end_dt\": current_timestamp(),\n",
    "        \"update_dt\": current_timestamp()\n",
    "    }).execute()\n",
    "    \n",
    "    # Step B: Insert the new records\n",
    "    # We append the df_new_records prepared earlier\n",
    "    df_new_records.write.format(\"delta\").mode(\"append\").saveAsTable(dim_table)\n",
    "\n",
    "print(\"SCD2 Load Complete.\")\n",
    "\n",
    "# Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {dim_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fb8b068",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "print(\"Final Dimension Data:\")\n",
    "# Sort by customer_id and start_date to see history\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT customer_id, first_name, plan_type, effective_start_dt, effective_end_dt, active_flg \n",
    "    FROM {dim_table} \n",
    "    ORDER BY customer_id, effective_start_dt\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
