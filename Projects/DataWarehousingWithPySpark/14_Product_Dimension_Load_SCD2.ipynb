{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d91cf7",
   "metadata": {},
   "source": [
    "# Product Dimension - End-to-End Pipeline (SCD Type 2)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we implement the pipeline for the **Product Dimension**.\n",
    "Similar to the Customer dimension, this is an **SCD Type 2** table. We track changes in product attributes (like price, size, or expiration) over time.\n",
    "\n",
    "## Architecture Flow\n",
    "1.  **Source:** `product_{rundate}.csv` file.\n",
    "2.  **Landing Layer:** Ingest raw CSV to Delta.\n",
    "3.  **Staging Layer:** \n",
    "    *   Deduplication based on `product_id`.\n",
    "    *   Type casting (Price to Float, Quantity to Int, etc.).\n",
    "    *   Preparation of SCD2 columns.\n",
    "4.  **Dimension Layer (SCD Type 2):**\n",
    "    *   **Merge (Update):** Expire old active records for products that have changed.\n",
    "    *   **Insert:** Append new versions of the products as active records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21895923",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "import uuid\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Product Dimension Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "landing_file_name = f\"product_{run_date}.csv\"\n",
    "\n",
    "# Paths\n",
    "base_path = \"s3a://warehouse/\" # Update as needed\n",
    "source_path = f\"{base_path}landing/source/product/{landing_file_name}\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4212677a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULATION: Create Dummy Source CSV ---\n",
    "data = [\n",
    "    (\"P001\", \"Purina Pro Plan\", \"Purina\", \"Dry\", \"Chicken\", \"5 kgs\", \"20.00\", \"100\", \"2024-12-31\", \"https://img.url/p001\"),\n",
    "    (\"P002\", \"Hill's Science Diet\", \"Hill's\", \"Wet\", \"Beef\", \"10 cans\", \"15.50\", \"50\", \"2023-06-30\", \"https://img.url/p002\"),\n",
    "    (\"P003\", \"Blue Buffalo\", \"Blue Buffalo\", \"Dry\", \"Lamb\", \"2 kgs\", \"12.00\", \"200\", \"2025-01-01\", \"https://img.url/p003\")\n",
    "]\n",
    "columns = [\"product_id\", \"product_name\", \"brand\", \"type\", \"flavor\", \"size\", \"price\", \"quantity\", \"expiration_date\", \"image_url\"]\n",
    "\n",
    "df_source = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write to Source Path\n",
    "df_source.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(source_path.replace(landing_file_name, \"\"))\n",
    "print(\"Source file created.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9612e49",
   "metadata": {},
   "source": [
    "## 1. Landing Load\n",
    "Read the CSV, cast to String, and write to the Landing Delta table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f97da9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- LANDING ---\n",
    "df_raw = spark.read.option(\"header\", \"true\").csv(source_path)\n",
    "\n",
    "# Cast to String & Audit\n",
    "df_landing = df_raw.select([col(c).cast(\"string\") for c in df_raw.columns]) \\\n",
    "    .withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "# Write\n",
    "landing_table = f\"{schema_name}.dim_product_ld\"\n",
    "df_landing.write.format(\"delta\").mode(\"append\").saveAsTable(landing_table)\n",
    "print(f\"Loaded to {landing_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aea8c3a",
   "metadata": {},
   "source": [
    "## 2. Staging Load\n",
    "Transformations:\n",
    "1.  **Type Casting:** Convert `price` to Float, `quantity` to Integer, `expiration_date` to Date.\n",
    "2.  **Deduplication:** Based on `product_id`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd1d254e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- STAGING ---\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "df_ld = spark.read.table(landing_table)\n",
    "\n",
    "# Dedupe\n",
    "window_spec = Window.partitionBy(\"product_id\").orderBy(col(\"insert_dt\").desc())\n",
    "df_deduped = df_ld.withColumn(\"rn\", row_number().over(window_spec)).filter(\"rn=1\").drop(\"rn\")\n",
    "\n",
    "# Transformations\n",
    "df_stg = df_deduped \\\n",
    "    .withColumn(\"price\", col(\"price\").cast(\"float\")) \\\n",
    "    .withColumn(\"quantity\", col(\"quantity\").cast(\"integer\")) \\\n",
    "    .withColumn(\"expiration_date\", to_date(col(\"expiration_date\"), \"yyyy-MM-dd\")) \\\n",
    "    .withColumn(\"update_dt\", current_timestamp())\n",
    "\n",
    "# Select Columns\n",
    "stg_cols = [\"product_id\", \"product_name\", \"brand\", \"type\", \"flavor\", \"size\", \n",
    "            \"price\", \"quantity\", \"expiration_date\", \"image_url\", \n",
    "            \"insert_dt\", \"update_dt\", \"rundate\"]\n",
    "\n",
    "df_stg_final = df_stg.select(stg_cols)\n",
    "\n",
    "# Write\n",
    "staging_table = f\"{schema_name}.dim_product_stg\"\n",
    "df_stg_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_table)\n",
    "print(f\"Loaded to {staging_table}\")\n",
    "df_stg_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10a38c48",
   "metadata": {},
   "source": [
    "## 3. Dimension Load (SCD Type 2)\n",
    "\n",
    "### The SCD2 Logic\n",
    "1.  **Surrogate Key:** Generate `row_wid` using UUID.\n",
    "2.  **SCD Columns:**\n",
    "    *   `effective_start_dt`: Current Timestamp.\n",
    "    *   `effective_end_dt`: High Date (9999-12-31).\n",
    "    *   `active_flg`: 'Y'.\n",
    "3.  **Update (Expire):** If `product_id` matches and is active -> Set `active_flg='N'` and `effective_end_dt=now`.\n",
    "4.  **Insert:** Append new records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da4117f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- DIMENSION (SCD2) ---\n",
    "\n",
    "# UUID UDF\n",
    "uuid_udf = udf(lambda: str(uuid.uuid4()), StringType())\n",
    "\n",
    "# Read Staging\n",
    "df_stage = spark.read.table(staging_table)\n",
    "\n",
    "# Prepare Data for Insertion (New Records)\n",
    "df_new_records = df_stage \\\n",
    "    .withColumn(\"row_wid\", uuid_udf()) \\\n",
    "    .withColumn(\"effective_start_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"effective_end_dt\", to_timestamp(lit(\"9999-12-31 00:00:00\"))) \\\n",
    "    .withColumn(\"active_flg\", lit(\"Y\"))\n",
    "\n",
    "dim_table = f\"{schema_name}.dim_product\"\n",
    "\n",
    "# --- SCD 2 IMPLEMENTATION ---\n",
    "\n",
    "if not DeltaTable.isDeltaTable(spark, f\"/user/hive/warehouse/{dim_table}\"):\n",
    "    # FIRST RUN\n",
    "    print(\"Table not found. Creating Initial Load.\")\n",
    "    df_new_records.write.format(\"delta\").saveAsTable(dim_table)\n",
    "else:\n",
    "    print(\"Incremental Load: executing SCD2 Logic.\")\n",
    "    delta_target = DeltaTable.forName(spark, dim_table)\n",
    "    \n",
    "    # Step A: Update existing active records to expire them\n",
    "    delta_target.alias(\"tgt\").merge(\n",
    "        df_stage.alias(\"src\"),\n",
    "        \"tgt.product_id = src.product_id AND tgt.active_flg = 'Y'\"\n",
    "    ).whenMatchedUpdate(set={\n",
    "        \"active_flg\": lit(\"N\"),\n",
    "        \"effective_end_dt\": current_timestamp(),\n",
    "        \"update_dt\": current_timestamp()\n",
    "    }).execute()\n",
    "    \n",
    "    # Step B: Insert the new records\n",
    "    df_new_records.write.format(\"delta\").mode(\"append\").saveAsTable(dim_table)\n",
    "\n",
    "print(\"SCD2 Load Complete.\")\n",
    "\n",
    "# Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {dim_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b52cdcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Validation\n",
    "print(\"Final Dimension Data:\")\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT product_id, product_name, price, effective_start_dt, effective_end_dt, active_flg \n",
    "    FROM {dim_table} \n",
    "    ORDER BY product_id, effective_start_dt\n",
    "\"\"\").show(truncate=False)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
