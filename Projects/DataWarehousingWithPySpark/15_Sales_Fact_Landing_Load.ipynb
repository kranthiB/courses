{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2ecad3f9",
   "metadata": {},
   "source": [
    "# Sales Fact - Landing Data Load (JSON Source)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we begin loading the **Sales Fact** table.\n",
    "*   **Source Data:** The stores drop sales data in **JSON** format.\n",
    "*   **File Pattern:** `order_{rundate}_{seq}.json` (e.g., `order_20220101_1.json`). Multiple files can exist for a single run date.\n",
    "*   **Data Structure:** Nested JSON containing an array of `orders`, where each order contains an array of `order_lines`.\n",
    "\n",
    "## The Landing Layer Strategy\n",
    "Instead of parsing the complex JSON structure immediately, we will:\n",
    "1.  **Read as Text:** Read the entire JSON file content into a single column (typically named `value`).\n",
    "2.  **Add Audit:** Attach `insert_dt` and `rundate`.\n",
    "3.  **Write to Delta:** Store this raw string representation in the Landing Delta table (`fact_sales_ld`).\n",
    "\n",
    "This approach ensures efficient ingestion and allows us to handle parsing failures or schema changes downstream in the Staging layer without stopping the ingestion pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e05d5105",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "import json\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales Fact Landing Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "landing_file_pattern = f\"order_{run_date}_*.json\"\n",
    "\n",
    "# Paths\n",
    "base_path = \"s3a://warehouse/\" # Update as needed\n",
    "source_path = f\"{base_path}landing/source/orders/\"\n",
    "landing_table = f\"{schema_name}.fact_sales_ld\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82f7255f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULATION: Create Dummy JSON Source Files ---\n",
    "# We will create 2 JSON files to simulate multiple sequence files dropping for the same day.\n",
    "\n",
    "# Helper function to write json\n",
    "def write_json_file(filename, data_dict):\n",
    "    path = source_path + filename\n",
    "    # Convert dict to JSON string\n",
    "    json_str = json.dumps(data_dict)\n",
    "    # Create RDD and write as text to simulate a file\n",
    "    spark.sparkContext.parallelize([json_str]).saveAsTextFile(path)\n",
    "    print(f\"Created simulation file: {path}\")\n",
    "\n",
    "# Data for File 1\n",
    "data_1 = {\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"O1001\",\n",
    "            \"invoice_num\": \"INV1001\",\n",
    "            \"order_date\": \"2022-01-01\",\n",
    "            \"store_id\": 5001,\n",
    "            \"customer_id\": \"C001\",\n",
    "            \"order_lines\": [\n",
    "                {\"product_id\": \"P001\", \"qty\": 2, \"price\": 20.00, \"tax\": 2.5, \"discount\": 0},\n",
    "                {\"product_id\": \"P002\", \"qty\": 1, \"price\": 15.50, \"tax\": 1.5, \"discount\": 1.0}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Data for File 2\n",
    "data_2 = {\n",
    "    \"orders\": [\n",
    "        {\n",
    "            \"order_id\": \"O1002\",\n",
    "            \"invoice_num\": \"INV1002\",\n",
    "            \"order_date\": \"2022-01-01\",\n",
    "            \"store_id\": 5002,\n",
    "            \"customer_id\": \"C002\",\n",
    "            \"order_lines\": [\n",
    "                {\"product_id\": \"P003\", \"qty\": 5, \"price\": 12.00, \"tax\": 5.0, \"discount\": 2.0}\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "# Write files (clean up first if exists for simulation)\n",
    "# Note: In real local spark, saveAsTextFile creates a folder. We assume the reader handles it.\n",
    "import shutil\n",
    "import os\n",
    "# Logic to mock file creation locally if S3 is not available\n",
    "# For this notebook, we assume the reader will read whatever is at 'source_path'\n",
    "# Here we write df to text to simulate\n",
    "spark.createDataFrame([(json.dumps(data_1),)], [\"value\"]).coalesce(1).write.mode(\"overwrite\").text(source_path + f\"order_{run_date}_1.json\")\n",
    "spark.createDataFrame([(json.dumps(data_2),)], [\"value\"]).coalesce(1).write.mode(\"overwrite\").text(source_path + f\"order_{run_date}_2.json\")\n",
    "\n",
    "print(\"Source JSON files generated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf56c0e9",
   "metadata": {},
   "source": [
    "## 1. Landing Load (Read as Text)\n",
    "We read the JSON files using `spark.read.text`. This treats the entire file content (or line) as a string. This is crucial for handling semi-structured data where the schema might evolve or be complex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74a36a7c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- READ DATA ---\n",
    "\n",
    "# Read all files matching the pattern for the rundate\n",
    "# wholetext=True ensures multi-line JSONs are read as a single record if needed, \n",
    "# though here we generated single-line JSONs.\n",
    "df_raw = spark.read.option(\"wholetext\", \"false\").text(source_path + landing_file_pattern)\n",
    "\n",
    "# Add Audit Columns\n",
    "df_landing = df_raw \\\n",
    "    .withColumn(\"insert_dt\", current_timestamp()) \\\n",
    "    .withColumn(\"rundate\", lit(run_date))\n",
    "\n",
    "print(\"Raw Data Preview (Value column contains JSON string):\")\n",
    "df_landing.show(truncate=False)\n",
    "df_landing.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f3e221",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- WRITE TO LANDING ---\n",
    "\n",
    "# Write to Delta Table\n",
    "df_landing.write.format(\"delta\").mode(\"append\").saveAsTable(landing_table)\n",
    "\n",
    "print(f\"Data successfully written to {landing_table}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "009dcc5d",
   "metadata": {},
   "source": [
    "## 2. Post-Load Activities\n",
    "1.  **Archive:** Move processed files to archive folder.\n",
    "2.  **Log:** Update Job Control.\n",
    "3.  **Manifest:** Generate Symlink Manifest for Athena."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a050bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- JOB CONTROL & MANIFEST ---\n",
    "\n",
    "# 1. Update Job Control (Mock Function)\n",
    "def insert_log(spark, schema, table, count, rundate):\n",
    "    print(f\"LOG: {schema}.{table} loaded with {count} rows for {rundate}\")\n",
    "\n",
    "insert_log(spark, schema_name, \"fact_sales_ld\", df_landing.count(), run_date)\n",
    "\n",
    "# 2. Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {landing_table}\")\n",
    "print(\"Manifest generated.\")\n",
    "\n",
    "# 3. Validation\n",
    "print(\"Data in Landing Table:\")\n",
    "spark.sql(f\"SELECT * FROM {landing_table}\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
