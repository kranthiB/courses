{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db934a9f",
   "metadata": {},
   "source": [
    "# Sales Fact - Staging Data Load (JSON Parsing & Explosion)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we transform the raw JSON strings from the Landing layer into a structured Staging table.\n",
    "*   **Source:** `fact_sales_ld` (Contains raw JSON in `value` column).\n",
    "*   **Goal:** Parse the JSON, explode nested arrays (`orders` and `order_lines`), and flatten the structure.\n",
    "*   **Transformations:**\n",
    "    1.  **Schema Inference:** Determine the schema of the JSON string.\n",
    "    2.  **Parse:** Use `from_json` to convert the string to a Struct.\n",
    "    3.  **Explode:** \n",
    "        *   Explode `orders` array to get individual orders.\n",
    "        *   Explode `order_lines` array to get individual line items per order.\n",
    "    4.  **Enrichment:** \n",
    "        *   Join with `dim_product` to get `price` (since it's missing in the source).\n",
    "        *   Calculate `line_total`, `tax_amount`, and `discount_amount`.\n",
    "        *   Generate `integration_key` (Composite Key).\n",
    "\n",
    "## Key Concept: Explosion\n",
    "The source data is hierarchical: `Root -> Orders[] -> OrderLines[]`.\n",
    "To store this in a relational Data Warehouse fact table, we need to flatten it so that **one row represents one line item**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb03c4be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales Fact Staging Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "\n",
    "# Paths & Tables\n",
    "landing_table = f\"{schema_name}.fact_sales_ld\"\n",
    "staging_table = f\"{schema_name}.fact_sales_stg\"\n",
    "dim_product = f\"{schema_name}.dim_product\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5237f54",
   "metadata": {},
   "source": [
    "## 1. Read Landing Data\n",
    "Read the raw JSON strings. In a real incremental load, we would filter by `insert_dt`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2296b58e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read Landing Table\n",
    "df_raw = spark.read.table(landing_table)\n",
    "\n",
    "# For this load, let's process everything. \n",
    "# In production: filter where insert_dt > max_timestamp\n",
    "print(f\"Raw Records: {df_raw.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3bd42ca",
   "metadata": {},
   "source": [
    "## 2. Schema Inference & Parsing\n",
    "We sample the data to infer the JSON schema, then use `from_json` to parse the `value` column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff370bd6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Infer Schema\n",
    "# We take a sample record to determine the structure\n",
    "sample_json = df_raw.select(\"value\").first()[0]\n",
    "json_schema = schema_of_json(sample_json)\n",
    "\n",
    "print(\"Inferred JSON Schema:\")\n",
    "print(json_schema)\n",
    "\n",
    "# 2. Parse JSON\n",
    "df_parsed = df_raw.withColumn(\"json_data\", from_json(col(\"value\"), json_schema))\n",
    "\n",
    "# 3. Explode Orders\n",
    "# json_data.orders is an Array. We explode it to get one row per order.\n",
    "df_orders = df_parsed.select(explode(col(\"json_data.orders\")).alias(\"order\"), \"insert_dt\", \"rundate\")\n",
    "\n",
    "# 4. Explode Order Lines\n",
    "# order.order_lines is also an Array. We explode it to get one row per line item.\n",
    "# We keep the parent order details as well.\n",
    "df_exploded = df_orders.select(\n",
    "    col(\"order.order_id\"),\n",
    "    col(\"order.invoice_num\"),\n",
    "    col(\"order.order_date\"),\n",
    "    col(\"order.store_id\"),\n",
    "    col(\"order.customer_id\"),\n",
    "    explode(col(\"order.order_lines\")).alias(\"line\"),\n",
    "    \"insert_dt\",\n",
    "    \"rundate\"\n",
    ")\n",
    "\n",
    "# 5. Flatten Structure\n",
    "df_flattened = df_exploded.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"invoice_num\"),\n",
    "    col(\"order_date\"),\n",
    "    col(\"store_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"line.product_id\"),\n",
    "    col(\"line.qty\"),\n",
    "    col(\"line.tax\"),\n",
    "    col(\"line.tax_type\"), # e.g., Percentage\n",
    "    col(\"line.discount\"),\n",
    "    col(\"line.discount_type\"), # e.g., Percentage\n",
    "    \"insert_dt\",\n",
    "    \"rundate\"\n",
    ")\n",
    "\n",
    "print(\"Exploded & Flattened Data:\")\n",
    "df_flattened.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4da5d72",
   "metadata": {},
   "source": [
    "## 3. Enrichment & Calculations\n",
    "The source data contains `qty`, `tax`, and `discount`, but usually not the unit `price`. We need to look up the price from the **Product Dimension**.\n",
    "\n",
    "**Logic:**\n",
    "1.  **Lookup Price:** Join with `dim_product` on `product_id` (and ensure `active_flg='Y'`).\n",
    "2.  **Calculate Totals:**\n",
    "    *   `sub_total` = qty * price\n",
    "    *   `tax_amt` = (sub_total * tax / 100) OR absolute tax\n",
    "    *   `discount_amt` = (sub_total * discount / 100) OR absolute discount\n",
    "    *   `line_total` = sub_total + tax_amt - discount_amt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbadf509",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read Product Dimension for Price Lookup\n",
    "df_product = spark.read.table(dim_product).filter(\"active_flg = 'Y'\") \\\n",
    "    .select(\"product_id\", \"price\", \"row_wid\") \\\n",
    "    .withColumnRenamed(\"row_wid\", \"product_wid\")\n",
    "\n",
    "# 2. Join to get Price\n",
    "df_joined = df_flattened.join(df_product, \"product_id\", \"left\")\n",
    "\n",
    "# 3. Calculations\n",
    "df_calc = df_joined \\\n",
    "    .withColumn(\"sub_total\", col(\"qty\") * col(\"price\")) \\\n",
    "    .withColumn(\"tax_amt\", when(col(\"tax_type\") == \"Percentage\", (col(\"sub_total\") * col(\"tax\") / 100)).otherwise(col(\"tax\"))) \\\n",
    "    .withColumn(\"discount_amt\", when(col(\"discount_type\") == \"Percentage\", (col(\"sub_total\") * col(\"discount\") / 100)).otherwise(col(\"discount\"))) \\\n",
    "    .withColumn(\"line_total\", col(\"sub_total\") + col(\"tax_amt\") - col(\"discount_amt\"))\n",
    "\n",
    "# 4. Generate Integration Key (Unique ID for the Fact Row)\n",
    "# Ideally a combination of OrderID + ProductID + Sequence\n",
    "df_stg = df_calc.withColumn(\"integration_key\", md5(concat(col(\"order_id\"), col(\"product_id\")))) \\\n",
    "    .withColumn(\"update_dt\", current_timestamp())\n",
    "\n",
    "# 5. Select Final Staging Columns\n",
    "final_cols = [\n",
    "    \"integration_key\", \"order_id\", \"invoice_num\", \"order_date\", \n",
    "    \"store_id\", \"customer_id\", \"product_id\", \"product_wid\",\n",
    "    \"qty\", \"price\", \"tax_amt\", \"discount_amt\", \"line_total\",\n",
    "    \"insert_dt\", \"update_dt\", \"rundate\"\n",
    "]\n",
    "\n",
    "df_stg_final = df_stg.select(final_cols)\n",
    "\n",
    "print(\"Final Staging Data:\")\n",
    "df_stg_final.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8087d0d3",
   "metadata": {},
   "source": [
    "## 4. Write to Staging\n",
    "Overwrite the staging table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e153eee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write to Staging Table\n",
    "df_stg_final.write.format(\"delta\").mode(\"overwrite\").saveAsTable(staging_table)\n",
    "print(f\"Data loaded to {staging_table}\")\n",
    "\n",
    "# Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {staging_table}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
