{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dcd2654",
   "metadata": {},
   "source": [
    "# Sales Fact - Data Warehouse Load (Star Schema Integration)\n",
    "\n",
    "## Overview\n",
    "In this notebook, we perform the final load for the **Sales Fact Table**.\n",
    "*   **Source:** `fact_sales_stg` (Flattened sales data).\n",
    "*   **Target:** `fact_sales` (Final Fact table in the Data Warehouse).\n",
    "*   **Strategy:** **Append-Only**. Sales transactions are historical facts and usually do not change once finalized (unless there are returns/corrections, which are often handled as new negative transactions or separate processes).\n",
    "\n",
    "## Key Integration Step: Surrogate Key Lookups\n",
    "A Fact table should ideally contain **Foreign Keys (Surrogate Keys)** pointing to the Dimension tables, rather than Natural Keys. This ensures referential integrity and performance.\n",
    "\n",
    "We will join the Staging data with:\n",
    "1.  **Dim Store** (to get `store_wid`)\n",
    "2.  **Dim Customer** (to get `customer_wid`)\n",
    "3.  **Dim Product** (to get `product_wid`)\n",
    "4.  **Dim Date** (to get `date_wid`) - *Note: In many designs, date_wid is just the integer date YYYYMMDD*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ec34ec0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from delta.tables import *\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Sales Fact Load\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220101\"\n",
    "schema_name = \"pyspark_warehouse\"\n",
    "\n",
    "# Table Names\n",
    "stg_table = f\"{schema_name}.fact_sales_stg\"\n",
    "dim_store = f\"{schema_name}.dim_store\"\n",
    "dim_customer = f\"{schema_name}.dim_customer\"\n",
    "dim_product = f\"{schema_name}.dim_product\"\n",
    "dim_date = f\"{schema_name}.dim_date\"\n",
    "fact_table = f\"{schema_name}.fact_sales\"\n",
    "\n",
    "print(f\"Processing Run Date: {run_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d489aa9",
   "metadata": {},
   "source": [
    "## 1. Read Staging Data\n",
    "Read the processed sales data from the staging layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e6edf7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read Staging\n",
    "df_stg = spark.read.table(stg_table)\n",
    "\n",
    "print(f\"Staging Records: {df_stg.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "421230a2",
   "metadata": {},
   "source": [
    "## 2. Dimension Lookups (Surrogate Keys)\n",
    "We join the staging data with dimensions to retrieve the Warehouse IDs (WIDs).\n",
    "*   **Join Condition:** Natural Keys (e.g., `store_id`)\n",
    "*   **Filter:** For SCD2 dimensions (Customer, Product), we strictly should check if the transaction date falls between `effective_start_dt` and `effective_end_dt`. \n",
    "    *   *Simplification for this tutorial:* We will join on the Natural Key and filter for `active_flg = 'Y'` (Current View) or just assume the dimensions align with the transaction date for the initial load. \n",
    "    *   *Best Practice:* `stg.order_date BETWEEN dim.start_date AND dim.end_date`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce7659f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read Dimensions\n",
    "df_dim_store = spark.read.table(dim_store).select(\"store_id\", \"row_wid\").withColumnRenamed(\"row_wid\", \"store_wid\")\n",
    "df_dim_cust = spark.read.table(dim_customer).where(\"active_flg='Y'\").select(\"customer_id\", \"row_wid\").withColumnRenamed(\"row_wid\", \"customer_wid\")\n",
    "df_dim_prod = spark.read.table(dim_product).where(\"active_flg='Y'\").select(\"product_id\", \"row_wid\").withColumnRenamed(\"row_wid\", \"product_wid\")\n",
    "# For Date, usually we join on the date column\n",
    "df_dim_date = spark.read.table(dim_date).select(\"date\", \"row_wid\").withColumnRenamed(\"row_wid\", \"date_wid\")\n",
    "\n",
    "# 2. Perform Joins\n",
    "# Start with Staging\n",
    "df_joined = df_stg \\\n",
    "    .join(df_dim_store, \"store_id\", \"left\") \\\n",
    "    .join(df_dim_cust, \"customer_id\", \"left\") \\\n",
    "    .join(df_dim_prod, \"product_id\", \"left\") \\\n",
    "    .join(df_dim_date, df_stg[\"order_date\"] == df_dim_date[\"date\"], \"left\")\n",
    "\n",
    "# 3. Select Final Fact Columns\n",
    "final_cols = [\n",
    "    \"integration_key\", \n",
    "    \"store_wid\", \n",
    "    \"customer_wid\", \n",
    "    \"product_wid\", \n",
    "    \"date_wid\", \n",
    "    \"order_id\", \n",
    "    \"invoice_num\", \n",
    "    \"qty\", \n",
    "    \"price\", \n",
    "    \"tax_amt\", \n",
    "    \"discount_amt\", \n",
    "    \"line_total\", \n",
    "    \"insert_dt\", \n",
    "    \"update_dt\", \n",
    "    \"rundate\"\n",
    "]\n",
    "\n",
    "# Handle potential nulls in WIDs (e.g., default to -1 or keep null)\n",
    "df_fact_prep = df_joined.select(final_cols) \\\n",
    "    .fillna(-1, subset=[\"store_wid\", \"customer_wid\", \"product_wid\", \"date_wid\"])\n",
    "\n",
    "print(\"Fact Table Data Preview:\")\n",
    "df_fact_prep.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb6d1c43",
   "metadata": {},
   "source": [
    "## 3. Write to Fact Table\n",
    "We append the data to the Fact table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d241640",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write to Fact Table (Append)\n",
    "df_fact_prep.write.format(\"delta\").mode(\"append\").saveAsTable(fact_table)\n",
    "\n",
    "print(f\"Data successfully loaded to {fact_table}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac88ecfe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- POST LOAD ACTIVITIES ---\n",
    "\n",
    "# 1. Update Job Control\n",
    "# (Mock logic)\n",
    "print(f\"LOG: {fact_table} loaded with {df_fact_prep.count()} rows.\")\n",
    "\n",
    "# 2. Generate Manifest\n",
    "spark.sql(f\"GENERATE symlink_format_manifest FOR TABLE {fact_table}\")\n",
    "print(\"Manifest generated.\")\n",
    "\n",
    "# 3. Final Analysis Query\n",
    "spark.sql(f\"\"\"\n",
    "    SELECT \n",
    "        d.year, d.month, \n",
    "        p.category, \n",
    "        SUM(f.line_total) as total_sales\n",
    "    FROM {fact_table} f\n",
    "    JOIN {dim_date} d ON f.date_wid = d.row_wid\n",
    "    JOIN {dim_product} p ON f.product_wid = p.row_wid\n",
    "    GROUP BY d.year, d.month, p.category\n",
    "    ORDER BY total_sales DESC\n",
    "    LIMIT 5\n",
    "\"\"\").show() # Note: Adjust columns based on actual schema availability"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
