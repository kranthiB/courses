{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f8504e87",
   "metadata": {},
   "source": [
    "# Incremental Data Load & Production Execution\n",
    "\n",
    "## Overview\n",
    "Up to this point, we have executed our pipelines interactively using Jupyter Notebooks. While notebooks are great for development and exploration, production workloads typically run as automated batch jobs.\n",
    "\n",
    "**The Standard Approach:**\n",
    "1.  **Code format:** Convert `.ipynb` notebooks to standard Python scripts (`.py`).\n",
    "2.  **Execution:** Use `spark-submit` command-line utility to submit jobs to the Spark Cluster.\n",
    "3.  **Arguments:** Pass runtime parameters (like `rundate`) dynamically via the command line.\n",
    "\n",
    "## Objective\n",
    "In this notebook, we will simulate a production **Incremental Load** for a new date: **2022-01-04**.\n",
    "1.  **Generate Data:** Create incremental source files for Store and Customer dimensions.\n",
    "2.  **Convert Code:** Explain how to convert notebooks to scripts.\n",
    "3.  **Execute Pipeline:** Run the loads for Store (SCD1) and Customer (SCD2).\n",
    "4.  **Validate:** Verify that Store data was updated in-place and Customer data preserved history."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b8960c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initialize Spark Session for data generation and validation\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Incremental Load Setup\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Job Parameters\n",
    "run_date = \"20220104\"\n",
    "base_path = \"s3a://warehouse/\" \n",
    "\n",
    "print(f\"Preparing Incremental Load for: {run_date}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf94c9c0",
   "metadata": {},
   "source": [
    "## 1. Generate Incremental Data\n",
    "We simulate the arrival of new data files for **Store** and **Customer**.\n",
    "\n",
    "**Store Data (SCD Type 1 Changes):**\n",
    "*   Store 5004 & 5006 have updated phone numbers.\n",
    "*   Expectation: The existing records in `dim_store` should be updated.\n",
    "\n",
    "**Customer Data (SCD Type 2 Changes):**\n",
    "*   Customer C003 changes Plan Type.\n",
    "*   Customer C014 changes Phone Number.\n",
    "*   Expectation: New active records created, old records marked inactive."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9445d76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Create Store Incremental File ---\n",
    "store_data = [\n",
    "    (\"5004\", \"Pet House OR\", \"321 Birch Blvd\", \"Small Town\", \"OR\", \"76684\", \"91-88822-11111\"), # Phone Changed\n",
    "    (\"5006\", \"Pet House JK\", \"987 Cedar Rd\", \"Hill Town\", \"JK\", \"22222\", \"91-33333-44444\")  # Phone Changed\n",
    "]\n",
    "store_cols = [\"store_id\", \"store_name\", \"address\", \"city\", \"state\", \"zip\", \"phone\"]\n",
    "\n",
    "df_store_inc = spark.createDataFrame(store_data, store_cols)\n",
    "store_path = f\"{base_path}landing/source/store/store_{run_date}.csv\"\n",
    "\n",
    "# Write CSV\n",
    "df_store_inc.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(store_path.replace(f\"store_{run_date}.csv\", \"\"))\n",
    "print(f\"Store Incremental Data created at: {store_path}\")\n",
    "\n",
    "# --- 2. Create Customer Incremental File ---\n",
    "cust_data = [\n",
    "    (\"C003\", \"Imtiaz Ali\", \"789 Oak Ave\", \"Bigcity\", \"JK\", \"9876\", \"91-00000-00002\", \"imtiaz@email.com\", \"1990-03-03\", \"Platinum\"), # Plan Changed to Platinum\n",
    "    (\"C014\", \"Madison White\", \"687 Elm St\", \"Smallville\", \"MH\", \"66555\", \"91-99999-88888\", \"madisonwhite@email.com\", \"1982-02-02\", \"Basic\") # Phone Changed\n",
    "]\n",
    "cust_cols = [\"customer_id\", \"name\", \"address\", \"city\", \"state\", \"zip_code\", \"phone_number\", \"email\", \"date_of_birth\", \"plan_type\"]\n",
    "\n",
    "df_cust_inc = spark.createDataFrame(cust_data, cust_cols)\n",
    "cust_path = f\"{base_path}landing/source/customer/customer_{run_date}.csv\"\n",
    "\n",
    "# Write CSV\n",
    "df_cust_inc.coalesce(1).write.mode(\"overwrite\").option(\"header\", \"true\").csv(cust_path.replace(f\"customer_{run_date}.csv\", \"\"))\n",
    "print(f\"Customer Incremental Data created at: {cust_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c9be35e",
   "metadata": {},
   "source": [
    "## 2. Convert Notebooks to Scripts\n",
    "In a production environment, we use the `jupyter nbconvert` utility to transform our interactive `.ipynb` files into executable `.py` scripts.\n",
    "\n",
    "*Note: The command below is for demonstration. It assumes the notebooks exist in the current directory.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "316babba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Example commands to convert notebooks (These are shell commands)\n",
    "# !jupyter nbconvert --to script 12_Store_Dimension_Load_End_to_End.ipynb\n",
    "# !jupyter nbconvert --to script 13_Customer_Dimension_Load_SCD2.ipynb\n",
    "\n",
    "print(\"To convert notebooks to scripts, run:\")\n",
    "print(\"jupyter nbconvert --to script <notebook_name>.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9186fbe9",
   "metadata": {},
   "source": [
    "## 3. Execute Pipeline via Spark-Submit\n",
    "We use `spark-submit` to trigger the jobs. Crucially, we modify our utility code to accept the `run_date` as a command-line argument (`sys.argv[1]`) instead of reading a config file.\n",
    "\n",
    "### Simulated Execution\n",
    "Since we are inside a notebook, we will simulate the execution logic by calling the specific loads for the new rundate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3773a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- SIMULATING THE INCREMENTAL BATCH RUN ---\n",
    "\n",
    "# In reality, you would run:\n",
    "# !spark-submit master local[*] load_store_dim.py 20220104\n",
    "# !spark-submit master local[*] load_customer_dim.py 20220104\n",
    "\n",
    "# Here, we will define a helper to run the logic for the new date\n",
    "\n",
    "def run_incremental_validation(table_name, run_date):\n",
    "    print(f\"--- Validating Load for {table_name} on {run_date} ---\")\n",
    "    df = spark.sql(f\"SELECT * FROM pyspark_warehouse.{table_name}\")\n",
    "    \n",
    "    if \"dim_store\" in table_name:\n",
    "        # Check for updated phone numbers\n",
    "        df.filter(col(\"store_id\").isin([\"5004\", \"5006\"])).show(truncate=False)\n",
    "    elif \"dim_customer\" in table_name:\n",
    "        # Check for history preservation (SCD2)\n",
    "        df.filter(col(\"customer_id\").isin([\"C003\", \"C014\"])).orderBy(\"customer_id\", \"effective_start_dt\").show(truncate=False)\n",
    "\n",
    "# Note: The actual loading logic would repeat the code from notebooks 12 and 13\n",
    "# but filtered for the new date. For the purpose of this tutorial, \n",
    "# assume the scripts have run externally or the data is loaded via the simulated logic below."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c70ee7b",
   "metadata": {},
   "source": [
    "## 4. Validation Results\n",
    "After the batch jobs complete, we query the Data Warehouse to ensure the incremental logic worked correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d01cd9f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Validate Store (SCD1) ---\n",
    "print(\"Validating Store Dimension (SCD Type 1 - Updates):\")\n",
    "# We expect the phone numbers to be the NEW values from 20220104 file\n",
    "# Old values should be overwritten.\n",
    "# (Simulated query assuming data was loaded)\n",
    "# spark.sql(\"SELECT store_id, phone, update_dt FROM pyspark_warehouse.dim_store WHERE store_id IN ('5004', '5006')\").show()\n",
    "\n",
    "print(\"Validating Customer Dimension (SCD Type 2 - History):\")\n",
    "# We expect 2 rows for C003 and C014:\n",
    "# 1. Old row: active_flg='N', effective_end_dt = timestamp\n",
    "# 2. New row: active_flg='Y', effective_end_dt = 9999-12-31\n",
    "# (Simulated query assuming data was loaded)\n",
    "# spark.sql(\"SELECT customer_id, plan_type, phone_number, active_flg, effective_start_dt, effective_end_dt FROM pyspark_warehouse.dim_customer WHERE customer_id IN ('C003', 'C014') ORDER BY customer_id, effective_start_dt\").show()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
