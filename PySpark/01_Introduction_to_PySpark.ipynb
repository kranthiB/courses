{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01d76265",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 1: Introduction to Apache Spark\n",
    "\n",
    "Welcome to the \"PySpark Zero to Hero\" course. In this series, we will move from the absolute basics of Spark to advanced production-level scenarios.\n",
    "\n",
    "### Why this Course?\n",
    "Before writing our first line of code, it is essential to understand why PySpark is a critical skill in the modern data landscape:\n",
    "\n",
    "1.  **Industry Demand:** Spark is the de-facto standard for big data processing, widely used by Data Engineers, Analysts, and Scientists.\n",
    "2.  **Beyond Syntax:** With the rise of Generative AI, writing code has become easier. However, the real value lies in understanding **architecture**â€”knowing *how* distributed computing works and *why* we choose specific optimization strategies.\n",
    "3.  **Production Focus:** This course focuses not just on \"hello world\" examples, but on the tips and tricks required for real-world production environments."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "361cf282",
   "metadata": {},
   "source": [
    "## What is Apache Spark?\n",
    "\n",
    "Apache Spark is an open-source **Unified Computing Engine** designed for fast, parallel data processing on computer clusters.\n",
    "\n",
    "### Core Capabilities\n",
    "*   **Unified Engine:** It acts as a \"one-stop-shop\" for big data, supporting SQL, Streaming, Machine Learning (MLlib), and Graph processing within a single framework.\n",
    "*   **Multi-Language:** While written in Scala, Spark provides robust APIs for Python (PySpark), Java, and R.\n",
    "*   **Distributed Processing:** Spark splits data into chunks and processes them in parallel across multiple nodes (computers)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac8dbccf",
   "metadata": {},
   "source": [
    "## Why Spark Wins: Speed & Memory\n",
    "\n",
    "The most significant advantage of Spark over traditional engines like Hadoop MapReduce is performance. Spark can be up to **100x faster**.\n",
    "\n",
    "| Feature | Hadoop MapReduce | Apache Spark |\n",
    "| :--- | :--- | :--- |\n",
    "| **Processing Type** | Disk-Based | **In-Memory (RAM)** |\n",
    "| **I/O Operations** | Writes to hard disk after every map/reduce step. | Keeps data in RAM between operations. |\n",
    "| **Speed** | Slower due to heavy Disk I/O. | Blazing fast due to memory computation. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6337a038",
   "metadata": {},
   "source": [
    "## Spark Components\n",
    "\n",
    "We can visualize the Spark ecosystem in three distinct layers. Understanding this hierarchy helps when debugging performance issues later.\n",
    "\n",
    "1.  **Libraries & Ecosystem (Top Layer):**\n",
    "    *   This includes high-level tools like **Structured Streaming**, **MLlib**, and **GraphX**.\n",
    "    \n",
    "2.  **Structured APIs (Middle Layer):**\n",
    "    *   This is where we will spend 90% of our time.\n",
    "    *   Includes **DataFrames**, **Datasets**, and **Spark SQL**.\n",
    "    *   These APIs are optimized to run efficiently regardless of the language (Python/Scala) used.\n",
    "\n",
    "3.  **Low-Level APIs (Foundation):**\n",
    "    *   **RDDs (Resilient Distributed Datasets)** & Distributed Variables.\n",
    "    *   This is the assembly language of Spark. Even when we use DataFrames, Spark compiles them down to RDDs for execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c0cd84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Setting up the PySpark Environment\n",
    "# If running in Colab/Local, ensure pyspark is installed: pip install pyspark\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Create the SparkSession\n",
    "# This is the entry point to programming Spark with the Dataset and DataFrame API.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"PySpark_Zero_To_Hero_Init\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Validate the setup\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "\n",
    "# 3. Simple Test: Create a minimal DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Id\"])\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f8366bb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "In this notebook, we established the foundational \"What\" and \"Why\" of Apache Spark. \n",
    "\n",
    "**Key Takeaways:**\n",
    "*   Spark runs 100x faster than MapReduce by utilizing **RAM (In-Memory)** processing.\n",
    "*   The ecosystem is built on top of **RDDs**, but modern development uses **DataFrames** (Structured APIs).\n",
    "\n",
    "In the next notebook, we will dive \"Under the Hood\" to visualize how Spark physically distributes data across a cluster."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
