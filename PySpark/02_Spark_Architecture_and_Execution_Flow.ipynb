{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7b1b3ea6",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 2: Driver, Executors, and The DAG\n",
    "\n",
    "In the previous lecture, we learned that Spark is fast because it processes data in memory. Today, we answer the question: **\"How does Spark actually distribute the work?\"**\n",
    "\n",
    "Understanding the architecture is the difference between a developer who just writes code and an engineer who can optimize and debug complex pipelines.\n",
    "\n",
    "### Key Concepts We Will Cover:\n",
    "1.  **Driver & Executors**: The Boss and the Workers.\n",
    "2.  **The Hierarchy**: Jobs, Stages, and Tasks.\n",
    "3.  **Shuffle**: The expensive operation that divides stages."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7930cbf6",
   "metadata": {},
   "source": [
    "## 1. The Master-Slave Architecture\n",
    "\n",
    "Spark uses a Master-Slave architecture. You can think of it like a construction site or a classroom.\n",
    "\n",
    "### The Driver (The \"Heart\" or \"Instructor\")\n",
    "*   **Role:** The Manager.\n",
    "*   **Responsibilities:**\n",
    "    *   It is the heart of the Spark Application.\n",
    "    *   It converts your code into a logical plan and then a physical plan.\n",
    "    *   It distributes work to the Executors.\n",
    "    *   It maintains the status of the entire application.\n",
    "\n",
    "### The Executors (The \"Workers\")\n",
    "*   **Role:** The Laborers.\n",
    "*   **Responsibilities:**\n",
    "    *   They are JVM processes running on the cluster nodes.\n",
    "    *   **Execute the Code:** They run the actual tasks assigned by the Driver.\n",
    "    *   **Report Status:** They constantly report success/failure back to the Driver.\n",
    "    *   **Store Data:** They handle data storage (caching) in memory.\n",
    "\n",
    "> **Rule of Thumb:** The Driver thinks; the Executors work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab9d8c7b",
   "metadata": {},
   "source": [
    "## 2. Visualizing Parallel Processing: The Marble Example\n",
    "\n",
    "Imagine an **Instructor** (Driver) wants to count the total number of marbles inside several bags distributed among **Students** (Executors).\n",
    "\n",
    "### Step 1: Local Count (Stage 1)\n",
    "Instead of the Instructor counting every marble alone, they ask the students: *\"Count the marbles in your own bag and write the number on a piece of paper.\"*\n",
    "*   Student A counts 10.\n",
    "*   Student B counts 5.\n",
    "*   Student C counts 25.\n",
    "*   **Spark Equivalent:** This is a **Task**. Each Executor processes its own slice of data (Partition) in parallel.\n",
    "\n",
    "### Step 2: The Shuffle (Data Movement)\n",
    "To get the total, the individual counts must be brought together. One student (or the Instructor) collects all the pieces of paper.\n",
    "*   **Spark Equivalent:** This is a **Shuffle**. Data is moved between machines (or stages) to group related data together.\n",
    "\n",
    "### Step 3: Global Count (Stage 2)\n",
    "The numbers (10, 5, 25...) are summed up to get the final result (40).\n",
    "*   **Spark Equivalent:** This is the final **Aggregation**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7470a54b",
   "metadata": {},
   "source": [
    "## 3. The Execution Hierarchy\n",
    "\n",
    "When you run a command in Spark, it breaks down as follows:\n",
    "\n",
    "### A. Job\n",
    "*   A **Job** is triggered whenever you perform an **Action** (like `.count()`, `.show()`, `.collect()`, or `.write()`).\n",
    "*   If your code has no Action, no Job is created (Lazy Evaluation).\n",
    "\n",
    "### B. Stage\n",
    "*   A Job is divided into **Stages**.\n",
    "*   **The Boundary:** Stages are separated by **Shuffle** operations.\n",
    "*   If Spark can do work without moving data between nodes (e.g., filtering data), it stays in the same Stage.\n",
    "*   If Spark needs to move data (e.g., GroupBy, Join), it creates a new Stage.\n",
    "\n",
    "### C. Task\n",
    "*   A Stage is further divided into **Tasks**.\n",
    "*   **Task = Data Partition.**\n",
    "*   If you have 10 partitions of data, Stage 1 will have 10 Tasks.\n",
    "*   **Parallelism:** One Core can execute only **One Task** at a time.\n",
    "    *   *Example:* If you have 3 Executors with 2 Cores each (Total 6 Cores), Spark can run 6 Tasks simultaneously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f96b102",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder.appName(\"Spark_Architecture_Demo\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# Create dummy data representing \"Bags of Marbles\"\n",
    "# Group represents the Bag, Value represents marbles in a handful\n",
    "data = [\n",
    "    (\"Bag_A\", 10), (\"Bag_A\", 20),  # Bag A total: 30\n",
    "    (\"Bag_B\", 5),  (\"Bag_B\", 5),   # Bag B total: 10\n",
    "    (\"Bag_C\", 25), (\"Bag_C\", 25)   # Bag C total: 50\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"Bag_ID\", \"Marble_Count\"])\n",
    "\n",
    "print(\"--- Data Distribution (Tasks in Stage 1) ---\")\n",
    "df.show()\n",
    "\n",
    "# Triggering a Job with a Shuffle (GroupBy)\n",
    "print(\"--- Calculating Totals (Stage 2 after Shuffle) ---\")\n",
    "df_grouped = df.groupBy(\"Bag_ID\").agg(sum(\"Marble_Count\").alias(\"Total_Marbles\"))\n",
    "\n",
    "# This Action triggers the Job\n",
    "df_grouped.show()\n",
    "\n",
    "# Detailed Explanation:\n",
    "# 1. Spark reads the data (Stage 1).\n",
    "# 2. Executors sum the marbles locally for each partition (Task).\n",
    "# 3. A SHUFFLE occurs to move all \"Bag_A\" data to one node, \"Bag_B\" to another.\n",
    "# 4. The final sum is calculated (Stage 2)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bdf00d1",
   "metadata": {},
   "source": [
    "## Summary: The flow of Execution\n",
    "\n",
    "1.  **User** submits a script.\n",
    "2.  **Driver** starts, builds the plan, and creates a **Job**.\n",
    "3.  **Driver** splits the Job into **Stages** based on where data needs to be shuffled.\n",
    "4.  **Driver** splits Stages into **Tasks** based on data partitions.\n",
    "5.  **Driver** schedules Tasks on **Executors**.\n",
    "6.  **Executors** (using their Cores) run the tasks and return results.\n",
    "\n",
    "**Coming Up Next:**\n",
    "In the next notebook, we will look at the **DAG (Directed Acyclic Graph)**, Execution Plans, and the difference between `SparkSession` and `SparkContext`."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
