{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "73c8986a",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 3: Core Spark Concepts\n",
    "\n",
    "Before we start writing complex code, we need to understand the vocabulary of Spark. If you don't understand these five concepts, you cannot write efficient Spark applications.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Partitions:** How data is split.\n",
    "2.  **Transformations:** The instructions (Logic).\n",
    "3.  **Actions:** The triggers (Execution).\n",
    "4.  **Lazy Evaluation:** Why Spark waits to run.\n",
    "5.  **SparkSession:** The entry point."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e280f090",
   "metadata": {},
   "source": [
    "## 1. What is a Partition?\n",
    "\n",
    "To allow Executors to work in parallel, Spark breaks the data down into chunks called **Partitions**.\n",
    "\n",
    "*   **Analogy:** Recall the \"Bag of Marbles\" from the previous lecture. The marbles were inside small **pouches**.\n",
    "*   **The Logic:**\n",
    "    *   1 Pouch = 1 Partition.\n",
    "    *   If you have 4 Pouches (Partitions), Spark can assign them to 4 Tasks running on 4 Cores simultaneously.\n",
    "    \n",
    "> **Key Takeaway:** The number of partitions determines the level of parallelism. If you have a huge cluster but only 1 partition, only 1 core will work while the rest sit idle."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59a9ba3b",
   "metadata": {},
   "source": [
    "## 2. What are Transformations?\n",
    "\n",
    "Transformations are the instructions or code used to modify data. They build the **Logical Plan**.\n",
    "*   *Examples:* `select()`, `filter()`, `groupBy()`, `withColumn()`.\n",
    "\n",
    "There are two types of Transformations:\n",
    "\n",
    "### A. Narrow Transformation\n",
    "*   **Definition:** Each input partition contributes to only **one** output partition.\n",
    "*   **Data Movement:** No data shuffling is required between nodes.\n",
    "*   *Examples:* `filter()`, `select()`, `map()`.\n",
    "*   *Scenario:* If you filter for `Salary > 10000`, the Executor can check its own partition without talking to other Executors.\n",
    "\n",
    "### B. Wide Transformation\n",
    "*   **Definition:** Input partitions contribute to **many** output partitions.\n",
    "*   **Data Movement:** This triggers a **Shuffle**. Data must move across the network to group related data together.\n",
    "*   *Examples:* `groupBy()`, `join()`, `distinct()`, `orderBy()`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fb1ce58",
   "metadata": {},
   "source": [
    "## 3. Lazy Evaluation & Actions\n",
    "\n",
    "Spark is **Lazy**. When you tell Spark to `filter` or `groupBy`, it does **not** execute the code immediately. Instead, it records the instructions in a **DAG (Directed Acyclic Graph)**.\n",
    "\n",
    "### The Sandwich Shop Analogy\n",
    "Imagine you go to a sandwich shop:\n",
    "1.  **Instruction 1:** \"I want a sandwich.\"\n",
    "2.  **Instruction 2:** \"Use white bread.\"\n",
    "3.  **Instruction 3 (Change of mind):** \"Actually, change that to brown bread.\"\n",
    "4.  **Action:** You pay for the order.\n",
    "\n",
    "*   **If the chef wasn't lazy:** They would have started making the white bread sandwich immediately (Step 2), then thrown it away when you changed your mind (Step 3). This wastes resources.\n",
    "*   **Because the chef is lazy (Spark):** They wait until you pay (**Action**). They see the full list of instructions, realize you ultimately want brown bread, and make the sandwich correctly the first time.\n",
    "\n",
    "### What is an Action?\n",
    "An Action is the command that forces Spark to execute the plan (The \"Payment\").\n",
    "*   *Examples:*\n",
    "    1.  **View Data:** `.show()`, `.take()`\n",
    "    2.  **Collect Data:** `.collect()` (brings data to Driver - careful!)\n",
    "    3.  **Write Data:** `.write.csv()`, `.saveAsTable()`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b18a752b",
   "metadata": {},
   "source": [
    "## 4. What is SparkSession?\n",
    "\n",
    "*   The **SparkSession** is the entry point for writing Spark applications.\n",
    "*   It represents the **Driver** process.\n",
    "*   **One-to-One Relationship:** For 1 Spark Application, there is 1 SparkSession.\n",
    "*   It coordinates the execution of code on the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5c4dd0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Initialize SparkSession ( The Driver / Entry Point )\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Lazy_Evaluation_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. Create Data (Partitions)\n",
    "# We create a list of numbers\n",
    "data = [(\"James\", \"Sales\", 3000),\n",
    "        (\"Michael\", \"Sales\", 4600),\n",
    "        (\"Robert\", \"Sales\", 4100),\n",
    "        (\"Maria\", \"Finance\", 3000),\n",
    "        (\"James\", \"Sales\", 3000),\n",
    "        (\"Scott\", \"Finance\", 3300),\n",
    "        (\"Jen\", \"Finance\", 3900),\n",
    "        (\"Jeff\", \"Marketing\", 3000),\n",
    "        (\"Kumar\", \"Marketing\", 2000),\n",
    "        (\"Saif\", \"Sales\", 4100)]\n",
    "\n",
    "columns = [\"Employee_Name\", \"Department\", \"Salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Check Partitions (accessing the underlying RDD)\n",
    "print(f\"Number of Partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 3. Transformations (LAZY - Nothing happens here yet!)\n",
    "# Step A: Filter (Narrow Transformation)\n",
    "df_filtered = df.filter(df[\"Salary\"] > 3000)\n",
    "\n",
    "# Step B: GroupBy (Wide Transformation - triggers Shuffle plan)\n",
    "df_grouped = df_filtered.groupBy(\"Department\").count()\n",
    "\n",
    "print(\"Transformations defined. logical plan created. No execution yet.\")\n",
    "\n",
    "# 4. Action (The Trigger)\n",
    "# This forces Spark to look at the plan and execute it.\n",
    "print(\"--- Triggering Action (.show) ---\")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db47ef80",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "*   **Partitions** are the units of parallelism.\n",
    "*   **Transformations** build the plan but don't run it (Lazy).\n",
    "*   **Narrow Transformations** are fast (no network traffic).\n",
    "*   **Wide Transformations** are slow (Shuffle / network traffic).\n",
    "*   **Actions** trigger the actual execution.\n",
    "\n",
    "**Coming Up Next:**\n",
    "In the next notebook, we will visualize the **DAG (Directed Acyclic Graph)**, understand **Structured APIs**, and see how Spark designs the **Execution Plan**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
