{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1794d84",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 4: DataFrames and Execution Plans\n",
    "\n",
    "In this module, we look at the core data structure of PySpark: the **DataFrame**. We also explore the \"magic\" that happens inside Spark when you run a queryâ€”specifically, how the **Catalyst Optimizer** turns your Python code into efficient physical machine code.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Structured APIs:** What is a DataFrame?\n",
    "2.  **Immutability:** Why can't we change data?\n",
    "3.  **The Life Cycle of a Spark Job:**\n",
    "    *   Logical Planning (The \"What\")\n",
    "    *   Physical Planning (The \"How\")\n",
    "4.  **The DAG:** Directed Acyclic Graph."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e7028f6",
   "metadata": {},
   "source": [
    "## 1. What is a DataFrame?\n",
    "\n",
    "A DataFrame is the most common Structured API in Spark.\n",
    "*   **Structure:** It looks exactly like a Table in a database or Excel. It has **Rows** and **Columns**.\n",
    "*   **Schema:** It has a defined schema (column names and data types).\n",
    "*   **Distributed:** Unlike a Pandas DataFrame (which sits on one computer), a Spark DataFrame is split into **Partitions** and spread across the cluster.\n",
    "\n",
    "> **Note:** DataFrames are built on top of RDDs. They are easier to use and much faster due to the optimization engine."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a30ef3f",
   "metadata": {},
   "source": [
    "## 2. Immutability\n",
    "\n",
    "DataFrames are **Immutable**. This means once you create a DataFrame, **you cannot change it**.\n",
    "\n",
    "*   **Scenario:** You have `df1` and you want to filter it.\n",
    "*   **Action:** Spark does *not* modify `df1`. Instead, it creates a new DataFrame `df2` that contains the filtered results.\n",
    "*   **Why?** This is crucial for fault tolerance in distributed computing. If a node crashes, Spark knows exactly how to recreate the data from the original immutable source."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff6ffcfe",
   "metadata": {},
   "source": [
    "## 3. How Spark Compiles Code: The Execution Plan\n",
    "\n",
    "When you write `df.select().filter()`, Spark does not run it immediately. It goes through four distinct phases to optimize your code. This process is handled by the **Catalyst Optimizer**.\n",
    "\n",
    "### Phase 1: Unresolved Logical Plan\n",
    "*   Spark checks your syntax.\n",
    "*   It knows you want to query a table named \"Sales\", but it doesn't know if that table actually exists or if the columns are correct.\n",
    "\n",
    "### Phase 2: Resolved Logical Plan\n",
    "*   Spark looks up the **Catalog** (metadata repository).\n",
    "*   It verifies: *\"Does the 'Sales' table exist? Does column 'Amount' exist?\"*\n",
    "*   If yes, the plan is \"Resolved\". If no, it throws an AnalysisException.\n",
    "\n",
    "### Phase 3: Optimized Logical Plan\n",
    "*   The Catalyst Optimizer applies rules to make queries faster.\n",
    "*   *Example:* If you `filter` data and then `select` a column, Spark will swap them to `filter` first (Predicate Pushdown) to reduce the data volume as early as possible.\n",
    "\n",
    "### Phase 4: Physical Plan\n",
    "*   Spark generates multiple physical plans (different ways to actually do the job on the hardware).\n",
    "*   **Cost Model:** It calculates the \"cost\" (CPU/RAM usage) of each plan.\n",
    "*   It selects the **Best Physical Plan** and sends it to the Executors."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abe80ea",
   "metadata": {},
   "source": [
    "## 4. The DAG (Directed Acyclic Graph)\n",
    "\n",
    "The final plan is visualized as a **DAG**.\n",
    "*   **Directed:** It flows in one direction (Input -> Output).\n",
    "*   **Acyclic:** It does not loop back on itself.\n",
    "*   **Graph:** A visual representation of steps.\n",
    "\n",
    "We will see this in the Spark UI (localhost:4040) when we run code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8de0ef8f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Execution_Plan_Deep_Dive\").master(\"local[*]\").getOrCreate()\n",
    "\n",
    "# 1. Create a simple DataFrame\n",
    "data = [(\"A\", 10), (\"B\", 20), (\"C\", 30), (\"A\", 40)]\n",
    "df = spark.createDataFrame(data, [\"Id\", \"Value\"])\n",
    "\n",
    "# 2. Define Transformations (Logical Plan builds here)\n",
    "df_filtered = df.filter(df[\"Value\"] > 15)\n",
    "df_result = df_filtered.select(\"Id\", \"Value\")\n",
    "\n",
    "# 3. Explain the Plan\n",
    "# This command shows the Physical Plan (and optionally the Logical Plans)\n",
    "print(\"--- Physical Plan ---\")\n",
    "df_result.explain()\n",
    "\n",
    "# To see the full details (Logical + Physical), uncomment below:\n",
    "# print(\"\\n--- Extended Plan (Logical + Physical) ---\")\n",
    "# df_result.explain(extended=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0dc7197b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **DataFrames** are distributed tables with a schema.\n",
    "2.  You don't modify DataFrames; you transform them into **new** DataFrames (Immutability).\n",
    "3.  **Catalyst Optimizer** is the brain of Spark SQL. It optimizes your code via:\n",
    "    *   Unresolved -> Resolved -> Optimized -> Physical Plan.\n",
    "4.  **Physical Plan** is what actually runs on the cluster.\n",
    "\n",
    "**Next Steps:**\n",
    "Now that we understand the theory of how Spark processes data, we are ready to set up our environment. Please proceed to **Notebook 5: Environment Setup**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
