{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a2ab80ae",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 5: Environment Setup (Docker Cluster)\n",
    "\n",
    "In this module, we will set up a professional PySpark environment on your local machine using **Docker**.\n",
    "\n",
    "### Why Docker?\n",
    "*   **Consistency:** It works exactly the same on Windows, Mac, and Linux.\n",
    "*   **Cluster Simulation:** We can simulate a real production environment (Master Node + Worker Nodes) on a single laptop.\n",
    "*   **No Mess:** It doesn't install Java, Scala, or Python globally on your laptop, keeping your OS clean.\n",
    "\n",
    "### Prerequisites\n",
    "1.  **Docker Desktop:** Download and install from [docker.com](https://www.docker.com/products/docker-desktop/).\n",
    "2.  **Git:** (Optional but recommended) to clone the repository."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367a48c7",
   "metadata": {},
   "source": [
    "## Option A: Standalone Mode (Quick Start)\n",
    "Use this if you just want to run code quickly without simulating a full cluster.\n",
    "\n",
    "**Steps:**\n",
    "1.  Open your Terminal (Mac/Linux) or Command Prompt (Windows).\n",
    "2.  Run the following command to download and start the container:\n",
    "    ```bash\n",
    "    docker run -p 8888:8888 -p 4040:4040 self/pyspark-jupyter-lab-old:latest\n",
    "    ```\n",
    "3.  **Access:**\n",
    "    *   Look at the terminal logs for a URL like `http://127.0.0.1:8888/lab?token=...`\n",
    "    *   Copy that token and paste it into your browser at `localhost:8888`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a888710c",
   "metadata": {},
   "source": [
    "## Option B: Cluster Mode (Master + 2 Workers)\n",
    "**This is the recommended setup for this course.** It creates a Master node and 2 Worker nodes to simulate distributed computing.\n",
    "\n",
    "**Steps:**\n",
    "1.  **Clone the Repository:**\n",
    "    Open your terminal and run:\n",
    "    ```bash\n",
    "    git clone https://github.com/subhamkharwal/docker-images.git\n",
    "    ```\n",
    "    *(Alternatively, download the ZIP from the GitHub link if you don't have Git).*\n",
    "\n",
    "2.  **Navigate to the Folder:**\n",
    "    ```bash\n",
    "    cd docker-images/pyspark-cluster-with-jupyter\n",
    "    ```\n",
    "\n",
    "3.  **Start the Cluster:**\n",
    "    Run the Docker Compose command:\n",
    "    ```bash\n",
    "    docker-compose up\n",
    "    ```\n",
    "    *Note: The first run will take time as it downloads the images.*\n",
    "\n",
    "4.  **Access Points:**\n",
    "    *   **Jupyter Lab:** [http://localhost:8888](http://localhost:8888)\n",
    "    *   **Spark Master UI:** [http://localhost:8080](http://localhost:8080) (To see your workers)\n",
    "    *   **Spark Application UI:** [http://localhost:4040](http://localhost:4040) (Only visible when a job is running)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "766ce406",
   "metadata": {},
   "source": [
    "## Important: Where to put your data files?\n",
    "\n",
    "When running in Cluster Mode, you cannot just read files from your Desktop or Documents folder. You must place your CSV/JSON files in the specific mapped folder.\n",
    "\n",
    "1.  Inside the folder you cloned (`pyspark-cluster-with-jupyter`), there is a folder named **`data`**.\n",
    "2.  **Action:** Any file you want to read in PySpark **must be pasted into this `data` folder**.\n",
    "3.  **In Code:** When reading files, use the path `/data/filename.csv`.\n",
    "\n",
    "*Example:*\n",
    "If you put `users.csv` in the local `data` folder, PySpark reads it as:\n",
    "`spark.read.csv(\"/data/users.csv\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bfd81ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "# We don't need to specify Master here because the Docker environment sets it automatically.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Environment_Test\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")\n",
    "\n",
    "# 2. Print System Info\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "\n",
    "# 3. Test Computation\n",
    "# We create a simple range of numbers to test if the workers are functioning.\n",
    "df = spark.range(100)\n",
    "print(f\"Count Test: {df.count()}\")\n",
    "\n",
    "# 4. Check UI\n",
    "print(\"Go to http://localhost:4040 to see this job in the Spark UI.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b78686e4",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "*   **Port Conflicts:** If `docker-compose up` fails saying \"Port is already allocated\", ensure you don't have another Docker container or Service running on port 8888 or 8080.\n",
    "*   **File Not Found:** If PySpark says \"Path does not exist\", verify you pasted the file into the `data` folder inside the cloned repository, not your general system download folder.\n",
    "*   **Hidden Characters:** When copying the token from the terminal, ensure you don't copy hidden space characters or the `token=` prefix."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
