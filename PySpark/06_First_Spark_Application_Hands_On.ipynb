{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ee426ebf",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 6: Hands-on - Creating DataFrames & Transformations\n",
    "\n",
    "In this notebook, we will write our first actual PySpark code. We will simulate a real-world scenario: generating employee data, filtering it, and saving the results.\n",
    "\n",
    "### Agenda:\n",
    "1.  **SparkSession:** Creating the entry point.\n",
    "2.  **Create DataFrame:** Making a distributed dataset from a list.\n",
    "3.  **Inspecting Data:** Using Actions like `.show()`.\n",
    "4.  **Transformations:** Filtering data.\n",
    "5.  **Immutability:** Understanding how new DataFrames are created.\n",
    "6.  **Bonus Tip:** Accessing the active session."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "564e9c87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create the SparkSession Object\n",
    "# appName: Identifying your application in the Spark UI\n",
    "# master: \"local[*]\" means run locally using all available cores\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_Introduction\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1525c9a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Define the Data (List of Lists)\n",
    "# Each inner list represents a Row: [ID, DeptID, Name, Age, Gender, Salary, HireDate]\n",
    "data = [\n",
    "    [\"001\", \"101\", \"John Doe\", \"30\", \"Male\", \"50000\", \"2015-01-01\"],\n",
    "    [\"002\", \"101\", \"Jane Smith\", \"25\", \"Female\", \"45000\", \"2016-04-15\"],\n",
    "    [\"003\", \"102\", \"Bob Brown\", \"35\", \"Male\", \"55000\", \"2014-05-01\"],\n",
    "    [\"004\", \"102\", \"Alice Lee\", \"28\", \"Female\", \"48000\", \"2017-09-30\"],\n",
    "    [\"005\", \"103\", \"Jack Chan\", \"40\", \"Male\", \"60000\", \"2013-04-01\"],\n",
    "    [\"006\", \"103\", \"Jill Wong\", \"32\", \"Female\", \"52000\", \"2018-07-01\"],\n",
    "    [\"007\", \"101\", \"James Johnson\", \"42\", \"Male\", \"70000\", \"2012-03-15\"],\n",
    "    [\"008\", \"102\", \"Kate Kim\", \"29\", \"Female\", \"51000\", \"2019-10-01\"],\n",
    "    [\"009\", \"103\", \"Tom Tan\", \"33\", \"Male\", \"58000\", \"2016-06-01\"],\n",
    "    [\"010\", \"104\", \"Lisa Lee\", \"27\", \"Female\", \"47000\", \"2018-08-01\"]\n",
    "]\n",
    "\n",
    "# 2. Define the Schema (Column Names)\n",
    "# Note: For this example, we are treating everything as StringType for simplicity, \n",
    "# but in real scenarios, you would use Integer/Date types.\n",
    "columns = [\"employee_id\", \"department_id\", \"name\", \"age\", \"gender\", \"salary\", \"hire_date\"]\n",
    "\n",
    "print(\"Data and Schema defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab6c4641",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create the DataFrame using the createDataFrame method\n",
    "emp = spark.createDataFrame(data=data, schema=columns)\n",
    "\n",
    "# Validate: Check the number of partitions\n",
    "# This tells us how many chunks the data is split into\n",
    "print(f\"Number of Partitions: {emp.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69218e4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# The dataframe created above is just a plan. \n",
    "# Nothing executes until we call an Action like .show()\n",
    "print(\"--- Employee Data ---\")\n",
    "emp.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06ff6cce",
   "metadata": {},
   "source": [
    "## Transformations & Immutability\n",
    "\n",
    "We now want to filter out employees who have a **Salary > 50,000**.\n",
    "\n",
    "Since DataFrames are **Immutable**, we cannot modify the `emp` DataFrame directly. Instead, we apply a transformation which returns a **new** DataFrame (`emp_final`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46f9a62a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Apply Transformation: Filter salary > 50000\n",
    "# Note: Even though salary is a string in our schema, Spark SQL can often implicitly handle the comparison.\n",
    "emp_final = emp.where(\"salary > 50000\")\n",
    "\n",
    "# Verify: Check if the partitions changed? (They usually stay the same unless reshuffled)\n",
    "print(f\"Partitions in filtered DF: {emp_final.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Note: If you check the Spark UI now (localhost:4040), you will NOT see a job for this filter yet.\n",
    "# That is Lazy Evaluation.\n",
    "print(\"Transformation defined (Lazy Evaluation). No Job triggered yet.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bba50c5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Now we call an Action to see the result\n",
    "print(\"--- Employees with Salary > 50,000 ---\")\n",
    "emp_final.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d15ab5d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Writing data is also an Action.\n",
    "# We write the filtered data to a CSV file.\n",
    "\n",
    "# 'overwrite' mode ensures we can run this cell multiple times without error.\n",
    "emp_final.write.mode(\"overwrite\").csv(\"data/output/high_salary_employees.csv\")\n",
    "\n",
    "print(\"Data written successfully to data/output/high_salary_employees.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "119d502a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus Tip:\n",
    "# Sometimes you are in a function where the 'spark' variable isn't available.\n",
    "# You can grab the existing active SparkSession without creating a new one.\n",
    "\n",
    "new_spark_ref = SparkSession.getActiveSession()\n",
    "\n",
    "print(f\"Original Object: {spark}\")\n",
    "print(f\"New Reference:   {new_spark_ref}\")\n",
    "\n",
    "# They point to the exact same memory address\n",
    "print(f\"Are they the same object? {spark is new_spark_ref}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbaeaec6",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **SparkSession** is the heart of the application.\n",
    "2.  **DataFrames** represent data in rows and columns but are distributed.\n",
    "3.  **Lazy Evaluation:** `emp.where(...)` did not run immediately. It waited for `.show()` or `.write()`.\n",
    "4.  **Immutability:** We created `emp_final` rather than modifying `emp`.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next video, we will dive deeper into **Read and Write modes** (CSV, Parquet, JSON) and understand schemas in depth."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
