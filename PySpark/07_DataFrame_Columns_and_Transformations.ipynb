{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "adfe8b4c",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 7: Columns, Schemas, and Structured Transformations\n",
    "\n",
    "In the previous module, we created a simple DataFrame. In this session, we dive deep into the structure of a DataFrame: **Rows, Columns, and Schemas**.\n",
    "\n",
    "We will learn how to manipulate data using the most common transformations.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Defining Schemas:** `StructType` vs. DDL Strings.\n",
    "2.  **Column Expressions:** `col()`, `expr()`, and string references.\n",
    "3.  **Basic Selection:** Using `.select()`.\n",
    "4.  **Advanced Selection:** Using `.selectExpr()` (The SQL-like way).\n",
    "5.  **Data Type Casting:** Converting String to Integer.\n",
    "6.  **Filtering:** Using `.where()` with conditions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c149443e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import col, expr\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Structured_Transformations\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265251b8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In production, we often define schemas explicitly using StructType.\n",
    "# This ensures type safety and handles null values correctly.\n",
    "\n",
    "data = [\n",
    "    (\"001\", \"John Doe\", \"30\", \"50000\"),\n",
    "    (\"002\", \"Jane Smith\", \"25\", \"45000\"),\n",
    "    (\"003\", \"Bob Brown\", \"35\", \"55000\"),\n",
    "    (\"004\", \"Alice Lee\", \"28\", \"48000\")\n",
    "]\n",
    "\n",
    "# Explicit Schema Definition\n",
    "schema = StructType([\n",
    "    StructField(\"emp_id\", StringType(), True),  # True = Nullable\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", StringType(), True),     # Intentionally keeping age as String to demo Casting later\n",
    "    StructField(\"salary\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"--- Original Schema ---\")\n",
    "df.printSchema()\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3c5714",
   "metadata": {},
   "source": [
    "## 2. Referencing Columns\n",
    "\n",
    "In PySpark, there are three main ways to refer to a column in a transformation:\n",
    "1.  **String:** `\"salary\"` (Simplest, but limited features)\n",
    "2.  **Column Object:** `col(\"salary\")` (Most common, allows operations like `col(\"age\") + 1`)\n",
    "3.  **DataFrame Reference:** `df[\"salary\"]` or `df.salary`\n",
    "4.  **Expression:** `expr(\"salary + 100\")` (SQL style)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c458ab5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Using .select() to pick specific columns\n",
    "# We will use different reference styles here\n",
    "\n",
    "df_selected = df.select(\n",
    "    col(\"emp_id\"),          # Style 2: Column Object\n",
    "    \"name\",                 # Style 1: String\n",
    "    df.salary               # Style 3: DF Reference\n",
    ")\n",
    "\n",
    "print(\"--- Selected Columns ---\")\n",
    "df_selected.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9108e39b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# expr() allows you to write SQL fragments directly inside Python code.\n",
    "# This is useful for renaming or simple arithmetic without importing specific functions.\n",
    "\n",
    "# Example: Rename 'name' to 'full_name' using alias\n",
    "df_expr = df.select(\n",
    "    col(\"emp_id\"),\n",
    "    expr(\"name as full_name\"),    # SQL style renaming\n",
    "    expr(\"salary + 500 as bonus\") # SQL style arithmetic\n",
    ")\n",
    "\n",
    "print(\"--- Using Expressions ---\")\n",
    "df_expr.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acbf66f7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# .selectExpr() combines .select() and .expr().\n",
    "# It accepts strings that are treated as SQL expressions.\n",
    "# This is often cleaner than wrapping everything in expr().\n",
    "\n",
    "# Scenario: \n",
    "# 1. Select emp_id\n",
    "# 2. Rename name -> full_name\n",
    "# 3. CAST age from String to Integer\n",
    "\n",
    "df_casted = df.selectExpr(\n",
    "    \"emp_id\",\n",
    "    \"name as full_name\",\n",
    "    \"cast(age as int) as age_int\",  # Casting in SQL style\n",
    "    \"cast(salary as double) as salary_double\"\n",
    ")\n",
    "\n",
    "print(\"--- SelectExpr with Casting ---\")\n",
    "df_casted.printSchema()\n",
    "df_casted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abec32f8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Filtering using .where() (or .filter(), they are aliases)\n",
    "# We want employees older than 25\n",
    "\n",
    "# Note: We use the 'df_casted' dataframe because 'age_int' is now an Integer.\n",
    "df_filtered = df_casted.where(\"age_int > 25\")\n",
    "\n",
    "print(\"--- Filtered Data (Age > 25) ---\")\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7f92705",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus Tip: Instead of writing the long StructType code (Cell 3),\n",
    "# Spark allows DDL (Data Definition Language) strings for schemas.\n",
    "\n",
    "# Simple String Schema\n",
    "ddl_schema = \"emp_id STRING, name STRING, age INT, salary DOUBLE\"\n",
    "\n",
    "data_simple = [(\"005\", \"Jack Chan\", 40, 60000.0)]\n",
    "\n",
    "df_ddl = spark.createDataFrame(data_simple, schema=ddl_schema)\n",
    "\n",
    "print(\"--- DataFrame created with DDL Schema ---\")\n",
    "df_ddl.printSchema()\n",
    "df_ddl.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d80cdd0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Schema:** Can be defined using `StructType` (Programmatic) or DDL String (Simple).\n",
    "2.  **`select()`:** Used to pick columns. Combine with `col()` for programmatic access.\n",
    "3.  **`expr()`:** Lets you write SQL snippets inside Python code.\n",
    "4.  **`selectExpr()`:** A shortcut for running SQL expressions on columns. Excellent for **Casting** data types.\n",
    "5.  **Casting:** Changing data types (e.g., String -> Int) is crucial for numerical filtering.\n",
    "\n",
    "**Next Steps:**\n",
    "We will explore more complex transformations, aggregations, and working with different file formats."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
