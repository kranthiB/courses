{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "decd5517",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 8: Advanced Column Operations & Filtering\n",
    "\n",
    "In this module, we will perform essential data manipulation tasks that are common in every ETL pipeline.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Adding Columns:** `withColumn()` & `lit()`.\n",
    "2.  **Calculated Columns:** Performing math on existing columns.\n",
    "3.  **Renaming Columns:** `withColumnRenamed()`.\n",
    "4.  **Dropping Columns:** `drop()`.\n",
    "5.  **Advanced Filtering:** `limit()`.\n",
    "6.  **Bonus:** Adding multiple columns efficiently using a loop (avoiding multiple `withColumn` calls)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cc5e14c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, expr\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Advanced_Column_Ops\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Create Dummy Data\n",
    "data = [\n",
    "    (\"001\", \"John Doe\", \"30\", \"50000\"),\n",
    "    (\"002\", \"Jane Smith\", \"25\", \"45000\"),\n",
    "    (\"003\", \"Bob Brown\", \"35\", \"55000\"),\n",
    "    (\"004\", \"Alice Lee\", \"28\", \"48000\")\n",
    "]\n",
    "columns = [\"emp_id\", \"name\", \"age\", \"salary\"]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Cast salary to Double for calculations\n",
    "df = df.withColumn(\"salary\", col(\"salary\").cast(\"double\"))\n",
    "\n",
    "print(\"--- Original DataFrame ---\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8831159f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Calculated Column: Add a 'tax' column (20% of salary)\n",
    "# 2. Static Column: Add a 'country' column with value 'USA' using lit()\n",
    "\n",
    "# lit() stands for Literal. It is used to add a constant value to a DataFrame.\n",
    "\n",
    "df_added = df \\\n",
    "    .withColumn(\"tax\", col(\"salary\") * 0.2) \\\n",
    "    .withColumn(\"country\", lit(\"USA\"))\n",
    "\n",
    "print(\"--- Added Tax and Country ---\")\n",
    "df_added.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c23ab98e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Renaming 'emp_id' to 'id' and 'name' to 'full_name'\n",
    "# We use withColumnRenamed(existing_name, new_name)\n",
    "\n",
    "df_renamed = df_added \\\n",
    "    .withColumnRenamed(\"emp_id\", \"id\") \\\n",
    "    .withColumnRenamed(\"name\", \"full_name\")\n",
    "\n",
    "print(\"--- Renamed Columns ---\")\n",
    "df_renamed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7921f7",
   "metadata": {},
   "source": [
    "> **Pro Tip:** Although Spark allows spaces in column names (e.g., \"Tax Amount\"), it is highly recommended to avoid them. Downstream systems (like Parquet files, Hive, or some SQL databases) often fail or require complex escaping when handling spaces. Stick to `snake_case` (e.g., `tax_amount`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d43afd41",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Removing columns we don't need.\n",
    "# Let's drop 'tax' and 'country' to return to a cleaner state.\n",
    "\n",
    "# You can drop single or multiple columns\n",
    "df_dropped = df_renamed.drop(\"tax\", \"country\")\n",
    "\n",
    "print(\"--- Dropped Columns ---\")\n",
    "df_dropped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d6fbb95",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sometimes you don't want to process the whole dataset, just a sample.\n",
    "# limit(n) returns n rows from the DataFrame.\n",
    "\n",
    "# Get top 2 employees\n",
    "df_limited = df_dropped.limit(2)\n",
    "\n",
    "print(\"--- Top 2 Rows ---\")\n",
    "df_limited.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb9a81c0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Problem: Chaining multiple .withColumn() calls can be inefficient and messy \n",
    "# if you have 50 columns to add.\n",
    "\n",
    "# Solution: Use a Loop or Dictionary to add multiple columns dynamically.\n",
    "\n",
    "# Define a dictionary of new columns {name: value/expression}\n",
    "new_columns = {\n",
    "    \"bonus\": col(\"salary\") * 0.1,\n",
    "    \"is_active\": lit(True),\n",
    "    \"department\": lit(\"IT\")\n",
    "}\n",
    "\n",
    "# Use a loop to apply them\n",
    "df_dynamic = df_dropped\n",
    "for col_name, col_val in new_columns.items():\n",
    "    df_dynamic = df_dynamic.withColumn(col_name, col_val)\n",
    "\n",
    "print(\"--- Dynamically Added Multiple Columns ---\")\n",
    "df_dynamic.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77608d6b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **`withColumn(name, logic)`**: Used to add a new column or overwrite an existing one.\n",
    "2.  **`lit(value)`**: Essential function to add static/constant values.\n",
    "3.  **`withColumnRenamed(old, new)`**: Renames a column.\n",
    "4.  **`drop(col1, col2)`**: Removes columns.\n",
    "5.  **`limit(n)`**: Returns top n rows (Action-like behavior but returns a DataFrame).\n",
    "6.  **Dynamic Columns**: Using loops with dictionaries keeps code clean when adding many columns.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **File I/O**: Reading and Writing CSV, Parquet, and JSON files, and understanding the different **Save Modes**."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
