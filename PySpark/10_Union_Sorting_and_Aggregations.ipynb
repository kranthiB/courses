{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4bf5510f",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 10: Union, Sorting, and Aggregations\n",
    "\n",
    "In this module, we move from row-level transformations to set-level operations and summarization.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Union:** Combining two DataFrames (Appending).\n",
    "2.  **Sorting:** Ordering data using `orderBy` and `sort`.\n",
    "3.  **Aggregations:** Summarizing data using `groupBy`, `sum`, `avg`, and `count`.\n",
    "4.  **Bonus:** Handling Unions when column order is different (`unionByName`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947ae82a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, avg, count, desc, asc\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Union_Sort_Agg\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Prepare Data: We will create TWO identical schemas but different data to demonstrate Union\n",
    "columns = [\"emp_id\", \"name\", \"department\", \"salary\"]\n",
    "\n",
    "data_1 = [\n",
    "    (\"001\", \"John Doe\", \"IT\", 50000),\n",
    "    (\"002\", \"Jane Smith\", \"HR\", 45000)\n",
    "]\n",
    "\n",
    "data_2 = [\n",
    "    (\"003\", \"Bob Brown\", \"IT\", 55000),\n",
    "    (\"004\", \"Alice Lee\", \"Sales\", 48000),\n",
    "    (\"001\", \"John Doe\", \"IT\", 50000) # Duplicate record for testing\n",
    "]\n",
    "\n",
    "df1 = spark.createDataFrame(data_1, columns)\n",
    "df2 = spark.createDataFrame(data_2, columns)\n",
    "\n",
    "print(\"--- DataFrame 1 ---\")\n",
    "df1.show()\n",
    "\n",
    "print(\"--- DataFrame 2 ---\")\n",
    "df2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23241dcf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Union combines rows from two DataFrames.\n",
    "# Requirement: Both DataFrames must have the SAME number of columns.\n",
    "# Note: In PySpark, 'union' behaves like SQL 'UNION ALL' (it keeps duplicates).\n",
    "\n",
    "df_union = df1.union(df2)\n",
    "\n",
    "print(\"--- Unioned DataFrame (Contains Duplicates) ---\")\n",
    "df_union.show()\n",
    "\n",
    "# To remove duplicates (Simulating SQL 'UNION'), use .distinct()\n",
    "df_unique = df_union.distinct()\n",
    "\n",
    "print(\"--- Distinct Union (No Duplicates) ---\")\n",
    "df_unique.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8591fceb",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sorting data using orderBy (or sort - they are aliases).\n",
    "# Let's sort by Salary in Descending order.\n",
    "\n",
    "# Method 1: Using col() object (Recommended)\n",
    "df_sorted = df_union.orderBy(col(\"salary\").desc())\n",
    "\n",
    "# Method 2: Using String syntax (Simple)\n",
    "# df_sorted = df_union.orderBy(\"salary\", ascending=False)\n",
    "\n",
    "print(\"--- Sorted by Salary (Desc) ---\")\n",
    "df_sorted.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99cc6b8d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Aggregations are used to summarize data.\n",
    "# Scenario: Count number of employees per department.\n",
    "\n",
    "df_grouped = df_union.groupBy(\"department\").count()\n",
    "\n",
    "print(\"--- Count per Department ---\")\n",
    "df_grouped.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34d36fef",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We often need multiple metrics at once (e.g., Total Salary AND Average Salary).\n",
    "# We use the .agg() function for this.\n",
    "\n",
    "df_summary = df_union.groupBy(\"department\").agg(\n",
    "    sum(\"salary\").alias(\"total_salary\"),\n",
    "    avg(\"salary\").alias(\"avg_salary\"),\n",
    "    count(\"emp_id\").alias(\"emp_count\")\n",
    ")\n",
    "\n",
    "print(\"--- Department Summary ---\")\n",
    "df_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01614c57",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In SQL, we use 'HAVING' to filter after aggregation.\n",
    "# In PySpark, we simply chain a .where() method after the aggregation.\n",
    "\n",
    "# Scenario: Show departments where Total Salary > 50,000\n",
    "df_high_budget = df_summary.where(col(\"total_salary\") > 50000)\n",
    "\n",
    "print(\"--- Departments with Budget > 50k ---\")\n",
    "df_high_budget.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be8e1dad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Problem: Standard .union() matches columns by position, not name.\n",
    "# If df1 has [\"id\", \"name\"] and df2 has [\"name\", \"id\"], .union() will corrupt the data.\n",
    "\n",
    "# Solution: Use .unionByName() to match columns safely.\n",
    "\n",
    "# Create df3 with different column order\n",
    "data_3 = [(\"Marketing\", 60000, \"005\", \"Mike Ross\")]\n",
    "columns_3 = [\"department\", \"salary\", \"emp_id\", \"name\"] # Different order\n",
    "\n",
    "df3 = spark.createDataFrame(data_3, columns_3)\n",
    "\n",
    "# Safe Union\n",
    "df_safe_union = df1.unionByName(df3)\n",
    "\n",
    "print(\"--- Safe Union (Column Order Auto-Resolved) ---\")\n",
    "df_safe_union.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79629c32",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **`union()`**: Appends data. Does **not** remove duplicates by default (acts like SQL `UNION ALL`).\n",
    "2.  **`distinct()`**: Removes duplicates.\n",
    "3.  **`orderBy(col.desc())`**: Sorts data.\n",
    "4.  **`groupBy().agg()`**: The standard pattern for calculating sums, averages, and counts.\n",
    "5.  **`unionByName()`**: Essential when datasets have the same columns but in different orders.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will cover **Joins** (Inner, Left, Right, Full) and handling ambiguous columns."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
