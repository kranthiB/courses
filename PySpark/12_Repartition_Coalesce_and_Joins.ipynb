{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32da8539",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 12: Partitioning Strategies & Joins\n",
    "\n",
    "In this module, we cover two critical topics:\n",
    "1.  **Partitioning:** Controlling how data is physically distributed across the cluster using `repartition()` and `coalesce()`.\n",
    "2.  **Joins:** Combining two DataFrames using Inner and Left joins, and handling duplicate column names.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Data Creation:** Employee and Department datasets.\n",
    "2.  **Partitioning:**\n",
    "    *   `repartition()` vs `coalesce()`.\n",
    "    *   Partitioning by Column.\n",
    "    *   Visualizing partition distribution using `spark_partition_id()`.\n",
    "3.  **Joins:**\n",
    "    *   Inner Join.\n",
    "    *   Left Join.\n",
    "    *   Handling **Ambiguous Columns** (The most common Join error)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642f73af",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit, spark_partition_id\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partitions_and_Joins\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Employee Data\n",
    "emp_data = [\n",
    "    (\"001\", \"John Doe\", \"101\", 50000),\n",
    "    (\"002\", \"Jane Smith\", \"102\", 60000),\n",
    "    (\"003\", \"Bob Brown\", \"101\", 55000),\n",
    "    (\"004\", \"Alice Lee\", \"103\", 52000),\n",
    "    (\"005\", \"Jack Chan\", \"102\", 48000),\n",
    "    (\"006\", \"N/A\", \"104\", 40000) # Dept 104 exists in Emp but not Dept\n",
    "]\n",
    "emp_cols = [\"emp_id\", \"name\", \"dept_id\", \"salary\"]\n",
    "emp_df = spark.createDataFrame(emp_data, emp_cols)\n",
    "\n",
    "# 2. Department Data\n",
    "dept_data = [\n",
    "    (\"101\", \"HR\", \"NY\"),\n",
    "    (\"102\", \"Finance\", \"CA\"),\n",
    "    (\"103\", \"Marketing\", \"TX\"),\n",
    "    (\"105\", \"Sales\", \"FL\") # Dept 105 exists in Dept but not Emp\n",
    "]\n",
    "dept_cols = [\"dept_id\", \"dept_name\", \"location\"]\n",
    "dept_df = spark.createDataFrame(dept_data, dept_cols)\n",
    "\n",
    "print(\"--- Employee Data ---\")\n",
    "emp_df.show()\n",
    "print(\"--- Department Data ---\")\n",
    "dept_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "906c3792",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check current number of partitions\n",
    "print(f\"Current Emp Partitions: {emp_df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# VISUALIZE PARTITIONS: \n",
    "# We add a column 'partition_id' to see which partition each row resides in.\n",
    "emp_df.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52eb781a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Repartition (Full Shuffle)\n",
    "# Can Increase or Decrease partitions. \n",
    "# Distributes data equally (Round Robin).\n",
    "# Expensive operation (Network Shuffle).\n",
    "df_repartitioned = emp_df.repartition(4)\n",
    "print(f\"Repartition Count: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# 2. Coalesce (Minimize Shuffle)\n",
    "# Can ONLY Decrease partitions.\n",
    "# Merges local partitions. Efficient.\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "print(f\"Coalesce Count: {df_coalesced.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1637647e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Repartition by a specific column ensures all data for that key ends up in the same partition.\n",
    "# This is useful before Joins or GroupBy.\n",
    "\n",
    "df_by_dept = emp_df.repartition(4, \"dept_id\")\n",
    "\n",
    "print(\"--- Data Repartitioned by Dept ID ---\")\n",
    "# Notice how rows with same dept_id have same partition_id\n",
    "df_by_dept.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "436bca86",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Inner Join: Returns only matching records (Dept 101, 102, 103)\n",
    "# Syntax: df1.join(df2, condition, type)\n",
    "\n",
    "join_condition = emp_df[\"dept_id\"] == dept_df[\"dept_id\"]\n",
    "\n",
    "df_inner = emp_df.join(dept_df, join_condition, \"inner\")\n",
    "\n",
    "print(\"--- Inner Join ---\")\n",
    "df_inner.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "995ea7c9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# PROBLEM: In the previous join, 'dept_id' appears twice (once from emp, once from dept).\n",
    "# If we try to select \"dept_id\", Spark gets confused and throws AnalysisException.\n",
    "\n",
    "# df_inner.select(\"dept_id\").show()  # <--- This will FAIL\n",
    "\n",
    "# SOLUTION 1: Reference the specific DataFrame\n",
    "df_inner.select(emp_df[\"dept_id\"], \"name\", \"dept_name\").show()\n",
    "\n",
    "# SOLUTION 2 (Better): Rename column before joining or drop duplicate after joining\n",
    "df_clean_join = emp_df.join(dept_df, join_condition, \"inner\") \\\n",
    "    .drop(dept_df[\"dept_id\"]) # Drop the duplicate column from the right side\n",
    "\n",
    "print(\"--- Clean Join (No Duplicate Columns) ---\")\n",
    "df_clean_join.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "358b675e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Left Join: Returns all rows from Left (Emp) and matched rows from Right (Dept).\n",
    "# Unmatched rows get NULL (Dept 104 will have NULL dept_name).\n",
    "\n",
    "df_left = emp_df.join(dept_df, join_condition, \"left\")\n",
    "\n",
    "print(\"--- Left Join ---\")\n",
    "df_left.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206507be",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# You can have multiple conditions in a join using & (AND) / | (OR).\n",
    "\n",
    "# Example: Join where Dept ID matches AND Salary > 50000\n",
    "complex_condition = (emp_df[\"dept_id\"] == dept_df[\"dept_id\"]) & (emp_df[\"salary\"] > 50000)\n",
    "\n",
    "df_complex = emp_df.join(dept_df, complex_condition, \"inner\")\n",
    "\n",
    "print(\"--- Complex Join (Match + Salary > 50k) ---\")\n",
    "df_complex.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fda8c0b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **`repartition(n)`**: Increases/Decreases partitions. Performs full shuffle. Good for filtering/joins.\n",
    "2.  **`coalesce(n)`**: Only Decreases partitions. No full shuffle. Good for writing files.\n",
    "3.  **`spark_partition_id()`**: Useful debugging function to see data distribution.\n",
    "4.  **Joins**:\n",
    "    *   Always be careful of **Ambiguous Columns** (columns with same name in both tables).\n",
    "    *   Best practice: Explicitly reference `df['col']` or drop the duplicate column immediately after join.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **Reading and Writing Files** (CSV, Parquet) and understanding **Spark Schemas** in depth."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
