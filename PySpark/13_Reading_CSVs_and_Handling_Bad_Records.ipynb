{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fba58b61",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 13: Reading CSVs, Spark UI Internals, and Bad Record Handling\n",
    "\n",
    "Reading a CSV file seems simple, but Spark does a lot of work under the hood. In this module, we will:\n",
    "1.  Read CSV files and understand why Spark triggers background jobs.\n",
    "2.  Learn about **Schema Inference** and why it can be expensive.\n",
    "3.  Handle **Bad Records** (Corrupt Data) using three different modes:\n",
    "    *   `PERMISSIVE` (Default)\n",
    "    *   `DROPMALFORMED`\n",
    "    *   `FAILFAST`\n",
    "\n",
    "### Prerequisites\n",
    "Ensure you have the `emp.csv` and `emp_new.csv` (the one with bad records) in your `data/input` folder as described in the video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b54a066",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, DoubleType, DateType\n",
    "\n",
    "# Initialize SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CSV_Read_And_Bad_Records\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active. Go to localhost:4040 to view Spark UI.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa2a1ee",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading a CSV without specifying schema.\n",
    "# We use 'inferSchema' = true.\n",
    "# Note: This triggers a Spark Job! Spark needs to read the file once to guess the data types.\n",
    "\n",
    "file_path = \"data/input/emp.csv\" # Ensure this path is correct for your setup\n",
    "\n",
    "df_inferred = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "print(\"--- Schema Inferred ---\")\n",
    "df_inferred.printSchema()\n",
    "# Go to Spark UI (localhost:4040) -> Jobs tab. You will see a job created just for reading!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "384e4ede",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In production, we define schema explicitly to avoid the extra read job (performance) \n",
    "# and to handle bad data correctly.\n",
    "\n",
    "# Define Schema\n",
    "emp_schema = StructType([\n",
    "    StructField(\"employee_id\", IntegerType(), True),\n",
    "    StructField(\"department_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", DoubleType(), True),\n",
    "    StructField(\"hire_date\", DateType(), True)\n",
    "])\n",
    "\n",
    "# Read with Schema\n",
    "df_schema = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(emp_schema) \\\n",
    "    .load(file_path)\n",
    "\n",
    "print(\"--- Schema Explicitly Defined ---\")\n",
    "df_schema.printSchema()\n",
    "\n",
    "# Note: If you check Spark UI now, NO JOB was triggered for this cell. \n",
    "# Spark accepted the schema blindly (Lazy Evaluation)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f176f47b",
   "metadata": {},
   "source": [
    "## Handling Bad Records\n",
    "We will now read a file (`emp_new.csv`) that contains corrupt data:\n",
    "*   A string \"Low\" in the `Salary` column (which expects Double).\n",
    "*   A string \"No Date\" in the `hire_date` column (which expects Date).\n",
    "\n",
    "Spark provides 3 modes to handle this:\n",
    "1.  **PERMISSIVE (Default):** Sets corrupt fields to `null` and records the bad record in a separate column.\n",
    "2.  **DROPMALFORMED:** Completely ignores (drops) the row containing bad data.\n",
    "3.  **FAILFAST:** Throws an exception immediately and stops the job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7373c7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. PERMISSIVE\n",
    "# We add a special column option 'columnNameOfCorruptRecord' to capture the bad raw data.\n",
    "\n",
    "bad_data_path = \"data/input/emp_new.csv\"\n",
    "\n",
    "df_permissive = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(emp_schema) \\\n",
    "    .option(\"mode\", \"PERMISSIVE\") \\\n",
    "    .option(\"columnNameOfCorruptRecord\", \"_corrupt_record\") \\\n",
    "    .load(bad_data_path)\n",
    "\n",
    "print(\"--- Permissive Mode (Nulls inserted for bad data) ---\")\n",
    "df_permissive.show()\n",
    "\n",
    "# Notice: \n",
    "# 1. The 'salary' column will be null for the bad row.\n",
    "# 2. The '_corrupt_record' column will contain the raw text line of the bad record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "912f8d31",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. DROPMALFORMED\n",
    "# Spark will silently drop the rows that don't match the schema.\n",
    "\n",
    "df_drop = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .schema(emp_schema) \\\n",
    "    .option(\"mode\", \"DROPMALFORMED\") \\\n",
    "    .load(bad_data_path)\n",
    "\n",
    "print(\"--- Drop Malformed Mode (Bad rows removed) ---\")\n",
    "df_drop.show()\n",
    "\n",
    "# Notice: The count of rows will be less than the original file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5bd9707",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 3. FAILFAST\n",
    "# Useful for critical data pipelines where data quality is paramount.\n",
    "\n",
    "print(\"--- Fail Fast Mode (Expect Error) ---\")\n",
    "try:\n",
    "    df_fail = spark.read \\\n",
    "        .format(\"csv\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .schema(emp_schema) \\\n",
    "        .option(\"mode\", \"FAILFAST\") \\\n",
    "        .load(bad_data_path)\n",
    "    \n",
    "    df_fail.show() # Action triggers the read\n",
    "except Exception as e:\n",
    "    print(\"Error Encountered: Data quality check failed!\")\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9496f6e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Instead of chaining .option().option().option(), use a dictionary.\n",
    "\n",
    "read_options = {\n",
    "    \"header\": \"true\",\n",
    "    \"inferSchema\": \"false\",\n",
    "    \"mode\": \"PERMISSIVE\",\n",
    "    \"sep\": \",\"\n",
    "}\n",
    "\n",
    "df_dict = spark.read \\\n",
    "    .format(\"csv\") \\\n",
    "    .options(**read_options) \\\n",
    "    .schema(emp_schema) \\\n",
    "    .load(file_path)\n",
    "\n",
    "print(\"--- Read using Dictionary Options ---\")\n",
    "df_dict.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8c3fbb0",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Inference vs. Schema:** Always provide a schema in production to improve performance and ensure data quality.\n",
    "2.  **Bad Records:**\n",
    "    *   Use `PERMISSIVE` to load data but flag errors (using `columnNameOfCorruptRecord`).\n",
    "    *   Use `DROPMALFORMED` to ignore bad data.\n",
    "    *   Use `FAILFAST` to stop the pipeline on error.\n",
    "3.  **Options:** Use dictionaries to manage configurations cleanly.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore reading **JSON** and **Parquet** files, which are the standard for Big Data storage."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
