{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "303d2a04",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 14: Reading Parquet, ORC, and Recursive File Lookup\n",
    "\n",
    "CSV is human-readable, but it is terrible for Big Data performance. In this module, we will learn how to read optimized **Columnar File Formats** like Parquet and ORC.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Row vs. Columnar Storage:** Why use Parquet?\n",
    "2.  **Reading Parquet Files:** `spark.read.parquet()`.\n",
    "3.  **Reading ORC Files:** `spark.read.orc()`.\n",
    "4.  **Recursive File Lookup:** Reading nested folders automatically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47735fd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Complex_File_Formats\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e33e66d",
   "metadata": {},
   "source": [
    "## Row vs. Columnar Storage\n",
    "\n",
    "*   **Row-Oriented (CSV, Avro, RDBMS):** Stores data row by row. Good for writing, bad for reading specific columns.\n",
    "*   **Column-Oriented (Parquet, ORC):** Stores data column by column. \n",
    "    *   **Benefits:**\n",
    "        1.  **Compression:** Better compression ratios.\n",
    "        2.  **Column Pruning:** If you `select(\"salary\")`, Spark only reads the salary column file blocks, ignoring the rest. This makes it incredibly fast for analytics.\n",
    "        3.  **Schema Embedded:** You don't need to specify schema; it's stored inside the file footer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72bbcc51",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading a Parquet file.\n",
    "# Notice we don't need to specify schema or header=true options.\n",
    "# Parquet files contain the schema metadata internally.\n",
    "\n",
    "parquet_path = \"data/input/sales_data.parquet\"\n",
    "\n",
    "df_parquet = spark.read.parquet(parquet_path)\n",
    "# OR: spark.read.format(\"parquet\").load(parquet_path)\n",
    "\n",
    "print(\"--- Parquet Data ---\")\n",
    "df_parquet.printSchema()\n",
    "df_parquet.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed969e4b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading an ORC file.\n",
    "# Similar to Parquet, ORC is also a columnar format, highly optimized for Hive.\n",
    "\n",
    "orc_path = \"data/input/sales_data.orc\"\n",
    "\n",
    "df_orc = spark.read.orc(orc_path)\n",
    "\n",
    "print(\"--- ORC Data ---\")\n",
    "df_orc.printSchema()\n",
    "df_orc.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bc382a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark can read a folder containing multiple part-files automatically.\n",
    "# Path: \"data/input/sales_total.parquet\" (This is a folder, not a file)\n",
    "\n",
    "folder_path = \"data/input/sales_total.parquet\"\n",
    "\n",
    "df_total = spark.read.parquet(folder_path)\n",
    "\n",
    "print(f\"Total Records from Folder: {df_total.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "108486b4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scenario: You have data nested deep inside folders like:\n",
    "# /data/year=2023/month=01/day=01/file.parquet\n",
    "# /data/year=2023/month=01/day=02/file.parquet\n",
    "\n",
    "# By default, Spark might not look deep enough. We can force recursive lookup.\n",
    "\n",
    "recursive_path = \"data/input/sales_recursive\"\n",
    "\n",
    "df_recursive = spark.read \\\n",
    "    .format(\"parquet\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .load(recursive_path)\n",
    "\n",
    "print(f\"Recursive Read Count: {df_recursive.count()}\")\n",
    "df_recursive.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8412860c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's prove Column Pruning via the Spark Execution Plan.\n",
    "# We select ONLY 'transacted_at'. Spark should optimize the read.\n",
    "\n",
    "df_pruned = df_parquet.select(\"transacted_at\")\n",
    "\n",
    "print(\"--- Execution Plan showing Column Pruning ---\")\n",
    "df_pruned.explain()\n",
    "\n",
    "# Look for \"ReadSchema\" in the Physical Plan output.\n",
    "# It should only list: struct<transacted_at:timestamp>\n",
    "# This confirms Spark ignored all other columns during the read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2323be7",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Parquet/ORC** are superior for reading data due to **Column Pruning** and compression.\n",
    "2.  **Schema** is built-in; no need to define it manually.\n",
    "3.  **`recursiveFileLookup`** helps read deeply nested directory structures.\n",
    "4.  Spark optimizes I/O by only reading the columns you specifically select.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we tackle **JSON** files, handling nested structures, multiline JSONs, and schema enforcement."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
