{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "481efd02",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 15: Working with JSON and Complex Data\n",
    "\n",
    "In the previous module, we looked at columnar formats like Parquet. Today, we focus on **JSON**, one of the most common formats for web APIs and NoSQL databases. JSON data in Spark can be complex, often containing nested structures (Structs) and Arrays.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Reading JSON:** Single-line vs. Multi-line files.\n",
    "2.  **Schema Handling:** Inference vs. Enforcement.\n",
    "3.  **JSON Functions:** Parsing JSON strings (`from_json`) and writing JSON strings (`to_json`).\n",
    "4.  **Complex Data:** Accessing nested fields and flattening arrays (`explode`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a57ae31c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, to_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, LongType, ArrayType, DoubleType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"JSON_Processing\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fc3015",
   "metadata": {},
   "source": [
    "## 1. Reading JSON Files\n",
    "\n",
    "Spark reads standard JSON (one JSON object per line) by default. However, many JSON files are \"pretty-printed\" (spread across multiple lines).\n",
    "\n",
    "*   **Single-line JSON:** Supported out of the box.\n",
    "*   **Multi-line JSON:** Requires the option `multiline=True`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0e5bc6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Read Single-line JSON (Standard)\n",
    "single_line_path = \"data/input/order_singleline.json\"\n",
    "\n",
    "df_single = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .load(single_line_path)\n",
    "\n",
    "print(\"--- Single Line JSON Schema ---\")\n",
    "df_single.printSchema()\n",
    "\n",
    "# 2. Read Multi-line JSON\n",
    "# If we read this without the option, Spark treats each line as a corrupt record.\n",
    "multi_line_path = \"data/input/order_multiline.json\"\n",
    "\n",
    "df_multi = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .load(multi_line_path)\n",
    "\n",
    "print(\"--- Multi Line JSON Data ---\")\n",
    "df_multi.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f689903",
   "metadata": {},
   "source": [
    "## 2. Enforcing Custom Schema\n",
    "\n",
    "While Spark infers schema automatically, for production pipelines, it is best practice to enforce a specific schema. This prevents data type errors and improves performance (Spark doesn't have to scan the file once to guess the types)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebea828a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Defining a complex schema with Arrays and Structs manually\n",
    "# Structure:\n",
    "# - contact: Array of Strings (originally inferred as Long in the video, but let's cast to String)\n",
    "# - customer_id: String\n",
    "# - order_id: String\n",
    "# - order_line_items: Array of Structs (Nested Data)\n",
    "\n",
    "custom_schema = StructType([\n",
    "    StructField(\"contact\", ArrayType(StringType())),\n",
    "    StructField(\"customer_id\", StringType()),\n",
    "    StructField(\"order_id\", StringType()),\n",
    "    StructField(\"order_line_items\", ArrayType(StructType([\n",
    "        StructField(\"amount\", DoubleType()),\n",
    "        StructField(\"item_id\", StringType()),\n",
    "        StructField(\"qty\", LongType())\n",
    "    ])))\n",
    "])\n",
    "\n",
    "# Reading with schema enforcement\n",
    "df_schema = spark.read \\\n",
    "    .format(\"json\") \\\n",
    "    .schema(custom_schema) \\\n",
    "    .load(single_line_path)\n",
    "\n",
    "print(\"--- Data with Enforced Schema ---\")\n",
    "df_schema.printSchema()\n",
    "df_schema.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ad9811",
   "metadata": {},
   "source": [
    "## 3. The `from_json` Function\n",
    "\n",
    "Sometimes, data isn't in a JSON file, but stored as a **JSON String** inside a text column (common in Kafka logs or CSVs).\n",
    "\n",
    "To handle this:\n",
    "1.  Read the data as Text (or CSV).\n",
    "2.  Use `from_json()` with a schema to convert the String column into a Struct/Map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "142f8c89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Read the JSON file as a plain Text file (simulating a raw string column)\n",
    "df_raw = spark.read.text(single_line_path)\n",
    "\n",
    "print(\"--- Raw Text Data ---\")\n",
    "df_raw.show(truncate=False)\n",
    "\n",
    "# Step 2: Parse the 'value' column using the schema we defined earlier\n",
    "df_parsed = df_raw.withColumn(\"parsed_data\", from_json(col(\"value\"), custom_schema))\n",
    "\n",
    "print(\"--- Parsed Data (Struct Column) ---\")\n",
    "df_parsed.printSchema()\n",
    "\n",
    "# Step 3: Select fields from the struct using Dot Notation\n",
    "df_final_parsed = df_parsed.select(\"parsed_data.*\")\n",
    "df_final_parsed.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f37b9e",
   "metadata": {},
   "source": [
    "## 4. The `to_json` Function\n",
    "\n",
    "This is the reverse of `from_json`. It takes a Struct or Array column and converts it back into a JSON string. This is useful when you need to write data to a downstream system (like Kafka) that expects a single payload string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edb0a03d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Convert the structured columns back into a single JSON string\n",
    "df_json_string = df_final_parsed.select(\n",
    "    to_json(struct(col(\"*\"))).alias(\"json_payload\")\n",
    ")\n",
    "\n",
    "print(\"--- Converted Back to JSON String ---\")\n",
    "df_json_string.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f791b45",
   "metadata": {},
   "source": [
    "## 5. Exploding Arrays\n",
    "\n",
    "When you have an Array of items (e.g., `order_line_items`), you often want to flatten it so that each item in the array becomes its own row.\n",
    "\n",
    "*   **`explode()`**: Creates a new row for each element in the given array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c01f089d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Our data has an array: order_line_items. One order has multiple items.\n",
    "# We want 1 row per Item, not 1 row per Order.\n",
    "\n",
    "# 1. Explode the array\n",
    "df_exploded = df_final_parsed.withColumn(\"exploded_item\", explode(col(\"order_line_items\")))\n",
    "\n",
    "# 2. Flatten the struct inside the array using Dot Notation\n",
    "df_flattened = df_exploded.select(\n",
    "    col(\"order_id\"),\n",
    "    col(\"customer_id\"),\n",
    "    col(\"exploded_item.item_id\"),\n",
    "    col(\"exploded_item.qty\"),\n",
    "    col(\"exploded_item.amount\")\n",
    ")\n",
    "\n",
    "print(\"--- Flattened Transactional Data ---\")\n",
    "df_flattened.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "659650a2",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Read Options:** Use `.option(\"multiline\", \"true\")` for non-standard JSON files.\n",
    "2.  **Schema:** Always define schemas for complex JSON to avoid inference costs and errors.\n",
    "3.  **Parsing:** Use `from_json` to turn string columns into usable Structs.\n",
    "4.  **Formatting:** Use `to_json` to turn Structs back into strings.\n",
    "5.  **Transformation:** Use `explode` to convert Arrays into rows and `.` notation to access nested fields.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will likely dive into Spark SQL or performing aggregations on this flattened data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
