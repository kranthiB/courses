{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "98534312",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 16: Writing Data in Spark\n",
    "\n",
    "Reading data is only half the battle. In this module, we will learn how to write data back to storage effectively. We will dive deep into how Spark's distributed architecture influences file generation and how to control the output structure.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Spark Writer API:** Basic syntax for writing data.\n",
    "2.  **Under the Hood:** How Partitions relate to Output Files.\n",
    "3.  **Partitioning Data:** Using `partitionBy` to create directory structures.\n",
    "4.  **Write Modes:** `append`, `overwrite`, `ignore`, and `error`.\n",
    "5.  **Bonus Tip:** How to write a single output file (handling the `part-00000` naming convention)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe308db",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Writing_Data\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c1a3e88",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We will use the Employee dataset for this exercise.\n",
    "data = [\n",
    "    (\"001\", \"John Doe\", 30, \"Male\", 50000, \"2015-01-01\", \"101\"),\n",
    "    (\"002\", \"Jane Smith\", 25, \"Female\", 45000, \"2016-02-15\", \"101\"),\n",
    "    (\"003\", \"Bob Brown\", 35, \"Male\", 55000, \"2014-05-01\", \"102\"),\n",
    "    (\"004\", \"Alice Lee\", 28, \"Female\", 48000, \"2017-09-30\", \"102\"),\n",
    "    (\"005\", \"Jack Chan\", 40, \"Male\", 60000, \"2013-08-21\", \"103\"),\n",
    "    (\"006\", \"Jill Wong\", 32, \"Female\", 52000, \"2018-12-01\", \"103\")\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"employee_id\", StringType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"age\", IntegerType(), True),\n",
    "    StructField(\"gender\", StringType(), True),\n",
    "    StructField(\"salary\", IntegerType(), True),\n",
    "    StructField(\"hire_date\", StringType(), True),\n",
    "    StructField(\"department_id\", StringType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c6dbeca",
   "metadata": {},
   "source": [
    "## 1. How Spark Writes Files\n",
    "\n",
    "In Spark, **1 Task processes 1 Partition and writes 1 File**.\n",
    "\n",
    "If your DataFrame has 8 partitions, Spark will launch 8 parallel tasks, and you will end up with 8 output files (e.g., `part-00000`, `part-00001`, etc.) inside the output folder.\n",
    "\n",
    "Let's check the default parallelism and the number of partitions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41bac20f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Check the default parallelism of the cluster (local machine)\n",
    "print(f\"Default Parallelism (Cores): {spark.sparkContext.defaultParallelism}\")\n",
    "\n",
    "# Check how many partitions our DataFrame has\n",
    "print(f\"DataFrame Partitions: {df.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Let's verify which data resides in which partition ID\n",
    "df.withColumn(\"partition_id\", spark_partition_id()).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2666f4a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Basic Write Syntax: df.write.format(...).save(...)\n",
    "# Note: Spark creates a DIRECTORY with the given name, not a file.\n",
    "\n",
    "output_path_basic = \"data/output/module_16/basic_write\"\n",
    "\n",
    "# Writing in Parquet format\n",
    "df.write \\\n",
    "    .format(\"parquet\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path_basic)\n",
    "\n",
    "print(f\"Data written to: {output_path_basic}\")\n",
    "# If you check this folder in your OS file explorer, you will see multiple part-files \n",
    "# matching the number of partitions (e.g., 8 files if you have 8 cores)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76fc1b22",
   "metadata": {},
   "source": [
    "## 2. Partitioning Data on Write\n",
    "\n",
    "Partitioning organizes data into sub-folders based on column values (e.g., `department_id=101/`).\n",
    "This improves read performance later because Spark can skip folders that aren't needed (Partition Pruning).\n",
    "\n",
    "**Syntax:** `.partitionBy(\"column_name\")`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84051852",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_path_partitioned = \"data/output/module_16/partitioned_write\"\n",
    "\n",
    "df.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"department_id\") \\\n",
    "    .save(output_path_partitioned)\n",
    "\n",
    "print(f\"Partitioned data written to: {output_path_partitioned}\")\n",
    "\n",
    "# Structure created on disk:\n",
    "# /partitioned_write\n",
    "#    |-- department_id=101/\n",
    "#           |-- part-00000.csv\n",
    "#    |-- department_id=102/\n",
    "#           |-- part-00000.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048dc658",
   "metadata": {},
   "source": [
    "## 3. Write Modes\n",
    "\n",
    "Spark provides different modes to handle existing data at the destination path:\n",
    "\n",
    "1.  **`error` (default):** Throws an error if the directory already exists.\n",
    "2.  **`append`:** Adds new files to the existing directory. (Be careful, this can result in duplicate data if run multiple times).\n",
    "3.  **`overwrite`:** Deletes the entire directory and writes fresh data.\n",
    "4.  **`ignore`:** If directory exists, do nothing (silently skip writing)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e8659d3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_path_modes = \"data/output/module_16/modes_test\"\n",
    "\n",
    "# First write (Initial creation)\n",
    "print(\"1. Initial Write...\")\n",
    "df.write.format(\"csv\").mode(\"overwrite\").save(output_path_modes)\n",
    "\n",
    "# Second write (Append) - File count will double\n",
    "print(\"2. Appending data...\")\n",
    "df.write.format(\"csv\").mode(\"append\").save(output_path_modes)\n",
    "\n",
    "# Third write (Error) - This should fail\n",
    "print(\"3. Testing Error mode (expecting failure)...\")\n",
    "try:\n",
    "    df.write.format(\"csv\").mode(\"error\").save(output_path_modes)\n",
    "except Exception as e:\n",
    "    print(f\"Error Caught: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e4bb3aa",
   "metadata": {},
   "source": [
    "## 4. Bonus: Writing a Single Output File\n",
    "\n",
    "Often, downstream systems expect a single CSV file, not a folder with `part-00000`, `part-00001`, etc.\n",
    "\n",
    "**Solution:**\n",
    "Use `repartition(1)` or `coalesce(1)` before writing. This forces all data into a single partition, processed by a single task, resulting in one output file.\n",
    "\n",
    "*Note: This is expensive (shuffle) for large datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7ed231",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_path_single = \"data/output/module_16/single_file\"\n",
    "\n",
    "# Force data into 1 partition\n",
    "df_single = df.repartition(1)\n",
    "\n",
    "df_single.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(output_path_single)\n",
    "\n",
    "print(f\"Single file written to: {output_path_single}\")\n",
    "# Check the folder: it will contain exactly one 'part-00000....csv' file containing all records."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7128b47d",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Output Files:** The number of output files equals the number of DataFrame partitions (Parallel Tasks).\n",
    "2.  **Organization:** Use `.partitionBy()` to create folder structures for optimized querying.\n",
    "3.  **Modes:** Use `overwrite` to replace data, `append` to add data.\n",
    "4.  **Single File:** Use `.repartition(1)` to merge all partitions before writing if a single file is required.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore **Spark Clusters**, exploring the internal configuration and how tasks are distributed across nodes."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
