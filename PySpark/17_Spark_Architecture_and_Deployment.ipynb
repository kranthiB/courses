{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35c4854d",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 17: Spark Architecture and Cluster Deployment\n",
    "\n",
    "Up until now, we have been running Spark in \"Local Mode\" (inside our Jupyter environment). In a real production environment, Spark runs on a **Cluster** of multiple machines.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Spark Architecture:** Driver, Executors, and Cluster Manager.\n",
    "2.  **Deployment Modes:** Client Mode vs. Cluster Mode.\n",
    "3.  **Spark Submit:** How to submit jobs to a cluster.\n",
    "4.  **Resource Allocation:** Configuring Cores and Memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe2d17e9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Even though we are simulating this locally, understanding the config is key.\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# We explicitly set master to local[*] here, but in a real cluster, \n",
    "# you would point this to the Cluster Manager URL (e.g., spark://master-node:7077)\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Cluster_Architecture_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark UI URL: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9bdc977",
   "metadata": {},
   "source": [
    "## 1. Spark Architecture Components\n",
    "\n",
    "*   **Driver Program:** The \"brain\" of the operation. It runs your `main()` function, creates the `SparkContext`, and converts your code into tasks.\n",
    "*   **Cluster Manager:** Allocates resources (CPU/RAM). Examples: Standalone, YARN, Kubernetes.\n",
    "*   **Executors:** The \"workers\". They run the actual tasks on the worker nodes and store data in memory/disk.\n",
    "\n",
    "**Flow:**\n",
    "1.  Driver asks Cluster Manager for resources.\n",
    "2.  Cluster Manager launches Executors on Worker Nodes.\n",
    "3.  Driver sends tasks (code) to Executors.\n",
    "4.  Executors run tasks and return results to Driver."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f02d7e8c",
   "metadata": {},
   "source": [
    "## 2. Client Mode vs. Cluster Mode\n",
    "\n",
    "This setting determines **where the Driver Program runs**.\n",
    "\n",
    "1.  **Client Mode (Default for Notebooks/Shell):**\n",
    "    *   **Driver:** Runs on the machine where you submitted the job (e.g., your laptop or edge node).\n",
    "    *   **Pros:** Interactive, you see logs immediately.\n",
    "    *   **Cons:** If you close your laptop, the job dies. High network latency if the client is far from the cluster.\n",
    "\n",
    "2.  **Cluster Mode (Production):**\n",
    "    *   **Driver:** Runs on one of the Worker Nodes inside the cluster.\n",
    "    *   **Pros:** Fire and forget. You can submit the job and disconnect. The cluster manages the driver.\n",
    "    *   **Cons:** Harder to debug (logs are stored on the cluster)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4416ade8",
   "metadata": {},
   "source": [
    "## 3. The `spark-submit` Command\n",
    "\n",
    "In production, we don't usually run jobs from Jupyter notebooks. We package our Python code into a `.py` file and submit it using the command line tool `spark-submit`.\n",
    "\n",
    "**Basic Syntax:**\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --master <master-url> \\\n",
    "  --deploy-mode <client|cluster> \\\n",
    "  --conf <key>=<value> \\\n",
    "  your_script.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1b58640",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Creating a Script for Submission\n",
    "\n",
    "script_content = \"\"\"\n",
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Create Spark Session\n",
    "spark = SparkSession.builder.appName(\"SparkSubmitTest\").getOrCreate()\n",
    "\n",
    "# Create a small DataFrame\n",
    "data = [(\"Alice\", 1), (\"Bob\", 2), (\"Charlie\", 3)]\n",
    "df = spark.createDataFrame(data, [\"Name\", \"Value\"])\n",
    "\n",
    "# Show Data (This goes to stdout logs)\n",
    "df.show()\n",
    "\n",
    "# Sleep to keep the UI alive for a bit so we can inspect it\n",
    "time.sleep(10)\n",
    "\n",
    "spark.stop()\n",
    "\"\"\"\n",
    "\n",
    "# Write the script to a file\n",
    "with open(\"my_spark_job.py\", \"w\") as f:\n",
    "    f.write(script_content)\n",
    "\n",
    "print(\"Created 'my_spark_job.py' successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b5b4407",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# We use the '!' magic command to run shell commands in Jupyter.\n",
    "# In a real terminal, you would just type the command without the '!'.\n",
    "\n",
    "print(\"Submitting the job...\")\n",
    "\n",
    "# This submits the job to our local \"cluster\" (local[*])\n",
    "!spark-submit \\\n",
    "  --master local[*] \\\n",
    "  --name \"My Submitted Job\" \\\n",
    "  my_spark_job.py\n",
    "\n",
    "print(\"Job Finished.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16ad44bf",
   "metadata": {},
   "source": [
    "## 4. Resource Allocation\n",
    "\n",
    "When submitting jobs, you can control how much power your job gets.\n",
    "\n",
    "**Key Configurations:**\n",
    "*   `--num-executors`: Total number of worker processes (YARN/K8s only).\n",
    "*   `--executor-cores`: Number of CPU cores per executor.\n",
    "*   `--executor-memory`: RAM per executor (e.g., `4g`).\n",
    "*   `--driver-memory`: RAM for the driver program.\n",
    "\n",
    "**Example Command:**\n",
    "```bash\n",
    "spark-submit \\\n",
    "  --master yarn \\\n",
    "  --deploy-mode cluster \\\n",
    "  --num-executors 4 \\\n",
    "  --executor-cores 2 \\\n",
    "  --executor-memory 4g \\\n",
    "  my_spark_job.py"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "883c3c23",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Architecture:** Driver controls the execution; Executors do the work.\n",
    "2.  **Deployment:** Use **Client Mode** for development/interactive work. Use **Cluster Mode** for production jobs.\n",
    "3.  **`spark-submit`:** The standard tool for launching Spark applications.\n",
    "4.  **Configuration:** Tuning memory and cores is essential for performance and stability.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore **Spark Configurations** in deeper detail, including environment variables and performance tuning properties."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
