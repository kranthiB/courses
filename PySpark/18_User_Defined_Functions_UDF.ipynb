{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c22ab617",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 18: User Defined Functions (UDFs)\n",
    "\n",
    "Spark provides a vast library of built-in functions, but sometimes you need to apply custom logic that isn't available out of the box. This is where **User Defined Functions (UDFs)** come in.\n",
    "\n",
    "However, UDFs in PySpark come with a performance cost. In this notebook, we will learn how to create them and understand the architecture behind why they can be slow.\n",
    "\n",
    "### Agenda:\n",
    "1.  **What is a UDF?** Extending Spark's capabilities.\n",
    "2.  **Creating a Python UDF:** The `udf()` function.\n",
    "3.  **Registering UDFs for SQL:** Using `spark.udf.register`.\n",
    "4.  **Performance Implications:** Serialization/Deserialization overhead.\n",
    "5.  **Best Practices:** Why Native Functions are better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f3c6b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf, expr\n",
    "from pyspark.sql.types import DoubleType, StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"UDF_Deep_Dive\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1f60c7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the Employee dataset\n",
    "file_path = \"data/input/employee.csv\"\n",
    "\n",
    "# Load data\n",
    "df = spark.read.format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .load(file_path)\n",
    "\n",
    "df.show(5)\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4d87788",
   "metadata": {},
   "source": [
    "## 1. Creating a UDF for DataFrame API\n",
    "\n",
    "To create a UDF, we follow two steps:\n",
    "1.  Define a standard Python function.\n",
    "2.  Convert it to a Spark UDF using `udf(function, return_type)`.\n",
    "\n",
    "**Scenario:** Let's calculate a **10% Bonus** based on the Salary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8360b69",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Define a standard Python function\n",
    "def calculate_bonus(salary):\n",
    "    if salary is None:\n",
    "        return 0.0\n",
    "    return salary * 0.1\n",
    "\n",
    "# Step 2: Register it as a UDF for DataFrame API\n",
    "# We must specify the return type (DoubleType), otherwise, it defaults to StringType.\n",
    "bonus_udf = udf(calculate_bonus, DoubleType())\n",
    "\n",
    "# Step 3: Apply the UDF\n",
    "df_with_bonus = df.withColumn(\"bonus\", bonus_udf(col(\"salary\")))\n",
    "\n",
    "print(\"--- DataFrame with UDF Calculated Bonus ---\")\n",
    "df_with_bonus.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32fd8e9a",
   "metadata": {},
   "source": [
    "## 2. Registering UDFs for Spark SQL\n",
    "\n",
    "If you want to use your Python function inside a SQL query (e.g., `spark.sql(\"SELECT ...\")`) or inside `expr()`, you must register it with the SparkSession.\n",
    "\n",
    "**Syntax:** `spark.udf.register(\"sql_function_name\", python_function, return_type)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c22150cd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Register the function for use in SQL/Expressions\n",
    "spark.udf.register(\"calculate_bonus_sql\", calculate_bonus, DoubleType())\n",
    "\n",
    "# Usage Method A: Using expr() inside withColumn\n",
    "df_sql_udf = df.withColumn(\"bonus_sql\", expr(\"calculate_bonus_sql(salary)\"))\n",
    "\n",
    "print(\"--- Bonus calculated using SQL Registered UDF ---\")\n",
    "df_sql_udf.show(5)\n",
    "\n",
    "# Usage Method B: Using spark.sql()\n",
    "df.createOrReplaceTempView(\"employees\")\n",
    "spark.sql(\"SELECT name, salary, calculate_bonus_sql(salary) as bonus FROM employees\").show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffc7fb83",
   "metadata": {},
   "source": [
    "## 3. Why are Python UDFs slower?\n",
    "\n",
    "When you run a standard Spark command (like `.filter()` or `.select()`), the code runs directly inside the JVM (Java Virtual Machine) on the executors.\n",
    "\n",
    "When you use a **Python UDF**:\n",
    "1.  **Serialization:** Spark (JVM) converts the data into a format Python can understand (Pickle).\n",
    "2.  **Process Spin-up:** It sends this data to a separate **Python Worker Process**.\n",
    "3.  **Execution:** Python processes the data row-by-row.\n",
    "4.  **Deserialization:** The result is sent back to the JVM and converted back to Spark format.\n",
    "\n",
    "This **Context Switching** and **Serialization/Deserialization** creates significant overhead."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0c33bd8",
   "metadata": {},
   "source": [
    "## 4. Best Practice: Use Native Expressions\n",
    "\n",
    "Whenever possible, avoid UDFs. Use the built-in Spark SQL functions (`pyspark.sql.functions`). They run directly in the JVM and are highly optimized (Catalyst Optimizer).\n",
    "\n",
    "Let's achieve the same result without a UDF."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8891ae0d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Native Spark approach\n",
    "# This logic is translated directly to optimized JVM bytecode. No Python overhead.\n",
    "\n",
    "df_native = df.withColumn(\"bonus_native\", col(\"salary\") * 0.1)\n",
    "\n",
    "print(\"--- Bonus calculated using Native Expressions (FASTEST) ---\")\n",
    "df_native.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1335a5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Flexibility:** UDFs allow you to implement complex logic not available in standard Spark functions.\n",
    "2.  **Implementation:**\n",
    "    *   `udf()` for DataFrame API.\n",
    "    *   `spark.udf.register()` for Spark SQL.\n",
    "3.  **Performance:** Python UDFs are slower due to serialization overhead between JVM and Python processes.\n",
    "4.  **Optimization:** Always prefer **Native Spark Functions** (`col() * 0.1`) over UDFs whenever possible.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore **Vectorized UDFs (Pandas UDFs)**, which solve the performance issues of standard Python UDFs by processing data in batches using Apache Arrow."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
