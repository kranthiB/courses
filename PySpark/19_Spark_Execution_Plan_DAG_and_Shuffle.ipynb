{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1710b9e7",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 19: Understanding Spark Execution Plan, DAG, and Shuffle\n",
    "\n",
    "To write optimized Spark applications, you must understand what happens under the hood. This module dives into the **DAG (Directed Acyclic Graph)**, how Spark breaks down jobs into **Stages** and **Tasks**, and the performance-critical concept of **Shuffle**.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Lazy Evaluation & Actions:** When does Spark actually run?\n",
    "2.  **The DAG:** Visualizing the execution graph.\n",
    "3.  **Jobs, Stages, and Tasks:** The hierarchy of execution.\n",
    "4.  **Shuffle:** What is it, and why is it expensive?\n",
    "5.  **Spark UI:** Analyzing the execution plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88765f8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "\n",
    "# We disable Adaptive Query Execution (AQE) temporarily to see the raw DAG\n",
    "# and full shuffle impact for learning purposes.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_Internals_DAG\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark UI Link: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "182f00ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We create two DataFrames to simulate a Join operation (which triggers a Shuffle)\n",
    "\n",
    "# DataFrame 1: Even numbers\n",
    "df1 = spark.range(0, 200, 2).withColumnRenamed(\"id\", \"id1\")\n",
    "# Repartition to simulate distributed data\n",
    "df1 = df1.repartition(5)\n",
    "\n",
    "# DataFrame 2: Multiples of 4\n",
    "df2 = spark.range(0, 200, 4).withColumnRenamed(\"id\", \"id2\")\n",
    "df2 = df2.repartition(7)\n",
    "\n",
    "print(\"DataFrames Created (Lazy Evaluation - Nothing executed yet)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd6e668",
   "metadata": {},
   "source": [
    "## 1. Triggering Execution\n",
    "\n",
    "Spark is lazy. The transformations above (`range`, `repartition`) built a logical plan but didn't execute it.\n",
    "Execution only starts when we call an **Action** (like `.count()`, `.show()`, `.collect()`).\n",
    "\n",
    "Let's perform a **Join**, which forces Spark to move data around (Shuffle)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e4bba55",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Joining the two DataFrames\n",
    "# This requires data with the same Key to be on the same Partition.\n",
    "# Spark must perform a SHUFFLE to achieve this.\n",
    "\n",
    "df_joined = df1.join(df2, df1[\"id1\"] == df2[\"id2\"], \"inner\")\n",
    "\n",
    "# Action to trigger the job\n",
    "df_joined.show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a77712b",
   "metadata": {},
   "source": [
    "## 2. The Explain Plan\n",
    "\n",
    "We can see how Spark intends to execute the query using `.explain()`.\n",
    "\n",
    "Look for:\n",
    "*   **Scan:** Reading data.\n",
    "*   **Exchange:** This indicates a **Shuffle** (Network transfer of data).\n",
    "*   **HashAggregate / SortMergeJoin:** The actual computation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a729ec52",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"--- Logical and Physical Plan ---\")\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "decf361c",
   "metadata": {},
   "source": [
    "## 3. Jobs, Stages, and Tasks\n",
    "\n",
    "*   **Job:** Triggered by an Action (e.g., `show()`).\n",
    "*   **Stage:** Spark breaks a Job into Stages at **Shuffle boundaries**.\n",
    "    *   If you have a `repartition` or `join`, Spark must end the current stage, write data to disk (shuffle write), and start a new stage (shuffle read).\n",
    "*   **Task:** The unit of work. One task per partition.\n",
    "\n",
    "**In our example:**\n",
    "1.  **Stage 0 & 1:** Read `df1` and `df2` in parallel (Narrow Transformation).\n",
    "2.  **Exchange (Shuffle):** Redistribute data based on Join Keys.\n",
    "3.  **Stage 2:** Perform the Join on the shuffled data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9275bb87",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# By default, Spark uses 200 shuffle partitions for joins/aggregations.\n",
    "# This is often too high for small data (causes overhead) or too low for huge data (OOM).\n",
    "\n",
    "current_partitions = spark.conf.get(\"spark.sql.shuffle.partitions\")\n",
    "print(f\"Default Shuffle Partitions: {current_partitions}\")\n",
    "\n",
    "# Let's change it to something smaller for our small dataset\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 4)\n",
    "\n",
    "# Re-run the join to see the difference in task count in Spark UI\n",
    "df_joined.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9656ced",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **DAG:** Spark builds a graph of transformations and only runs it when an Action is called.\n",
    "2.  **Shuffle:** The process of moving data between nodes (Exchanges). It is expensive (Disk I/O + Network).\n",
    "3.  **Stages:** Created whenever a Shuffle occurs. Minimizing shuffles improves performance.\n",
    "4.  **Tuning:** The configuration `spark.sql.shuffle.partitions` is critical for tuning join performance.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **Spark Memory Management** and how to deal with Out Of Memory (OOM) errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
