{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "914689c0",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 20: Optimizing Shuffle Operations\n",
    "\n",
    "Shuffle is the most expensive operation in Spark because it involves:\n",
    "1.  **Disk I/O:** Writing intermediate data to disk.\n",
    "2.  **Network I/O:** Transferring data between executor nodes.\n",
    "3.  **Serialization/Deserialization:** Converting data for transfer.\n",
    "\n",
    "In this module, we will learn how to optimize shuffle by tuning the partition count and understanding how to minimize data movement.\n",
    "\n",
    "### Agenda:\n",
    "1.  **The Shuffle Problem:** Why 200 partitions is the default and often wrong.\n",
    "2.  **Tuning Partitions:** How to calculate the optimal shuffle partition number.\n",
    "3.  **Coalesce vs. Repartition:** Efficiently changing partition counts.\n",
    "4.  **Best Practices:** Filtering early to reduce shuffle size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57ca7a48",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum, count\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Shuffle_Optimization\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8c48c7f",
   "metadata": {},
   "source": [
    "## 1. Default Shuffle Partitions\n",
    "\n",
    "By default, whenever a shuffle happens (Join/GroupBy), Spark creates **200 partitions**.\n",
    "\n",
    "*   **Too Many Partitions (Overkill):** For small data, you get thousands of tiny files and task scheduling overhead.\n",
    "*   **Too Few Partitions (Bottleneck):** For huge data, each partition becomes too large, causing Out Of Memory (OOM) errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafcdfab",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a small DataFrame\n",
    "df = spark.range(1, 1000)\n",
    "\n",
    "# Perform a wide transformation (GroupBy) -> Triggers Shuffle\n",
    "df_grouped = df.groupBy(\"id\").count()\n",
    "\n",
    "# Check the number of partitions AFTER the shuffle\n",
    "print(f\"Default Shuffle Partitions: {df_grouped.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Check the config value\n",
    "print(f\"Config Value: {spark.conf.get('spark.sql.shuffle.partitions')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4f4e13b",
   "metadata": {},
   "source": [
    "## 2. Tuning the Partition Count\n",
    "\n",
    "For smaller datasets (like our local examples), 200 is too high. We should lower it to match our core count or data size (e.g., 8 or 16).\n",
    "\n",
    "For larger datasets (TB scale), you might need to increase this to 1000 or more."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4ed423",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Set shuffle partitions to 8 (matching local cores)\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "\n",
    "# Perform the same operation again\n",
    "df_optimized = df.groupBy(\"id\").count()\n",
    "\n",
    "print(f\"Optimized Shuffle Partitions: {df_optimized.rdd.getNumPartitions()}\")\n",
    "df_optimized.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2210904f",
   "metadata": {},
   "source": [
    "## 3. Coalesce vs. Repartition\n",
    "\n",
    "Sometimes you want to change the partition count manually without a GroupBy/Join.\n",
    "\n",
    "*   **`repartition(n)`:** Full Shuffle. Redistributes data evenly. **Expensive**. Use when increasing partitions.\n",
    "*   **`coalesce(n)`:** No Shuffle (mostly). Merges existing partitions. **Efficient**. Use when decreasing partitions (e.g., before writing to disk)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3399e76",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Repartition: Increases partitions (Triggers Shuffle)\n",
    "df_repartitioned = df.repartition(10)\n",
    "print(f\"Repartitioned Count: {df_repartitioned.rdd.getNumPartitions()}\")\n",
    "\n",
    "# Coalesce: Decreases partitions (No Full Shuffle)\n",
    "df_coalesced = df_repartitioned.coalesce(2)\n",
    "print(f\"Coalesced Count: {df_coalesced.rdd.getNumPartitions()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b36a411d",
   "metadata": {},
   "source": [
    "## 4. Filter Early (Push Down Predicate)\n",
    "\n",
    "The best way to optimize shuffle is to **shuffle less data**.\n",
    "Always filter your DataFrames **BEFORE** joining or grouping. Spark's optimizer does this automatically in many cases, but explicit filtering is safer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723718e6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bad Practice: Join huge tables, then filter\n",
    "# Good Practice: Filter first, then join\n",
    "\n",
    "df1 = spark.range(1000).withColumnRenamed(\"id\", \"id1\")\n",
    "df2 = spark.range(1000).withColumnRenamed(\"id\", \"id2\")\n",
    "\n",
    "# Filter BEFORE Join\n",
    "df1_filtered = df1.filter(\"id1 > 500\")\n",
    "\n",
    "# Join smaller dataset\n",
    "df_joined = df1_filtered.join(df2, df1_filtered.id1 == df2.id2)\n",
    "\n",
    "df_joined.explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6bd4eca",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Default Partitions:** 200 is the default. Change it using `spark.sql.shuffle.partitions`.\n",
    "2.  **Optimization Rule:**\n",
    "    *   Small Data: Reduce partitions (e.g., 10-50).\n",
    "    *   Large Data: Increase partitions (e.g., 500-2000) to avoid OOM.\n",
    "3.  **Coalesce:** Use `coalesce` to reduce partition count efficiently before writing files.\n",
    "4.  **Filter Early:** Reduce the amount of data entering the shuffle stage.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore **Caching and Persistence**, techniques to speed up iterative algorithms by storing data in memory."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
