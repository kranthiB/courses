{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "750c52f5",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 21: Caching and Persistence\n",
    "\n",
    "When you reuse a DataFrame multiple times in your code (e.g., for multiple transformations or actions), Spark re-computes the entire lineage by default. This can be extremely slow.\n",
    "\n",
    "**Caching** allows you to save the intermediate DataFrame in memory (or disk) so that subsequent actions can read from the cache instead of re-computing from the source.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Why Cache?** The problem of re-computation.\n",
    "2.  **`cache()` vs. `persist()`:** Understanding the difference.\n",
    "3.  **Storage Levels:** Memory, Disk, and Serialization options.\n",
    "4.  **Unpersisting:** Clearing the cache to free up resources."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342042f6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark import StorageLevel\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Caching_and_Persistence\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6ce1a2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We create a larger dataset to make the time difference noticeable.\n",
    "# This creates a DataFrame with 10 Million rows.\n",
    "\n",
    "df = spark.range(1, 10000000).toDF(\"id\")\n",
    "df = df.withColumn(\"square\", col(\"id\") * col(\"id\"))\n",
    "\n",
    "# Force an action to materialize (but not cache yet)\n",
    "print(f\"Count: {df.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edeefe28",
   "metadata": {},
   "source": [
    "## 1. The Problem: Re-computation\n",
    "\n",
    "If we run two actions on the same DataFrame `df`, Spark computes it **twice**.\n",
    "Let's measure the time taken for two subsequent counts without caching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36d66ddd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "start_time = time.time()\n",
    "print(f\"Count 1: {df.count()}\")\n",
    "print(f\"Time taken for Count 1: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "start_time = time.time()\n",
    "print(f\"Count 2: {df.count()}\")\n",
    "print(f\"Time taken for Count 2: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Both should take roughly the same amount of time because the work is repeated."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f328f44b",
   "metadata": {},
   "source": [
    "## 2. Using `cache()`\n",
    "\n",
    "`cache()` is a shorthand for `persist(StorageLevel.MEMORY_AND_DISK)`.\n",
    "It stores the data in memory. If memory is full, it spills to disk.\n",
    "\n",
    "**Note:** Caching is **lazy**. The data is not actually cached until the *first* action is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aadbbc8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Mark the DataFrame for caching\n",
    "df.cache()\n",
    "\n",
    "print(\"DataFrame marked for caching (Lazy).\")\n",
    "\n",
    "# First Action: This will be slow because it has to compute AND cache the data.\n",
    "start_time = time.time()\n",
    "print(f\"Count 1 (Caching...): {df.count()}\")\n",
    "print(f\"Time taken (Build Cache): {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Second Action: This should be INSTANT because it reads from memory.\n",
    "start_time = time.time()\n",
    "print(f\"Count 2 (Read Cache): {df.count()}\")\n",
    "print(f\"Time taken (From Cache): {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27b8482e",
   "metadata": {},
   "source": [
    "## 3. Storage Levels with `persist()`\n",
    "\n",
    "If you want more control (e.g., Memory Only, Disk Only), use `persist()`.\n",
    "\n",
    "Common Storage Levels:\n",
    "*   **MEMORY_ONLY:** Fast, but fails if data > RAM.\n",
    "*   **MEMORY_AND_DISK (Default for cache):** Spills to disk if RAM is full.\n",
    "*   **DISK_ONLY:** Good for huge datasets that don't fit in RAM but are expensive to recompute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4253bcb5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unpersist first to clear the previous cache\n",
    "df.unpersist()\n",
    "\n",
    "# Persist with specific level (e.g., DISK_ONLY)\n",
    "df.persist(StorageLevel.DISK_ONLY)\n",
    "\n",
    "# First action caches to Disk\n",
    "df.count()\n",
    "\n",
    "# Second action reads from Disk (Slower than memory, but faster than re-compute)\n",
    "start_time = time.time()\n",
    "print(f\"Count (From Disk Cache): {df.count()}\")\n",
    "print(f\"Time taken: {time.time() - start_time:.2f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7bd6e340",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Always unpersist when done to free up cluster memory for other jobs.\n",
    "df.unpersist()\n",
    "print(\"Cache cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c27b7d5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Re-computation:** Spark re-runs the entire DAG for every action unless cached.\n",
    "2.  **`cache()`:** Stores data in Memory (and Disk if needed). Best for iterative algorithms.\n",
    "3.  **`persist()`:** Allows custom storage levels.\n",
    "4.  **Lazy Caching:** Data is only cached during the first Action, not when `.cache()` is called.\n",
    "5.  **Unpersist:** Always clean up your cache.\n",
    "\n",
    "**Next Steps:**\n",
    "In the final notebook of this series, we will explore **Spark SQL**, running SQL queries directly on DataFrames, and integrating with the Hive Metastore."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
