{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aff56a6a",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 22: Distributed Shared Variables\n",
    "\n",
    "Spark runs in a distributed manner, which means variables defined in your driver program are copied to each executor. However, updates made by executors are not sent back to the driver by default.\n",
    "\n",
    "To handle shared data efficiently, Spark provides two types of shared variables:\n",
    "1.  **Broadcast Variables:** Read-only variables cached on each machine (efficient for lookups).\n",
    "2.  **Accumulators:** Write-only variables used for counters and sums (efficient for metrics).\n",
    "\n",
    "### Agenda:\n",
    "1.  **The Problem:** Why standard Python variables don't work across clusters.\n",
    "2.  **Broadcast Variables:** Broadcasting a lookup table for joins.\n",
    "3.  **Accumulators:** Counting errors or specific events across the cluster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86133f22",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Shared_Variables\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84425bb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transaction Data (Employee ID, Sales Amount)\n",
    "data = [\n",
    "    (\"101\", 1000),\n",
    "    (\"102\", 1500),\n",
    "    (\"103\", 800),\n",
    "    (\"101\", 500),\n",
    "    (\"104\", 1200)\n",
    "]\n",
    "\n",
    "df = spark.createDataFrame(data, [\"emp_id\", \"sales\"])\n",
    "\n",
    "# Lookup Dictionary (Small dataset: Employee ID -> Name)\n",
    "# In a real scenario, this might come from a database or another small file.\n",
    "emp_lookup = {\"101\": \"John\", \"102\": \"Jane\", \"103\": \"Bob\", \"104\": \"Alice\"}\n",
    "\n",
    "print(\"Data Prepared\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6af51f",
   "metadata": {},
   "source": [
    "## 1. Broadcast Variables\n",
    "\n",
    "**Problem:** If we use `emp_lookup` directly in a UDF or map function, Spark serializes and sends a copy of this dictionary *with every task*. If the dictionary is large (e.g., 100MB) and you have 1000 tasks, that's huge network overhead.\n",
    "\n",
    "**Solution:** A **Broadcast Variable** sends the data to each **executor node** only once. All tasks on that node share the same read-only copy. This is highly efficient for \"Map-Side Joins\" or Lookups."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dd8c8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Create the Broadcast Variable\n",
    "broadcast_emp = spark.sparkContext.broadcast(emp_lookup)\n",
    "\n",
    "# Step 2: Use it in a UDF\n",
    "def get_emp_name(emp_id):\n",
    "    # Access the broadcasted value using .value\n",
    "    return broadcast_emp.value.get(emp_id, \"Unknown\")\n",
    "\n",
    "# Register UDF\n",
    "name_udf = udf(get_emp_name, StringType())\n",
    "\n",
    "# Apply\n",
    "df_with_names = df.withColumn(\"emp_name\", name_udf(col(\"emp_id\")))\n",
    "\n",
    "print(\"--- DataFrame using Broadcast Lookup ---\")\n",
    "df_with_names.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46a753ff",
   "metadata": {},
   "source": [
    "## 2. Accumulators\n",
    "\n",
    "**Problem:** If you define `counter = 0` in your driver and try to increment it inside a Spark transformation (like `foreach`), the update happens on the copy of the variable in the executor, not the driver. The driver's counter remains 0.\n",
    "\n",
    "**Solution:** **Accumulators** are variables that are only \"added\" to through an associative and commutative operation. They are perfect for global counters (e.g., counting bad records)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ea6eb9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Step 1: Initialize Accumulator\n",
    "high_sales_counter = spark.sparkContext.accumulator(0)\n",
    "\n",
    "# Define a function to update the accumulator\n",
    "def count_high_sales(row):\n",
    "    if row.sales > 1000:\n",
    "        high_sales_counter.add(1)\n",
    "\n",
    "# Step 2: Use foreach (Action) to iterate and update\n",
    "# Note: Accumulators do not change the DataFrame, they just update the variable.\n",
    "df.foreach(count_high_sales)\n",
    "\n",
    "# Step 3: Read the value back on the Driver\n",
    "print(f\"Number of High Value Sales (> 1000): {high_sales_counter.value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c99c9b5",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Broadcast Variables:**\n",
    "    *   Use for read-only lookup data (dictionaries, sets).\n",
    "    *   Reduces network I/O by sending data once per node, not per task.\n",
    "    *   Access via `.value`.\n",
    "\n",
    "2.  **Accumulators:**\n",
    "    *   Use for global counters across the cluster (e.g., error counts, processed rows).\n",
    "    *   Only the driver can read the value (`.value`). Executors can only write to it (`.add()`).\n",
    "    *   **Warning:** Be careful putting accumulators inside transformations (like map) because if a task re-runs due to failure, the accumulator might over-count. `foreach` (Action) is safer.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **Spark Memory Management** and how to tune executor memory to avoid OOM errors."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
