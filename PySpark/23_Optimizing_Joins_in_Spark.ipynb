{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d3230062",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 23: Optimizing Joins in Spark\n",
    "\n",
    "Joins are one of the most resource-intensive operations in Spark because they usually involve **Shuffling** large amounts of data across the network.\n",
    "\n",
    "In this module, we will learn how to optimize joins by choosing the right strategy based on the table sizes and using techniques like **Broadcasting** and **Bucketing**.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Join Strategies:** Shuffle Hash Join vs. Sort Merge Join vs. Broadcast Hash Join.\n",
    "2.  **Big vs. Small Table:** Using `broadcast()` for Map-Side Joins.\n",
    "3.  **Big vs. Big Table:** Understanding Sort Merge Join and avoiding skew.\n",
    "4.  **Bucketing:** Pre-shuffling data to speed up joins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461886a1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, broadcast\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType\n",
    "import time\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Optimizing_Joins\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.autoBroadcastJoinThreshold\", -1) \\\n",
    "    # We disable auto-broadcast initially to force Shuffle Joins for demonstration\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active (Auto Broadcast Disabled)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ae1668f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Big Table: Transactions (1 Million rows)\n",
    "transactions_df = spark.range(1, 1000000).toDF(\"txn_id\") \\\n",
    "    .withColumn(\"store_id\", (col(\"txn_id\") % 10).cast(\"integer\")) \\\n",
    "    .withColumn(\"amount\", col(\"txn_id\") * 0.5)\n",
    "\n",
    "# 2. Small Table: Stores (10 rows)\n",
    "stores_data = [(i, f\"Store_{i}\", f\"City_{i}\") for i in range(10)]\n",
    "schema = StructType([\n",
    "    StructField(\"store_id\", IntegerType(), False),\n",
    "    StructField(\"store_name\", StringType(), True),\n",
    "    StructField(\"city\", StringType(), True)\n",
    "])\n",
    "stores_df = spark.createDataFrame(stores_data, schema)\n",
    "\n",
    "print(\"--- Transactions (Big Table) ---\")\n",
    "transactions_df.show(5)\n",
    "\n",
    "print(\"--- Stores (Small Table) ---\")\n",
    "stores_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e88c0b13",
   "metadata": {},
   "source": [
    "## 1. Standard Join (Sort Merge Join)\n",
    "\n",
    "When joining two large tables (or when broadcast is disabled), Spark defaults to **Sort Merge Join**.\n",
    "Steps:\n",
    "1.  **Shuffle:** Move data with the same key to the same partition.\n",
    "2.  **Sort:** Sort data within each partition by the join key.\n",
    "3.  **Merge:** Iterate through both sorted datasets and join matching rows.\n",
    "\n",
    "This is expensive due to shuffling and sorting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06ebef7e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Perform a standard join\n",
    "start_time = time.time()\n",
    "\n",
    "joined_df = transactions_df.join(stores_data, \"store_id\")\n",
    "# Trigger Action\n",
    "print(f\"Count: {joined_df.count()}\")\n",
    "\n",
    "print(f\"Time taken (Sort Merge Join): {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Explain Plan\n",
    "print(\"--- Execution Plan ---\")\n",
    "joined_df.explain()\n",
    "# Look for 'SortMergeJoin' and 'Exchange' (Shuffle) in the plan."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "285c1a5d",
   "metadata": {},
   "source": [
    "## 2. Broadcast Hash Join (Map-Side Join)\n",
    "\n",
    "If one table is **small** (fits in memory), we can avoid shuffling the large table.\n",
    "Spark sends a copy of the small table to **every executor**. Each executor then joins its partition of the large table with the local copy of the small table.\n",
    "\n",
    "**Benefits:**\n",
    "*   NO Shuffle for the large table.\n",
    "*   NO Sorting required.\n",
    "*   Extremely fast."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27f4d920",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Explicitly broadcast the small table\n",
    "start_time = time.time()\n",
    "\n",
    "broadcast_joined_df = transactions_df.join(broadcast(stores_df), \"store_id\")\n",
    "# Trigger Action\n",
    "print(f\"Count: {broadcast_joined_df.count()}\")\n",
    "\n",
    "print(f\"Time taken (Broadcast Join): {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Explain Plan\n",
    "print(\"--- Execution Plan ---\")\n",
    "broadcast_joined_df.explain()\n",
    "# Look for 'BroadcastHashJoin' and notice there is NO 'Exchange' for the Transactions table."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23b52ae6",
   "metadata": {},
   "source": [
    "## 3. Bucketing for Big Table Joins\n",
    "\n",
    "When joining two **large** tables, Broadcast is not possible (OOM error).\n",
    "However, if we frequently join these tables on a specific column (e.g., `user_id`), we can **Bucket** them.\n",
    "\n",
    "**Bucketing** pre-shuffles and sorts the data into fixed \"buckets\" (files) on disk. When we join two bucketed tables:\n",
    "*   Spark knows that data for `user_id=1` is in Bucket 1 for BOTH tables.\n",
    "*   It skips the Shuffle and Sort phases entirely during the join."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afe63005",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We need to save the data as a managed table to use bucketing.\n",
    "# Note: bucketBy requires saving as a Table, not just a file.\n",
    "\n",
    "db_name = \"spark_optimization_demo\"\n",
    "spark.sql(f\"CREATE DATABASE IF NOT EXISTS {db_name}\")\n",
    "spark.sql(f\"USE {db_name}\")\n",
    "\n",
    "# Save Transactions Data as Bucketed Table\n",
    "transactions_df.write \\\n",
    "    .bucketBy(4, \"store_id\") \\\n",
    "    .sortBy(\"store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"transactions_bucketed\")\n",
    "\n",
    "# Save Stores Data as Bucketed Table\n",
    "stores_df.write \\\n",
    "    .bucketBy(4, \"store_id\") \\\n",
    "    .sortBy(\"store_id\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"stores_bucketed\")\n",
    "\n",
    "print(\"Bucketed Tables Created.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9069da8b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read tables back\n",
    "t_bucketed = spark.table(\"transactions_bucketed\")\n",
    "s_bucketed = spark.table(\"stores_bucketed\")\n",
    "\n",
    "# Join them\n",
    "start_time = time.time()\n",
    "bucket_join_df = t_bucketed.join(s_bucketed, \"store_id\")\n",
    "print(f\"Count: {bucket_join_df.count()}\")\n",
    "print(f\"Time taken (Bucketed Join): {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Explain Plan\n",
    "print(\"--- Execution Plan (Bucketed) ---\")\n",
    "bucket_join_df.explain()\n",
    "# Ideally, you should NOT see 'Exchange' (Shuffle) here, \n",
    "# because the data was already pre-shuffled during the write phase."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4af62085",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Sort Merge Join:** Default for big tables. Safe but slow (Shuffle + Sort).\n",
    "2.  **Broadcast Join:** Best for Big + Small table. Avoiding shuffle makes it very fast. Use `broadcast()`.\n",
    "3.  **Bucketing:** Best for frequent Big + Big table joins. Pre-shuffles data on write to speed up future reads/joins.\n",
    "\n",
    "**Next Steps:**\n",
    "This concludes the core optimization techniques. In the next (and final) notebook, we will briefly cover **Spark SQL** syntax and how to mix SQL with DataFrames seamlessly."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
