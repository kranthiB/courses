{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4642e19",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 24: Dynamic Allocation and Resource Management\n",
    "\n",
    "In a shared cluster environment, managing resources efficiently is critical. \n",
    "\n",
    "*   **Static Allocation (Default):** You request a fixed number of executors (e.g., `--num-executors 10`) at the start. These resources are reserved for your app until it finishes, even if the app is idle. This leads to resource wastage.\n",
    "*   **Dynamic Allocation:** Spark adds executors when there are pending tasks (Scale Up) and removes executors when they have been idle for a specific time (Scale Down).\n",
    "\n",
    "### Agenda:\n",
    "1.  **Static vs. Dynamic:** Understanding the difference.\n",
    "2.  **Configuration:** Enabling Dynamic Allocation properties.\n",
    "3.  **Shuffle Tracking:** Handling shuffle data when executors are removed.\n",
    "4.  **Hands-on:** Configuring a SparkSession with Dynamic Allocation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c7645",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import time\n",
    "\n",
    "# Note: Dynamic Allocation is most effective on a Cluster Manager (Standalone, YARN, K8s).\n",
    "# In 'local' mode, these settings might not trigger physical scaling of processes \n",
    "# as everything runs in one JVM, but this is how you configure it for Production.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Dynamic_Allocation_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.shuffleTracking.enabled\", \"true\") \\\n",
    "    .config(\"spark.dynamicAllocation.minExecutors\", \"0\") \\\n",
    "    .config(\"spark.dynamicAllocation.maxExecutors\", \"5\") \\\n",
    "    .config(\"spark.dynamicAllocation.initialExecutors\", \"1\") \\\n",
    "    .config(\"spark.dynamicAllocation.executorIdleTimeout\", \"60s\") \\\n",
    "    .config(\"spark.dynamicAllocation.schedulerBacklogTimeout\", \"1s\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Active with Dynamic Allocation Configured\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40a6092",
   "metadata": {},
   "source": [
    "## 2. Key Configurations Explained\n",
    "\n",
    "1.  **`spark.dynamicAllocation.enabled`**: Master switch to turn the feature on.\n",
    "2.  **`spark.dynamicAllocation.shuffleTracking.enabled`**: \n",
    "    *   When an executor is removed, the shuffle files it wrote to disk are usually lost.\n",
    "    *   In older versions, you needed an \"External Shuffle Service\" running on each worker node.\n",
    "    *   In newer versions (and on K8s), **Shuffle Tracking** allows the driver to track where shuffle data resides and prevents killing executors that hold active shuffle data.\n",
    "3.  **`...minExecutors` / `...maxExecutors`**: The boundaries for scaling.\n",
    "4.  **`...executorIdleTimeout`**: How long an executor must be idle (no running tasks) before it is removed.\n",
    "5.  **`...schedulerBacklogTimeout`**: How long pending tasks must wait before Spark requests *new* executors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa742ba",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's run a job that requires computation to trigger executor allocation.\n",
    "# In a real cluster, you would see the number of executors increase in the 'Executors' tab of Spark UI.\n",
    "\n",
    "print(\"Starting Job...\")\n",
    "\n",
    "# Creating a large range and performing a map transformation\n",
    "# This generates tasks. If we were on a cluster, Spark would request more executors \n",
    "# to handle these tasks in parallel if the backlog timeout is exceeded.\n",
    "rdd = spark.sparkContext.range(0, 10000000)\n",
    "count = rdd.map(lambda x: x * x).count()\n",
    "\n",
    "print(f\"Job Finished. Count: {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3ad2c4a",
   "metadata": {},
   "source": [
    "## 3. How to Observe Scaling (Concept)\n",
    "\n",
    "If you were running this on a **Spark Standalone Cluster** or **YARN**:\n",
    "\n",
    "1.  **Initial State:** Application starts with `initialExecutors` (e.g., 1).\n",
    "2.  **Job Submission:** When you run the code above, tasks pile up in the scheduler.\n",
    "3.  **Scale Up:** After `schedulerBacklogTimeout` (e.g., 1s), Spark requests more executors from the Resource Manager up to `maxExecutors`.\n",
    "4.  **Job Completion:** Once the job is done, the executors become idle.\n",
    "5.  **Scale Down:** After `executorIdleTimeout` (e.g., 60s), Spark starts killing the idle executors to free up cluster resources for other users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16ec602b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# If running on a cluster, we would wait here for > 60 seconds to see executors disappear.\n",
    "print(\"Waiting to simulate idle time...\")\n",
    "# time.sleep(70) \n",
    "print(\"Check Spark UI 'Executors' tab. In a real cluster, idle executors would be removed now.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261274ed",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Resource Efficiency:** Dynamic Allocation allows clusters to be utilized more efficiently by releasing unused resources.\n",
    "2.  **Shuffle Data:** The biggest challenge is preserving shuffle data when executors die. Use **External Shuffle Service** or **Shuffle Tracking**.\n",
    "3.  **Databricks vs. Spark:** \n",
    "    *   **Spark Dynamic Allocation:** Scales **Executors** (processes) inside existing nodes.\n",
    "    *   **Databricks Autoscaling:** Scales the actual **VMs/Nodes** (infrastructure) up and down.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **Spark Speculation**, a mechanism to handle slow-running tasks (stragglers)."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
