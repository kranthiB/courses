{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4531e08a",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 25: Skewness, Spillage, and Salting\n",
    "\n",
    "In distributed computing, **Data Skew** is one of the most common performance bottlenecks. It occurs when data is not evenly distributed across partitions, causing a few tasks to take significantly longer than others (stragglers).\n",
    "\n",
    "This often leads to **Spillage**, where the data for a single task exceeds the memory allocated to the executor, forcing Spark to write intermediate data to disk. This serialization/deserialization process drastically slows down the job.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Understanding Skewness:** How it looks in the Spark UI.\n",
    "2.  **Spillage:** Memory vs. Disk spillage.\n",
    "3.  **Diagnosis:** finding the skewed partition using `spark_partition_id()`.\n",
    "4.  **The Solution:** Implementing the **Salting Technique** to redistribute data evenly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feaa37ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "\n",
    "# We initiate the session with specific memory configs to easily reproduce spillage on smaller data\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Skewness_and_Salting\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Note: We disable Adaptive Query Execution (AQE) because AQE can automatically \n",
    "# handle skew joins in newer Spark versions. We want to see the problem manually first.\n",
    "\n",
    "print(\"Spark Session Created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "854cb08c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# In the video, we used external CSVs. Here, let's simulate skewed data \n",
    "# so you can run this notebook immediately.\n",
    "\n",
    "# 1. Create Department Data (Small lookup table)\n",
    "dept_data = [(i, f\"Dept_{i}\") for i in range(0, 10)]\n",
    "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "# 2. Create Skewed Employee Data\n",
    "# We will generate 1 million records. \n",
    "# 90% of employees will belong to dept_id 9 and 10 (Creating Skew)\n",
    "def generate_skewed_data():\n",
    "    data = []\n",
    "    for _ in range(1000000):\n",
    "        # High probability for dept 9 and 8\n",
    "        if random.random() > 0.1:\n",
    "            dept = random.choice([8, 9])\n",
    "        else:\n",
    "            dept = random.choice(range(0, 8))\n",
    "        data.append((dept, \"Emp_Name\"))\n",
    "    return data\n",
    "\n",
    "# Note: In a real scenario, this step involves reading a large file.\n",
    "# Generating 1M rows in local python might take a moment.\n",
    "print(\"Generating Skewed Data...\")\n",
    "emp_rdd = spark.sparkContext.parallelize(generate_skewed_data())\n",
    "emp_df = spark.createDataFrame(emp_rdd, [\"dept_id\", \"emp_name\"])\n",
    "\n",
    "print(\"Dataframes Ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42add8d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's perform a standard join.\n",
    "# Because dept_id 8 and 9 have massive amounts of data, the tasks processing \n",
    "# those specific keys will process significantly more records than others.\n",
    "\n",
    "df_joined = emp_df.join(dept_df, \"dept_id\", \"left_outer\")\n",
    "\n",
    "# Trigger the action (No-Op write is used for benchmarking)\n",
    "print(\"Running Skewed Join...\")\n",
    "df_joined.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(\"Skewed Join Completed.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1315c170",
   "metadata": {},
   "source": [
    "## 2. Diagnosing the Issue\n",
    "\n",
    "If you look at the **Spark UI** (Stages tab) for the job above:\n",
    "1.  **Event Timeline:** You will see most green bars (tasks) finish instantly, but one or two bars stretch out for much longer.\n",
    "2.  **Summary Metrics:** The *Max* duration will be much higher than *Median* or *75th percentile*.\n",
    "3.  **Spill (Memory/Disk):** You might see columns for \"Spill (Memory)\" and \"Spill (Disk)\". This means the data didn't fit in RAM.\n",
    "\n",
    "### Verifying Partition Distribution via Code\n",
    "We can use the `spark_partition_id()` function to group by partition and count records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dfc121",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Analyze the distribution of data across partitions after shuffle\n",
    "skew_analysis = df_joined \\\n",
    "    .withColumn(\"partition_num\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_num\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"Top 5 Heaviest Partitions:\")\n",
    "skew_analysis.show(5)\n",
    "\n",
    "# You will likely see 1 or 2 partitions with huge counts (e.g., 400k+) \n",
    "# while others have very few."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f2458eb",
   "metadata": {},
   "source": [
    "## 3. The Solution: Salting\n",
    "\n",
    "**Salting** involves adding a random number (the \"salt\") to the join keys of the skewed dataset (Employee) and replicating the rows of the small dataset (Department) to match those salts.\n",
    "\n",
    "**Logic:**\n",
    "1.  **Salt Factor:** Decide a number, e.g., 16 (matches your core count or a multiple of it).\n",
    "2.  **Small Table:** Cross Join (multiply) the department table with numbers 0-15. Create a new key `dept_id_salt` (e.g., `8_0`, `8_1`... `8_15`).\n",
    "3.  **Large Table:** Add a *random* number 0-15 to every employee row. Create a new key `dept_id_salt` (e.g., `8_3`).\n",
    "4.  **Join:** Join on the new composite key `dept_id_salt`.\n",
    "\n",
    "This breaks the massive \"bucket\" for Department 8 into 16 smaller buckets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c165f9a8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "SALT_FACTOR = 16  # Splitting the skewed key into 16 parts\n",
    "\n",
    "# 1. Create a DataFrame containing the range of salts\n",
    "salt_df = spark.range(0, SALT_FACTOR).toDF(\"salt_id\")\n",
    "\n",
    "# 2. Cross Join Department with Salt Range\n",
    "# If Dept table has 10 rows, it will now have 10 * 16 = 160 rows\n",
    "salted_dept_df = dept_df.crossJoin(salt_df) \\\n",
    "    .withColumn(\"salted_dept_id\", concat(col(\"dept_id\"), lit(\"_\"), col(\"salt_id\"))) \\\n",
    "    .drop(\"salt_id\")\n",
    "\n",
    "print(\"Salted Department Data Sample:\")\n",
    "salted_dept_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31ba0d2a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Add a random salt (0 to 15) to every record in the large skewed table\n",
    "salted_emp_df = emp_df \\\n",
    "    .withColumn(\"salt_id\", (rand() * SALT_FACTOR).cast(\"int\")) \\\n",
    "    .withColumn(\"salted_dept_id\", concat(col(\"dept_id\"), lit(\"_\"), col(\"salt_id\"))) \\\n",
    "    .drop(\"salt_id\")\n",
    "\n",
    "print(\"Salted Employee Data Sample:\")\n",
    "salted_emp_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac57ab4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Now we join on the new 'salted_dept_id'. \n",
    "# Since the data for dept 8 is now broken into 16 chunks (8_0 to 8_15), \n",
    "# Spark can process these chunks in parallel tasks!\n",
    "\n",
    "salted_joined_df = salted_emp_df.join(\n",
    "    salted_dept_df, \n",
    "    on=\"salted_dept_id\", \n",
    "    how=\"left_outer\"\n",
    ")\n",
    "\n",
    "print(\"Running Salted Join...\")\n",
    "# Trigger action\n",
    "salted_joined_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(\"Salted Join Completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1d1bb83",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify if the data is distributed more evenly now.\n",
    "salted_analysis = salted_joined_df \\\n",
    "    .withColumn(\"partition_num\", spark_partition_id()) \\\n",
    "    .groupBy(\"partition_num\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "\n",
    "print(\"Top 5 Heaviest Partitions (After Salting):\")\n",
    "salted_analysis.show(5)\n",
    "\n",
    "# You should see the counts are much lower and closer to each other \n",
    "# compared to the static join."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde618c8",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Skewness:** Occurs when keys are not unique or data is concentrated on specific keys.\n",
    "2.  **Spillage:** When memory fills up, Spark writes to disk, causing severe performance hits.\n",
    "3.  **Salting:**\n",
    "    *   **Pros:** Ideally distributes skewed data, eliminates spillage, allows parallel processing of stragglers.\n",
    "    *   **Cons:** Increases the size of the smaller table (replication). Be careful if the \"small\" table is actually quite large.\n",
    "4.  **AQE:** In Spark 3.0+, enabling `spark.sql.adaptive.enabled` can often handle skew automatically without needing manual salting code. However, knowing Salting is vital for edge cases or older Spark versions."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
