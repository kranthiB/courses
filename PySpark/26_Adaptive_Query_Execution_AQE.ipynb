{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "36e41c97",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 26: Adaptive Query Execution (AQE)\n",
    "\n",
    "Adaptive Query Execution (AQE) is an optimization technique in Spark SQL that makes use of **runtime statistics** to choose the most efficient query execution plan. \n",
    "\n",
    "In previous versions (Spark 2.x), the query plan was fixed once generated. In Spark 3.x with AQE, the plan can evolve while the query is running based on the actual data characteristics observed at intermediate stages.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Baseline:** Running a Skewed Join *without* AQE (Standard SortMergeJoin).\n",
    "2.  **Enable AQE:** Configuring Spark to use Adaptive Query Execution.\n",
    "3.  **Feature 1: Coalescing Post-Shuffle Partitions:** Dynamically reducing the number of shuffle partitions.\n",
    "4.  **Feature 2: Optimizing Skew Joins:** Automatically splitting skewed partitions.\n",
    "5.  **Feature 3: Dynamic Join Selection:** Switching from SortMergeJoin to BroadcastJoin at runtime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7769180",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "import random\n",
    "import time\n",
    "\n",
    "# Initialize Spark Session\n",
    "# We set shuffle partitions to 200 to demonstrate the \"Too many partitions\" problem\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"AQE_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"512m\") \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"200\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Create Department Data\n",
    "dept_data = [(i, f\"Dept_{i}\") for i in range(0, 10)]\n",
    "dept_df = spark.createDataFrame(dept_data, [\"dept_id\", \"dept_name\"])\n",
    "\n",
    "# 2. Create Skewed Employee Data (1 Million rows, skew on dept 8 and 9)\n",
    "# This is the same data generation logic from the previous module\n",
    "def generate_skewed_data():\n",
    "    data = []\n",
    "    for _ in range(1000000):\n",
    "        if random.random() > 0.1:\n",
    "            dept = random.choice([8, 9])\n",
    "        else:\n",
    "            dept = random.choice(range(0, 8))\n",
    "        data.append((dept, \"Emp_Name\"))\n",
    "    return data\n",
    "\n",
    "emp_rdd = spark.sparkContext.parallelize(generate_skewed_data())\n",
    "emp_df = spark.createDataFrame(emp_rdd, [\"dept_id\", \"emp_name\"])\n",
    "\n",
    "print(\"Data Generation Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9dd7e38",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# To see the impact of AQE, we first disable it.\n",
    "# We also disable autoBroadcastJoin to force a SortMergeJoin/Shuffle.\n",
    "\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"false\")\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\") # Disable Broadcast\n",
    "\n",
    "print(\"AQE Disabled. Running Baseline Join...\")\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df = emp_df.join(dept_df, \"dept_id\", \"left_outer\")\n",
    "# Trigger action\n",
    "joined_df.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(f\"Baseline Join Duration: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Observation in Spark UI:\n",
    "# 1. Check Stages: You will see 200 shuffle partitions.\n",
    "# 2. Most partitions are empty (processing 0 records), wasting resources.\n",
    "# 3. One or two partitions process huge data (skew), potentially causing spillage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85834fa3",
   "metadata": {},
   "source": [
    "## AQE Optimization Features\n",
    "\n",
    "When we enable `spark.sql.adaptive.enabled`, Spark unlocks three capabilities:\n",
    "\n",
    "1.  **Coalescing Shuffle Partitions:**\n",
    "    *   Instead of creating 200 fixed partitions, AQE looks at the data size after the shuffle map stage.\n",
    "    *   If partitions are small, it merges (coalesces) them into fewer, larger partitions.\n",
    "    \n",
    "2.  **Optimizing Skew Joins:**\n",
    "    *   If a partition is significantly larger than the median size, AQE splits it into smaller sub-partitions.\n",
    "    *   This happens automatically without manual \"Salting\".\n",
    "\n",
    "3.  **Converting Sort-Merge Join to Broadcast Join:**\n",
    "    *   If a table size is estimated to be large initially but turns out to be small after filtering (at runtime), AQE can switch the strategy to Broadcast Join dynamically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f07477f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Enable AQE\n",
    "spark.conf.set(\"spark.sql.adaptive.enabled\", \"true\")\n",
    "\n",
    "# Feature 1: Coalesce Partitions\n",
    "spark.conf.set(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\")\n",
    "# Set advisory size small (8MB) to trigger coalescing behavior on our small local data\n",
    "spark.conf.set(\"spark.sql.adaptive.advisoryPartitionSizeInBytes\", \"8m\") \n",
    "\n",
    "# Feature 2: Skew Join Optimization\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.enabled\", \"true\")\n",
    "# Set threshold small (10MB) to detect our local data as \"skewed\"\n",
    "spark.conf.set(\"spark.sql.adaptive.skewJoin.skewedPartitionThresholdInBytes\", \"10m\")\n",
    "\n",
    "# Keep Broadcast disabled for now to verify Skew handling in SortMergeJoin\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"-1\")\n",
    "\n",
    "print(\"AQE Enabled (Coalesce + Skew). Running Join...\")\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df_aqe = emp_df.join(dept_df, \"dept_id\", \"left_outer\")\n",
    "joined_df_aqe.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(f\"AQE Join Duration: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Observation in Spark UI:\n",
    "# 1. Shuffle Partitions: Instead of 200, you might see a number like 10 or 17 (Coalesced).\n",
    "# 2. Skew: The timeline should look more balanced. The skewed partition was split.\n",
    "# 3. Plan: Use joined_df_aqe.explain() to see 'CustomShuffleReader' in the plan."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03fa9b96",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify that partitions were actually coalesced.\n",
    "# Note: We use spark_partition_id() on the resulting DF.\n",
    "\n",
    "from pyspark.sql.functions import spark_partition_id\n",
    "\n",
    "part_count = joined_df_aqe.withColumn(\"pid\", spark_partition_id()).select(\"pid\").distinct().count()\n",
    "print(f\"Number of partitions used in AQE join: {part_count}\")\n",
    "# Expectation: Much lower than 200."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa6de2e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Feature 3: Converting SortMergeJoin to BroadcastJoin at Runtime\n",
    "# We re-enable the broadcast threshold (e.g., 10MB)\n",
    "spark.conf.set(\"spark.sql.autoBroadcastJoinThreshold\", \"10485760\") # 10MB\n",
    "\n",
    "print(\"AQE Enabled (Full Features). Running Join...\")\n",
    "\n",
    "start_time = time.time()\n",
    "joined_df_broadcast = emp_df.join(dept_df, \"dept_id\", \"left_outer\")\n",
    "joined_df_broadcast.write.format(\"noop\").mode(\"overwrite\").save()\n",
    "print(f\"AQE + Dynamic Broadcast Duration: {time.time() - start_time:.2f} seconds\")\n",
    "\n",
    "# Observation:\n",
    "# Even if we initially planned for a SortMergeJoin, AQE realizes 'dept_df' is tiny \n",
    "# at runtime and switches to a BroadcastHashJoin.\n",
    "# This avoids the shuffle entirely!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a04fdce",
   "metadata": {},
   "source": [
    "## Summary: Why use AQE?\n",
    "\n",
    "1.  **Simplicity:** You don't need to manually tune `spark.sql.shuffle.partitions` perfectly for every job. AQE adjusts it.\n",
    "2.  **Robustness:** It handles data skew automatically, removing the need for complex code hacks like Salting in most cases.\n",
    "3.  **Performance:** Dynamic strategy selection (switching to Broadcast) saves massive amounts of network I/O.\n",
    "\n",
    "**Note:** AQE is enabled by default in Spark 3.2+, but understanding these configs helps when debugging specific edge cases."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
