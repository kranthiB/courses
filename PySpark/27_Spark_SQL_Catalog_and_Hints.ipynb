{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "28532859",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 27: Spark SQL, Catalogs, and Query Hints\n",
    "\n",
    "Spark SQL allows us to query data using standard SQL syntax, providing an alternative to the DataFrame API. This module explores how to use Spark SQL, manage metadata using Catalogs, and optimize queries using SQL Hints.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Spark SQL Basics:** Registering DataFrames as Temp Views and querying them with SQL.\n",
    "2.  **Catalog Implementation:** Understanding `in-memory` vs. `hive` catalog for persisting metadata.\n",
    "3.  **Persistence:** Creating Managed Tables that survive application restarts.\n",
    "4.  **SQL Hints:** Using hints like `BROADCAST` and `SHUFFLE_MERGE` directly in SQL queries to control execution plans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e334607",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Initially, we start with the default \"in-memory\" catalog\n",
    "# This means tables created here will be lost when the session restarts.\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_SQL_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"in-memory\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created with In-Memory Catalog\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56fbe666",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create sample DataFrame\n",
    "data = [\n",
    "    (1, \"Alice\", 1000, \"HR\"),\n",
    "    (2, \"Bob\", 1200, \"Engineering\"),\n",
    "    (3, \"Charlie\", 1100, \"Engineering\"),\n",
    "    (4, \"David\", 1300, \"HR\")\n",
    "]\n",
    "schema = [\"id\", \"name\", \"salary\", \"dept_name\"]\n",
    "emp_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# 1. Register as Temporary View\n",
    "# This makes the dataframe accessible via SQL within this session\n",
    "emp_df.createOrReplaceTempView(\"employee\")\n",
    "\n",
    "# 2. Query using Spark SQL\n",
    "result = spark.sql(\"\"\"\n",
    "    SELECT dept_name, AVG(salary) as avg_salary\n",
    "    FROM employee\n",
    "    GROUP BY dept_name\n",
    "\"\"\")\n",
    "\n",
    "print(\"SQL Query Result:\")\n",
    "result.show()\n",
    "\n",
    "# 3. Check Catalog\n",
    "print(\"Listing Tables in Default Database:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5eedb1",
   "metadata": {},
   "source": [
    "## 2. Using Hive Catalog for Persistence\n",
    "\n",
    "The default catalog is transient. To persist table metadata (schema, location) across different Spark sessions, we enable Hive support. This creates a `metastore_db` locally (by default using Derby) to store table definitions.\n",
    "\n",
    "*Note: You might need to restart the kernel if switching catalog implementations in the same notebook context causes issues, but typically Spark handles separate sessions well if configured correctly.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d739b8c3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Stop previous session to switch configuration\n",
    "spark.stop()\n",
    "\n",
    "# Enable Hive Support to persist metadata\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_SQL_Hive_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"spark-warehouse\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Restarted with Hive Support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a8d9636",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Re-create DataFrame\n",
    "emp_df = spark.createDataFrame(data, schema)\n",
    "\n",
    "# Save as a MANAGED Table\n",
    "# This saves both data (in warehouse dir) and metadata (in metastore_db)\n",
    "emp_df.write.mode(\"overwrite\").saveAsTable(\"employee_managed\")\n",
    "\n",
    "print(\"Table 'employee_managed' created.\")\n",
    "\n",
    "# Verify it exists in catalog\n",
    "spark.sql(\"SHOW TABLES\").show()\n",
    "\n",
    "# Querying the table\n",
    "spark.sql(\"SELECT * FROM employee_managed WHERE salary > 1100\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "492abfd2",
   "metadata": {},
   "source": [
    "## 3. SQL Query Hints\n",
    "\n",
    "Just like in the DataFrame API, we can guide the optimizer using Hints in SQL.\n",
    "\n",
    "*   **Broadcast Hint:** `/*+ BROADCAST(table_name) */` - Forces a broadcast join.\n",
    "*   **Merge Join Hint:** `/*+ SHUFFLE_MERGE(table_name) */` - Forces a SortMergeJoin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "673a4289",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a small department DataFrame for joining\n",
    "dept_data = [(\"HR\", \"Human Resources\"), (\"Engineering\", \"Software Engineering\")]\n",
    "dept_df = spark.createDataFrame(dept_data, [\"dept_name\", \"dept_full_name\"])\n",
    "dept_df.createOrReplaceTempView(\"department\")\n",
    "\n",
    "# 1. Force Broadcast Join using SQL Hint\n",
    "print(\"Plan with Broadcast Hint:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ BROADCAST(d) */ \n",
    "        e.name, d.dept_full_name \n",
    "    FROM employee_managed e \n",
    "    JOIN department d ON e.dept_name = d.dept_name\n",
    "\"\"\").explain()\n",
    "\n",
    "# 2. Force Sort Merge Join using SQL Hint\n",
    "print(\"\\nPlan with Merge Hint:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT /*+ SHUFFLE_MERGE(d) */ \n",
    "        e.name, d.dept_full_name \n",
    "    FROM employee_managed e \n",
    "    JOIN department d ON e.dept_name = d.dept_name\n",
    "\"\"\").explain()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07805160",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Spark SQL:** Provides a declarative way to query DataFrames using SQL.\n",
    "2.  **Catalog:** \n",
    "    *   `in-memory`: Transient (lost on restart).\n",
    "    *   `hive`: Persistent (saves metadata to metastore).\n",
    "3.  **Tables:** `saveAsTable` creates persistent tables that can be queried in future sessions.\n",
    "4.  **Hints:** Use SQL comment syntax `/*+ HINT_NAME(table) */` to enforce join strategies."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
