{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f13d2b5d",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 28: Azure Cosmos DB Integration\n",
    "\n",
    "Azure Cosmos DB is a fully managed NoSQL database for modern app development. In big data pipelines, it is common to offload processed data from Spark to Cosmos DB for low-latency serving or to read transactional data from Cosmos DB into Spark for analytics.\n",
    "\n",
    "### Agenda:\n",
    "1.  **NoSQL vs SQL:** Understanding the need for Cosmos DB.\n",
    "2.  **Setup:** Configuring the Spark Session with the Cosmos DB Connector.\n",
    "3.  **Writing Data:** Loading JSON data into Cosmos DB using `ItemAppend` and `ItemOverwrite`.\n",
    "4.  **Reading Data:** Querying data from Cosmos DB into a Dataframe.\n",
    "5.  **Operations:** Deleting items using `ItemDelete`.\n",
    "6.  **Security:** Best practices for managing credentials.\n",
    "\n",
    "### Prerequisites\n",
    "To run this notebook, you need:\n",
    "1.  An **Azure Cosmos DB for NoSQL** account created in the Azure Portal.\n",
    "2.  A Database named `self` and a Container named `device-data`.\n",
    "3.  Partition Key: `/customerid`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37ecfdb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "\n",
    "# We need to load the Azure Cosmos DB Spark Connector.\n",
    "# The version depends on your Spark version.\n",
    "# For Spark 3.3.x, we use: com.azure:azure-cosmos-spark_3-3_2-12:4.15.0\n",
    "# Check Maven Central for the latest version compatible with your environment.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"CosmosDB_Integration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \"com.azure:azure-cosmos-spark_3-3_2-12:4.15.0\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created with Cosmos DB Connector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a99d5b53",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# -----------------------------------------------------------------\n",
    "# ⚠️ WARNING: Do not commit real keys to version control (Git).\n",
    "# In production, use Azure Key Vault or Spark Configs (spark-defaults.conf).\n",
    "# -----------------------------------------------------------------\n",
    "\n",
    "# Replace these with your actual Azure Cosmos DB credentials\n",
    "cosmos_endpoint = \"https://<YOUR_COSMOS_ACCOUNT>.documents.azure.com:443/\"\n",
    "cosmos_master_key = \"<YOUR_PRIMARY_KEY>\"\n",
    "cosmos_database_name = \"self\"\n",
    "cosmos_container_name = \"device-data\"\n",
    "\n",
    "# Base Configuration Dictionary\n",
    "cosmos_config = {\n",
    "    \"spark.cosmos.accountEndpoint\": cosmos_endpoint,\n",
    "    \"spark.cosmos.accountKey\": cosmos_master_key,\n",
    "    \"spark.cosmos.database\": cosmos_database_name,\n",
    "    \"spark.cosmos.container\": cosmos_container_name\n",
    "}\n",
    "\n",
    "print(\"Configuration Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6af6a5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's create sample device data similar to the video\n",
    "data = [\n",
    "    {\n",
    "        \"eventId\": \"e001\",\n",
    "        \"customerId\": \"C001\",\n",
    "        \"deviceType\": \"Sensor\",\n",
    "        \"temperature\": 75,\n",
    "        \"timestamp\": \"2023-01-01T10:00:00\"\n",
    "    },\n",
    "    {\n",
    "        \"eventId\": \"e002\",\n",
    "        \"customerId\": \"C002\",\n",
    "        \"deviceType\": \"Thermostat\",\n",
    "        \"temperature\": 68,\n",
    "        \"timestamp\": \"2023-01-01T10:05:00\"\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create DataFrame\n",
    "df = spark.createDataFrame(data)\n",
    "\n",
    "# Cosmos DB requires a unique 'id' field for every document.\n",
    "# In our data, 'eventId' is unique, so we map 'eventId' to 'id'.\n",
    "df_to_write = df.withColumn(\"id\", col(\"eventId\"))\n",
    "\n",
    "print(\"Data prepared for writing:\")\n",
    "df_to_write.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b617666f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Writing data to Cosmos DB using \"cosmos.oltp\" format\n",
    "# spark.cosmos.write.strategy: \"ItemAppend\" (Default) - Adds new items.\n",
    "\n",
    "try:\n",
    "    df_to_write.write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**cosmos_config) \\\n",
    "        .option(\"spark.cosmos.write.strategy\", \"ItemAppend\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Data successfully written to Cosmos DB.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Cosmos DB: {e}\")\n",
    "    print(\"Ensure your Cosmos DB Endpoint and Key are correct and the container exists.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df9e3c0c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading data back from Cosmos DB to verify\n",
    "# Note: inferSchema=true allows Spark to detect data types from JSON documents\n",
    "\n",
    "try:\n",
    "    df_read = spark.read \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**cosmos_config) \\\n",
    "        .option(\"spark.cosmos.read.inferSchema.enabled\", \"true\") \\\n",
    "        .load()\n",
    "\n",
    "    print(\"Data read from Cosmos DB:\")\n",
    "    df_read.show(truncate=False)\n",
    "    df_read.printSchema()\n",
    "except Exception as e:\n",
    "    print(f\"Error reading from Cosmos DB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbadefbe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# To update an item, we modify the data and use \"ItemOverwrite\" strategy.\n",
    "# We MUST provide the same 'id' and 'partition key' (customerId) to find and replace the item.\n",
    "\n",
    "# Let's change temperature for eventId 'e001'\n",
    "df_updated = df_to_write.withColumn(\n",
    "    \"temperature\", \n",
    "    when(col(\"id\") == \"e001\", 90).otherwise(col(\"temperature\"))\n",
    ")\n",
    "\n",
    "try:\n",
    "    df_updated.write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**cosmos_config) \\\n",
    "        .option(\"spark.cosmos.write.strategy\", \"ItemOverwrite\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Data updated in Cosmos DB.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error updating Cosmos DB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "715700c6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# To delete items, we only need the 'id' and the 'partition key'.\n",
    "# We can select specific rows to delete.\n",
    "\n",
    "df_to_delete = df_updated.filter(col(\"id\") == \"e002\").select(\"id\", \"customerId\")\n",
    "\n",
    "try:\n",
    "    df_to_delete.write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**cosmos_config) \\\n",
    "        .option(\"spark.cosmos.write.strategy\", \"ItemDelete\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "        \n",
    "    print(\"Item e002 deleted from Cosmos DB.\")\n",
    "except Exception as e:\n",
    "    print(f\"Error deleting from Cosmos DB: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4b5367c",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Connectors:** You must import the specific `azure-cosmos-spark` JAR matching your Spark version.\n",
    "2.  **Format:** Use `cosmos.oltp` for transactional read/write.\n",
    "3.  **Write Strategies:**\n",
    "    *   `ItemAppend`: Insert new items (fails on conflict by default unless configured).\n",
    "    *   `ItemOverwrite`: Upsert (Insert or Update).\n",
    "    *   `ItemDelete`: Delete items based on `id` and Partition Key.\n",
    "4.  **ID Requirement:** Every item in Cosmos DB must have a unique string column named `id`.\n",
    "\n",
    "### Security Best Practice\n",
    "In the video, we moved the keys to `spark-defaults.conf` (or `spark-env.sh`) on the cluster.\n",
    "This allows you to access configs like:\n",
    "```python\n",
    "# In production code\n",
    "endpoint = spark.conf.get(\"spark.cosmos.accountEndpoint\")\n",
    "key = spark.conf.get(\"spark.cosmos.accountKey\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
