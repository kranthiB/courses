{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "df8b477c",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 29: Delta Lake, Time Travel, and Schema Evolution\n",
    "\n",
    "Delta Lake is an open-source storage layer that brings reliability, performance, and lifecycle management to data lakes. It enables the **Lakehouse Architecture**, combining the best elements of Data Lakes and Data Warehouses.\n",
    "\n",
    "### Key Features of Delta Lake:\n",
    "1.  **ACID Transactions:** Ensures data integrity.\n",
    "2.  **Time Travel:** Access previous versions of data.\n",
    "3.  **Schema Evolution:** Allows schema changes without corrupting data.\n",
    "4.  **DML Support:** Supports UPDATE, DELETE, and MERGE operations.\n",
    "5.  **Scalable Metadata:** Handles petabytes of data efficiently.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Setup:** Configuring Spark with Delta Lake support.\n",
    "2.  **Data Writing:** Saving data as a Delta table.\n",
    "3.  **DML Operations:** Updating and Deleting data (not possible in standard Parquet).\n",
    "4.  **Time Travel:** Querying older versions of the data.\n",
    "5.  **Vacuum:** Cleaning up old files to save space.\n",
    "6.  **Schema Evolution:** Adding new columns dynamically using `mergeSchema`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51147d4f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from delta import *\n",
    "\n",
    "# Configure Spark Session with Delta Lake packages\n",
    "# Note: If running on Databricks, this is pre-configured.\n",
    "# For local setups, ensure you have the correct delta-spark version.\n",
    "\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Delta_Lake_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") # Adjust version as needed\n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created with Delta Lake Support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7db7a006",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Sample Sales Data\n",
    "data = [\n",
    "    (1, \"Widget A\", 100, \"2023-01-01\"),\n",
    "    (2, \"Widget B\", 150, \"2023-01-02\"),\n",
    "    (3, \"Widget A\", 100, \"2023-01-03\"),\n",
    "    (4, \"Widget C\", 200, \"2023-01-04\")\n",
    "]\n",
    "columns = [\"transaction_id\", \"product\", \"amount\", \"date\"]\n",
    "\n",
    "df_sales = spark.createDataFrame(data, columns)\n",
    "\n",
    "# Write data as Delta Table\n",
    "# We save it as a managed table in the Hive Metastore (or local warehouse)\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"sales_delta\")\n",
    "\n",
    "print(\"Delta Table 'sales_delta' created.\")\n",
    "\n",
    "# View the data\n",
    "spark.sql(\"SELECT * FROM sales_delta\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d40d7b04",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Standard Parquet tables do not support UPDATE. Delta Lake does.\n",
    "# Scenario: Update amount to 0 for transaction_id = 1\n",
    "\n",
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, \"sales_delta\")\n",
    "\n",
    "# Update using DeltaTable API\n",
    "deltaTable.update(\n",
    "    condition = \"transaction_id = 1\",\n",
    "    set = { \"amount\": \"0\" }\n",
    ")\n",
    "\n",
    "print(\"Data Updated.\")\n",
    "spark.sql(\"SELECT * FROM sales_delta ORDER BY transaction_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da09e94a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Delta Lake maintains a transaction log (JSON files in _delta_log folder).\n",
    "# We can view the history of operations.\n",
    "\n",
    "print(\"Table History:\")\n",
    "deltaTable.history().select(\"version\", \"timestamp\", \"operation\", \"operationParameters\").show(truncate=False)\n",
    "\n",
    "# Read Version 0 (Before the Update)\n",
    "print(\"Data at Version 0 (Original):\")\n",
    "df_v0 = spark.read.format(\"delta\").option(\"versionAsOf\", 0).table(\"sales_delta\")\n",
    "df_v0.show()\n",
    "\n",
    "# Read Version 1 (After the Update)\n",
    "print(\"Data at Version 1 (Current):\")\n",
    "df_v1 = spark.read.format(\"delta\").option(\"versionAsOf\", 1).table(\"sales_delta\")\n",
    "df_v1.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f728b649",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We can rollback the table to a previous state using Restore command (SQL syntax)\n",
    "# Restoring to Version 0\n",
    "\n",
    "spark.sql(\"RESTORE TABLE sales_delta TO VERSION AS OF 0\")\n",
    "\n",
    "print(\"Table Restored to Version 0.\")\n",
    "spark.sql(\"SELECT * FROM sales_delta ORDER BY transaction_id\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85dabd49",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scenario: New data arrives with an extra column 'customer_id'.\n",
    "# By default, append fails if schema doesn't match.\n",
    "# We use 'mergeSchema' option to allow evolution.\n",
    "\n",
    "new_data = [\n",
    "    (5, \"Widget D\", 300, \"2023-01-05\", 101),\n",
    "    (6, \"Widget E\", 120, \"2023-01-06\", 102)\n",
    "]\n",
    "# Note: Schema has 5 columns now\n",
    "new_columns = [\"transaction_id\", \"product\", \"amount\", \"date\", \"customer_id\"]\n",
    "\n",
    "df_new = spark.createDataFrame(new_data, new_columns)\n",
    "\n",
    "# Append with Schema Evolution\n",
    "df_new.write.format(\"delta\") \\\n",
    "    .mode(\"append\") \\\n",
    "    .option(\"mergeSchema\", \"true\") \\\n",
    "    .saveAsTable(\"sales_delta\")\n",
    "\n",
    "print(\"New Data Appended with Schema Evolution.\")\n",
    "spark.sql(\"SELECT * FROM sales_delta\").show()\n",
    "\n",
    "# Notice that older records will have NULL for 'customer_id'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f7e834e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Vacuum removes old data files that are no longer in the latest state of the table.\n",
    "# CAUTION: After running VACUUM with retention 0, you lose the ability to Time Travel \n",
    "# to versions older than the retention period.\n",
    "\n",
    "# By default, Spark prevents vacuuming files less than 168 hours (7 days) old.\n",
    "# To force it for demo, we disable the retention check.\n",
    "spark.conf.set(\"spark.databricks.delta.retentionDurationCheck.enabled\", \"false\")\n",
    "\n",
    "# Vacuum files older than 0 hours (removes all history except current state)\n",
    "deltaTable.vacuum(0)\n",
    "\n",
    "print(\"Vacuum complete. History storage cleared.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956670bb",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Delta Lake** provides ACID properties on top of cloud object storage.\n",
    "2.  **DML:** We successfully updated data in place.\n",
    "3.  **Time Travel:** We queried older versions and restored the table state.\n",
    "4.  **Schema Evolution:** We added a new column dynamically during an append operation.\n",
    "5.  **Maintenance:** `VACUUM` helps reclaim storage space but limits time travel capabilities.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will explore advanced optimization techniques for Delta Tables, including Z-Ordering and Optimize."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
