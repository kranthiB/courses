{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9db50626",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 30: Data Scanning and Partitioning Strategy\n",
    "\n",
    "In Big Data systems, I/O (Input/Output) is often the biggest bottleneck. Reading data from disk is expensive. \n",
    "**Data Scanning** refers to the process of reading data files. To optimize performance, we want to scan *only* the data we need.\n",
    "\n",
    "**Partitioning** is the technique of dividing a large dataset into smaller, manageable parts (folders) based on specific columns (e.g., Date, Country).\n",
    "\n",
    "### Agenda:\n",
    "1.  **The Problem:** Unnecessary data scanning in non-partitioned datasets.\n",
    "2.  **The Solution:** Using `partitionBy` to organize data physically.\n",
    "3.  **Partition Pruning:** How Spark automatically skips irrelevant files.\n",
    "4.  **High Cardinality:** Understanding the risks of over-partitioning (Small File Problem)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecef51b9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# Setup Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Partitioning_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Generate Synthetic Sales Data\n",
    "# We create data for 3 distinct countries\n",
    "data = [\n",
    "    (1, \"USA\", 100, \"2023-01-01\"),\n",
    "    (2, \"USA\", 150, \"2023-01-02\"),\n",
    "    (3, \"USA\", 120, \"2023-01-03\"),\n",
    "    (4, \"India\", 200, \"2023-01-01\"),\n",
    "    (5, \"India\", 250, \"2023-01-02\"),\n",
    "    (6, \"UK\", 300, \"2023-01-01\"),\n",
    "    (7, \"UK\", 350, \"2023-01-02\"),\n",
    "    (8, \"UK\", 400, \"2023-01-03\")\n",
    "]\n",
    "\n",
    "columns = [\"order_id\", \"country\", \"amount\", \"date\"]\n",
    "df = spark.createDataFrame(data, columns)\n",
    "\n",
    "print(\"Source Data:\")\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e576ff",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Write data as a standard Parquet file (Flat structure)\n",
    "# In a real distributed system, this might create multiple files, but they won't be separated by folders.\n",
    "\n",
    "output_path_flat = \"data/sales_flat\"\n",
    "df.write.mode(\"overwrite\").parquet(output_path_flat)\n",
    "\n",
    "print(f\"Data written to {output_path_flat}\")\n",
    "\n",
    "# Let's look at the file structure (using Python os command)\n",
    "print(\"\\nFile Structure (Flat):\")\n",
    "for file in os.listdir(output_path_flat):\n",
    "    if file.endswith(\".parquet\"):\n",
    "        print(f\"  - {file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4aaa2e36",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the flat data\n",
    "df_flat = spark.read.parquet(output_path_flat)\n",
    "\n",
    "# Query: Get sales only for 'India'\n",
    "india_sales_flat = df_flat.filter(\"country = 'India'\")\n",
    "\n",
    "print(\"Plan for Non-Partitioned Read:\")\n",
    "# Explain shows the physical plan. Look for 'PushedFilters'.\n",
    "# Even with PushedFilters, Spark usually has to read the File Metadata or Footer of ALL files \n",
    "# to know if 'India' exists inside them.\n",
    "india_sales_flat.explain()\n",
    "\n",
    "print(f\"Count: {india_sales_flat.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31945fe7",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 2. Write data partitioned by 'country'\n",
    "# This physically segregates data into folders based on the country column.\n",
    "\n",
    "output_path_part = \"data/sales_partitioned\"\n",
    "df.write.mode(\"overwrite\").partitionBy(\"country\").parquet(output_path_part)\n",
    "\n",
    "print(f\"Data written to {output_path_part}\")\n",
    "\n",
    "# Let's look at the file structure (Partitioned)\n",
    "# Notice the sub-folders like 'country=India', 'country=USA'\n",
    "print(\"\\nFile Structure (Partitioned):\")\n",
    "for folder in os.listdir(output_path_part):\n",
    "    if not folder.startswith(\".\") and not folder.startswith(\"_\"):\n",
    "        print(f\"  - {folder}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b47f87f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Reading the partitioned data\n",
    "# Spark discovers the 'country' column from the directory structure automatically.\n",
    "df_part = spark.read.parquet(output_path_part)\n",
    "\n",
    "# Query: Get sales only for 'India'\n",
    "india_sales_part = df_part.filter(\"country = 'India'\")\n",
    "\n",
    "print(\"Plan for Partitioned Read:\")\n",
    "# Look for 'PartitionFilters' in the explain plan.\n",
    "# Spark realizes it only needs to look into the 'country=India' folder.\n",
    "# It completely SKIPS scanning folders for USA and UK.\n",
    "india_sales_part.explain()\n",
    "\n",
    "print(f\"Count: {india_sales_part.count()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ff4059b",
   "metadata": {},
   "source": [
    "## The Risk: High Cardinality Columns\n",
    "\n",
    "**Cardinality** refers to the number of unique values in a column.\n",
    "*   **Low Cardinality:** Country, Region, Status (Active/Inactive) -> **Good for Partitioning**.\n",
    "*   **High Cardinality:** Order ID, User ID, Timestamp, Transaction ID -> **Bad for Partitioning**.\n",
    "\n",
    "**Why is High Cardinality bad?**\n",
    "If you partition by `order_id` (unique for every row), Spark will create a separate folder and a separate small file for *every single row*.\n",
    "1.  **Small File Problem:** Reading thousands of tiny files is much slower than reading one large file due to metadata overhead.\n",
    "2.  **NameNode Pressure:** In Hadoop/HDFS, this can crash the NameNode.\n",
    "3.  **Listing Overhead:** Just listing the files to start the job takes a long time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfcfdb1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning up the generated data\n",
    "try:\n",
    "    shutil.rmtree(\"data/sales_flat\")\n",
    "    shutil.rmtree(\"data/sales_partitioned\")\n",
    "    print(\"Cleanup successful.\")\n",
    "except Exception as e:\n",
    "    print(f\"Cleanup failed: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93a3dc07",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Data Scanning:** To make queries fast, minimize the amount of data scanned.\n",
    "2.  **Partitioning:** Physically groups data into directories (`key=value`).\n",
    "3.  **Partition Pruning:** When you filter on a partitioned column (e.g., `WHERE country='India'`), Spark skips scanning directories that don't match. This provides massive performance gains.\n",
    "4.  **Best Practice:** \n",
    "    *   Partition by columns commonly used in `WHERE` clauses.\n",
    "    *   Ensure columns have **Low Cardinality** (e.g., Date, Region).\n",
    "    *   Avoid partitioning by unique IDs or high cardinality columns.\n",
    "\n",
    "**Next Steps:**\n",
    "In the next module, we will look at **Z-Ordering**, an advanced optimization technique used in Delta Lake to speed up queries on columns that have high cardinality where partitioning isn't suitable."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
