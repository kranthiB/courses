{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ec0d9767",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 31: Data Skipping and Z-Ordering\n",
    "\n",
    "In the previous module, we learned about **Partitioning** to skip huge chunks of data (directories). However, partitioning has a limitation: it works best for low-cardinality columns (like Country, Date). \n",
    "\n",
    "**What if we need to filter by a High-Cardinality column (e.g., UserID, OrderID, Timestamp)?**\n",
    "Creating millions of partitions (folders) for each OrderID is known as the **Small File Problem** and will crash the system.\n",
    "\n",
    "**The Solution: Z-Ordering (Data Skipping)**\n",
    "Z-Ordering is a technique to co-locate related information in the same set of files. Delta Lake automatically collects statistics (min/max values) for the Z-Ordered columns, allowing the engine to skip individual files within a partition that do not contain the data.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Partitioning vs. Z-Ordering:** When to use which.\n",
    "2.  **Setup:** Generate synthetic sales data.\n",
    "3.  **The Problem:** Scanning files for high-cardinality lookups.\n",
    "4.  **The Solution:** Applying `OPTIMIZE ... ZORDER BY`.\n",
    "5.  **Converting Parquet to Delta:** Using `convertToDelta`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbbc26d4",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, rand, lit\n",
    "from delta.tables import *\n",
    "import shutil\n",
    "\n",
    "# Setup Spark with Delta Lake\n",
    "builder = SparkSession.builder \\\n",
    "    .appName(\"Delta_Optimization_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .config(\"spark.jars.packages\", \"io.delta:delta-core_2.12:2.4.0\") \n",
    "\n",
    "spark = configure_spark_with_delta_pip(builder).getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created with Delta Support\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85f363",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We need enough data to generate multiple files to demonstrate skipping.\n",
    "# We will generate 1 Million rows.\n",
    "\n",
    "print(\"Generating Data...\")\n",
    "# InvoiceNo will be high cardinality (almost unique)\n",
    "# Country will be low cardinality (good for partitioning)\n",
    "\n",
    "data = spark.range(0, 1000000).withColumnRenamed(\"id\", \"InvoiceNo\") \\\n",
    "    .withColumn(\"Country\", \\\n",
    "                (rand() * 5).cast(\"int\")) \\\n",
    "    .withColumn(\"Country\", \\\n",
    "                # Map random ints to Country codes\n",
    "                lit(\"USA\").when(col(\"Country\") == 0, \"USA\") \\\n",
    "                .when(col(\"Country\") == 1, \"India\") \\\n",
    "                .when(col(\"Country\") == 2, \"UK\") \\\n",
    "                .when(col(\"Country\") == 3, \"Canada\") \\\n",
    "                .otherwise(\"Australia\")) \\\n",
    "    .withColumn(\"Amount\", (rand() * 1000).cast(\"int\"))\n",
    "\n",
    "# Repartition to simulate a real scenario with many small files initially\n",
    "df_sales = data.repartition(50)\n",
    "\n",
    "print(\"Data Generation Complete.\")\n",
    "df_sales.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff6c5b3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We partition by Country because it has low cardinality (5 unique values)\n",
    "# We DO NOT partition by InvoiceNo because it has 1M unique values.\n",
    "\n",
    "delta_path = \"data/delta_sales_zorder\"\n",
    "\n",
    "# Clean up if exists\n",
    "shutil.rmtree(delta_path, ignore_errors=True)\n",
    "\n",
    "print(\"Writing Partitioned Delta Table...\")\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"Country\") \\\n",
    "    .save(delta_path)\n",
    "\n",
    "print(\"Write Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f7a016",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's try to search for a specific InvoiceNo.\n",
    "# Without Z-Ordering, Spark might have to scan ALL files in the specific Country partition \n",
    "# because it doesn't know which file contains InvoiceNo=50000.\n",
    "\n",
    "target_invoice = 500000\n",
    "\n",
    "print(\"Querying without Z-Order:\")\n",
    "# We filter by Country (Partition Pruning happens here) AND InvoiceNo\n",
    "df_read = spark.read.format(\"delta\").load(delta_path)\n",
    "\n",
    "# NOTE: In a real cluster UI, you would see the number of files scanned.\n",
    "# Here we check the physical plan.\n",
    "df_filtered = df_read.filter((col(\"Country\") == \"India\") & (col(\"InvoiceNo\") == target_invoice))\n",
    "df_filtered.explain()\n",
    "df_filtered.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a934580",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Z-Ordering co-locates data. It sorts data by InvoiceNo within each partition (Country) \n",
    "# and rewrites the files.\n",
    "# This updates the Delta Log (min/max stats) for InvoiceNo for each file.\n",
    "\n",
    "deltaTable = DeltaTable.forPath(spark, delta_path)\n",
    "\n",
    "print(\"Running OPTIMIZE with ZORDER BY InvoiceNo...\")\n",
    "\n",
    "# Python API for Z-Order\n",
    "deltaTable.optimize().executeZOrderBy(\"InvoiceNo\")\n",
    "\n",
    "# SQL Equivalent: \n",
    "# spark.sql(f\"OPTIMIZE '{delta_path}' ZORDER BY (InvoiceNo)\")\n",
    "\n",
    "print(\"Optimization Complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6dc3f35",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Now when we run the same query, Spark checks the Delta Log first.\n",
    "# It looks at the min/max InvoiceNo for files inside 'Country=India'.\n",
    "# It skips files where 500000 is not in the [min, max] range.\n",
    "\n",
    "print(\"Querying WITH Z-Order:\")\n",
    "df_optimized = spark.read.format(\"delta\").load(delta_path)\n",
    "df_optimized_filter = df_optimized.filter((col(\"Country\") == \"India\") & (col(\"InvoiceNo\") == target_invoice))\n",
    "\n",
    "df_optimized_filter.show()\n",
    "\n",
    "# Check History to see the OPTIMIZE operation\n",
    "deltaTable.history().select(\"version\", \"operation\", \"operationParameters\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "020c27dd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Bonus: If you have an existing Parquet Data Lake, you don't need to rewrite everything.\n",
    "# You can convert it in-place to Delta Lake.\n",
    "\n",
    "# 1. Write dummy Parquet data\n",
    "parquet_path = \"data/legacy_parquet\"\n",
    "df_sales.write.mode(\"overwrite\").parquet(parquet_path)\n",
    "\n",
    "print(\"Parquet Table Created.\")\n",
    "\n",
    "# 2. Convert to Delta\n",
    "# This commands indexes the files and creates the _delta_log directory\n",
    "dt = DeltaTable.convertToDelta(spark, f\"parquet.`{parquet_path}`\")\n",
    "\n",
    "print(f\"Converted {parquet_path} to Delta Table.\")\n",
    "print(\"Is Delta Table?\", DeltaTable.isDeltaTable(spark, parquet_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28ba7a04",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Partitioning:** Best for low cardinality columns (Country, Date). Physical separation of folders.\n",
    "2.  **Z-Ordering:** Best for high cardinality columns (ID, Timestamp) used frequently in filters. It sorts data within files to maximize **Data Skipping**.\n",
    "3.  **Optimize:** The `OPTIMIZE` command compacts small files (Bin-packing) and performs Z-Ordering if specified.\n",
    "4.  **Convert:** Use `ConvertToDelta` to migrate existing data lakes without moving data."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
