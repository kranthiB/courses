{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c9b80636",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 32: Deletion Vectors and Liquid Clustering\n",
    "\n",
    "In this module, we explore two advanced optimization features in Delta Lake:\n",
    "\n",
    "1.  **Deletion Vectors:** A storage optimization feature that speeds up `DELETE` and `UPDATE` operations by avoiding rewriting entire Parquet files. Instead, it marks rows as deleted in a separate vector file.\n",
    "2.  **Liquid Clustering:** An intelligent data layout technique that replaces traditional Partitioning and Z-Ordering. It adapts to data skew and query patterns automatically.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Setup:** Prepare the environment and dataset.\n",
    "2.  **Deletion Vectors:** \n",
    "    *   Enable the feature on a Delta table.\n",
    "    *   Perform DELETE/UPDATE operations.\n",
    "    *   Observe how files are managed (rewrites vs. markers).\n",
    "3.  **Liquid Clustering:**\n",
    "    *   Create a table with Liquid Clustering enabled (`CLUSTER BY`).\n",
    "    *   Perform operations and observe automatic layout optimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95beeb3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This notebook is best run on Databricks Runtime 13.3 LTS or higher \n",
    "# to support Liquid Clustering fully.\n",
    "\n",
    "# If running locally, ensure you have Delta Lake 3.1.0+ configured.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create Spark Session (if not already available in Databricks)\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Delta_Advanced_Features\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\") \\\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e1a1acd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We will use a sample sales dataset similar to previous modules\n",
    "data = [\n",
    "    (101, \"Product A\", 100, \"2024-01-01\"),\n",
    "    (102, \"Product B\", 200, \"2024-01-01\"),\n",
    "    (103, \"Product A\", 150, \"2024-01-02\"),\n",
    "    (104, \"Product C\", 300, \"2024-01-02\"),\n",
    "    (105, \"Product B\", 250, \"2024-01-03\")\n",
    "]\n",
    "columns = [\"invoice_id\", \"product\", \"amount\", \"date\"]\n",
    "\n",
    "df_sales = spark.createDataFrame(data, columns)\n",
    "df_sales.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "984b4eae",
   "metadata": {},
   "source": [
    "## 1. Deletion Vectors\n",
    "\n",
    "Traditionally, when you delete a single row from a Parquet file in a Delta table, Delta Lake has to rewrite the **entire** Parquet file without that row. This is expensive for large files.\n",
    "\n",
    "With **Deletion Vectors**, Delta writes a small \"deletion vector\" file indicating which rows are invalid. The original data file remains untouched. This makes deletes and updates significantly faster."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b184f345",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Create a standard Delta Table first\n",
    "table_name = \"sales_dv_demo\"\n",
    "df_sales.write.format(\"delta\").mode(\"overwrite\").saveAsTable(table_name)\n",
    "\n",
    "# Enable Deletion Vectors property on the table\n",
    "# Note: Once enabled, tables might not be readable by older Delta versions.\n",
    "spark.sql(f\"\"\"\n",
    "    ALTER TABLE {table_name} \n",
    "    SET TBLPROPERTIES ('delta.enableDeletionVectors' = true)\n",
    "\"\"\")\n",
    "\n",
    "print(f\"Deletion Vectors enabled for {table_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0700213",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "\n",
    "deltaTable = DeltaTable.forName(spark, table_name)\n",
    "\n",
    "# Delete a specific invoice\n",
    "print(\"Deleting invoice_id = 101...\")\n",
    "deltaTable.delete(\"invoice_id = 101\")\n",
    "\n",
    "# Check History\n",
    "# In the operation metrics, look for 'numDeletedRows' and file operations.\n",
    "# With DV enabled, you might see fewer bytes written compared to a full rewrite.\n",
    "history = deltaTable.history().select(\"version\", \"operation\", \"operationMetrics\")\n",
    "display(history) # Use history.show(truncate=False) in local pyspark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ed1132e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Verify the row is gone\n",
    "spark.sql(f\"SELECT * FROM {table_name}\").show()\n",
    "\n",
    "# Note: The physical parquet file for the deleted row still exists.\n",
    "# The deletion vector tells Spark to ignore that row during reads.\n",
    "# Running 'OPTIMIZE' or 'VACUUM' later will eventually compact and remove old files."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dfad697",
   "metadata": {},
   "source": [
    "## 2. Liquid Clustering\n",
    "\n",
    "Liquid Clustering simplifies data layout. Instead of deciding between `PARTITION BY` (physical folders) and `ZORDER BY` (file sorting), you simply use `CLUSTER BY`.\n",
    "\n",
    "Delta Lake manages the physical layout dynamically, clustering data based on the columns you specify. It avoids the \"small file problem\" of over-partitioning and the need for manual `OPTIMIZE ZORDER BY` runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ec2642",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "liquid_table_name = \"sales_liquid_demo\"\n",
    "\n",
    "# Syntax: CLUSTER BY (col1, col2...)\n",
    "# We will cluster by 'product' and 'date'\n",
    "df_sales.write.format(\"delta\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .clusterBy(\"product\", \"date\") \\\n",
    "    .saveAsTable(liquid_table_name)\n",
    "\n",
    "print(f\"Table '{liquid_table_name}' created with Liquid Clustering.\")\n",
    "\n",
    "# Describe table to confirm clustering\n",
    "spark.sql(f\"DESCRIBE EXTENDED {liquid_table_name}\").show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "123a9c7a",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Liquid clustering works best when data grows. \n",
    "# Let's append more data.\n",
    "new_data = [\n",
    "    (106, \"Product A\", 120, \"2024-01-04\"),\n",
    "    (107, \"Product C\", 310, \"2024-01-04\")\n",
    "]\n",
    "df_new = spark.createDataFrame(new_data, columns)\n",
    "\n",
    "df_new.write.format(\"delta\").mode(\"append\").saveAsTable(liquid_table_name)\n",
    "\n",
    "# Run OPTIMIZE\n",
    "# With Liquid Clustering, OPTIMIZE will automatically cluster data based on the keys provided.\n",
    "# You don't need to specify ZORDER BY.\n",
    "print(\"Running Optimization...\")\n",
    "spark.sql(f\"OPTIMIZE {liquid_table_name}\")\n",
    "\n",
    "print(\"Optimization Complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7de6300",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Deletion Vectors:**\n",
    "    *   **Benefit:** Faster DELETE/UPDATE/MERGE operations.\n",
    "    *   **Mechanism:** Writes markers instead of rewriting full files.\n",
    "    *   **Usage:** Enable via TBLPROPERTIES.\n",
    "\n",
    "2.  **Liquid Clustering:**\n",
    "    *   **Benefit:** Solves the partition cardinality problem and Z-Order maintenance.\n",
    "    *   **Mechanism:** Flexible, dynamic data layout managed by Delta.\n",
    "    *   **Usage:** Use `CLUSTER BY` during table creation. `OPTIMIZE` maintains the layout.\n",
    "\n",
    "**Note:** Liquid Clustering is the recommended strategy for most new Delta tables in Databricks."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
