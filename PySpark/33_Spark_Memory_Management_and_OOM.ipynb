{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "abc3386d",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 33: Spark Memory Management and OOM Analysis\n",
    "\n",
    "In this module, we dive deep into **Spark Memory Management**. Understanding how Spark manages memory inside the JVM is crucial for tuning applications and avoiding the dreaded **Out Of Memory (OOM)** errors.\n",
    "\n",
    "We will simulate a constrained environment (small executors) to mathematically verify Spark's memory division and intentionally trigger OOM errors to understand their root causes.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Memory Theory:** Understand On-Heap, Off-Heap, Reserved, User, and Unified Memory.\n",
    "2.  **Environment Setup:** Create a Spark Session with constrained memory (512MB per executor).\n",
    "3.  **Memory Calculation:** Manually calculate available Storage/Execution memory and verify against Spark UI.\n",
    "4.  **OOM Simulation 1:** Data Explosion using `explode()` on strings.\n",
    "5.  **OOM Simulation 2:** Handling large single-record files (Skew)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2071d91e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, lower, lit, count\n",
    "\n",
    "# We configure a specific amount of memory to validate calculations easily.\n",
    "# Executor Memory: 512MB\n",
    "# Executor Cores: 4\n",
    "# Note: In a local environment, 'spark.executor.memory' might not strictly apply \n",
    "# the same way as a cluster (YARN/K8s), but it helps simulate the limits.\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark_Memory_Management_OOM\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.executor.memory\", \"512MB\") \\\n",
    "    .config(\"spark.executor.cores\", \"4\") \\\n",
    "    .config(\"spark.memory.fraction\", \"0.6\") \\\n",
    "    .config(\"spark.memory.storageFraction\", \"0.5\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"false\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created with constrained memory.\")\n",
    "print(f\"Spark UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4846583",
   "metadata": {},
   "source": [
    "## 1. Spark Memory Architecture\n",
    "\n",
    "When you request `512MB` for an executor, you do not get the full 512MB for data storage and execution. The JVM and Spark partition this memory as follows:\n",
    "\n",
    "1.  **JVM Heap:** The total memory requested (`512MB`).\n",
    "2.  **Reserved Memory:** Spark reserves **300MB** for internal metadata and failure recovery. This is hardcoded.\n",
    "3.  **Usable Memory:** `(JVM Heap - Reserved Memory)`.\n",
    "    *   *Note:* The JVM itself has overhead. Spark typically considers only ~89-90% of the heap as \"safe\" to use before calculating the 300MB deduction, though exact behavior depends on the Spark version and deploy mode.\n",
    "\n",
    "### The Unified Region (60/40 Split)\n",
    "The remaining usable memory is split based on `spark.memory.fraction` (default 0.6):\n",
    "1.  **Spark Memory (60%):** Unified region for **Storage** (cached data) and **Execution** (shuffles, joins, sorts).\n",
    "2.  **User Memory (40%):** For your UDFs, RDD metadata, and custom data structures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "484a6d9d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's perform the calculation based on standard Spark Memory Management logic.\n",
    "\n",
    "executor_memory_mb = 512\n",
    "\n",
    "# 1. JVM Usable (Spark estimates ~89-90% of heap is actually available for its regions \n",
    "# due to object headers/GC overhead).\n",
    "jvm_usable_fraction = 0.89  \n",
    "jvm_usable_memory = executor_memory_mb * jvm_usable_fraction\n",
    "print(f\"JVM Usable Memory (~89%): {jvm_usable_memory:.2f} MB\")\n",
    "\n",
    "# 2. Subtract Reserved Memory\n",
    "reserved_memory = 300\n",
    "usable_memory = jvm_usable_memory - reserved_memory\n",
    "print(f\"Usable Memory after Reserve: {usable_memory:.2f} MB\")\n",
    "\n",
    "# 3. Spark Unified Memory (Default 60% of Usable)\n",
    "# Config: spark.memory.fraction = 0.6\n",
    "spark_memory_fraction = 0.6\n",
    "unified_memory = usable_memory * spark_memory_fraction\n",
    "\n",
    "print(f\"Total Unified Spark Memory (Storage + Execution): {unified_memory:.2f} MB\")\n",
    "\n",
    "# 4. User Memory (Remaining 40%)\n",
    "user_memory = usable_memory * (1 - spark_memory_fraction)\n",
    "print(f\"User Memory: {user_memory:.2f} MB\")\n",
    "\n",
    "# 5. Storage Memory (Default 50% of Unified, immune to eviction)\n",
    "storage_pool = unified_memory * 0.5\n",
    "print(f\"Protected Storage Memory: {storage_pool:.2f} MB\")\n",
    "\n",
    "# Note: Check the 'Executors' tab in Spark UI to see the actual 'Storage Memory' column.\n",
    "# It should match the 'Total Unified Spark Memory' calculation closely (~93 MB)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddc765d",
   "metadata": {},
   "source": [
    "## 2. OOM Simulation: The `explode` Operation\n",
    "\n",
    "One common cause of OOM is **Data Explosion**. A transformation like `explode` creates a new row for every element in an array.\n",
    "\n",
    "If you have a dataset where a single row contains a massive array (or you generate one via `crossJoin` or string splitting), the resulting data volume can instantly exceed the **Execution Memory**, causing a spill to disk or an OOM crash if the serialized object is too large."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14b18941",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We will create a dummy dataset or read a text file.\n",
    "# For this demo, let's assume we have a text file with some content.\n",
    "# If running locally without the file, we can generate a small DataFrame.\n",
    "\n",
    "data = [(\"This is a sample line of text for demonstration purposes\",)]\n",
    "df_text = spark.createDataFrame(data, [\"value\"])\n",
    "\n",
    "# Let's verify schema\n",
    "df_text.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74c321c1",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Logic:\n",
    "# 1. Split the sentence into words (Array).\n",
    "# 2. EXPLODE the array (Rows multiply).\n",
    "# 3. Group and Count.\n",
    "\n",
    "# To simulate OOM, imagine the line is massive or we duplicate the string many times.\n",
    "# Here is the logic flow that usually consumes Execution Memory.\n",
    "\n",
    "df_split = df_text.withColumn(\"splitted_val\", split(\"value\", \" \"))\n",
    "    \n",
    "df_exploded = df_split.withColumn(\"exploded_val\", explode(\"splitted_val\")) \\\n",
    "    .drop(\"value\", \"splitted_val\")\n",
    "\n",
    "df_count = df_exploded.groupBy(\"exploded_val\").count()\n",
    "\n",
    "print(\"Plan created. Running action...\")\n",
    "df_count.show()\n",
    "\n",
    "# Note: If the source string was huge (MBs size) and explode created millions of rows\n",
    "# inside a single partition, this would crash the tiny 512MB executor."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27e3ac5a",
   "metadata": {},
   "source": [
    "## 3. OOM Simulation: Large Single Records\n",
    "\n",
    "Spark processes data partition by partition. However, to process a single row, that entire row must fit into memory. \n",
    "\n",
    "If you read a file format like JSON or Text where a **single record** (e.g., a single line of text without newlines) is larger than the available Execution Memory (e.g., > 100MB in our constrained setup), Spark cannot spill that single record to disk. It must exist in memory to be deserialized.\n",
    "\n",
    "**Result:** Immediate `java.lang.OutOfMemoryError: Java heap space`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1d72d8c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Simulating the reading of a \"Single Line\" file vs \"Multi Line\" file.\n",
    "\n",
    "# Case A: Reading a file where the entire content is on one line.\n",
    "# If 'text_file_singleline_xs.txt' is > 50MB (approx our execution memory per core),\n",
    "# this operation will fail.\n",
    "\n",
    "# Uncomment to run if file exists\n",
    "# df_single = spark.read.text(\"path/to/text_file_singleline_xs.txt\")\n",
    "# df_single.show() \n",
    "# ^ This triggers OOM because the single row object > available memory.\n",
    "\n",
    "print(\"To avoid OOM with large single records:\")\n",
    "print(\"1. Increase Executor Memory (Vertical Scaling).\")\n",
    "print(\"2. Pre-process file to split lines.\")\n",
    "print(\"3. Use 'wholetext' option with caution (reads file as one record - risky).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd323d89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Case B: Reading the same data but formatted with newlines (Multi-line).\n",
    "# Each record is small. Spark can process them one by one or batch them safely.\n",
    "\n",
    "# df_multi = spark.read.text(\"path/to/text_file_xs.txt\")\n",
    "# df_multi.select(split(lower(\"value\"), \" \").alias(\"words\")) \\\n",
    "#     .select(explode(\"words\").alias(\"word\")) \\\n",
    "#     .groupBy(\"word\").count() \\\n",
    "#     .write.format(\"noop\").mode(\"overwrite\").save()\n",
    "\n",
    "# Using 'noop' format is great for benchmarking (No Operation write).\n",
    "# It forces the execution without writing output files.\n",
    "\n",
    "print(\"Benchmarking execution complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f699b12",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Memory Division:**\n",
    "    *   **Reserved:** 300MB (fixed).\n",
    "    *   **Spark Fraction:** 60% of (Heap - Reserved).\n",
    "    *   **Storage/Execution:** Share this 60% unified pool. Execution can evict Storage, but Storage cannot evict Execution.\n",
    "\n",
    "2.  **OOM Causes:**\n",
    "    *   **Data Explosion:** `explode`, `crossJoin` producing more data than fits in Execution memory.\n",
    "    *   **Skew/Big Records:** A single record larger than the memory available to a single task/core.\n",
    "    *   **GC Overhead:** If GC spends too much time clearing space (98% time) and recovers little memory (<2%), Spark throws `GC overhead limit exceeded`.\n",
    "\n",
    "3.  **Solutions:**\n",
    "    *   Tune `spark.memory.fraction` (rarely needed).\n",
    "    *   Increase Executor Memory.\n",
    "    *   Fix Code Logic (avoid massive single records, handle skew).\n",
    "    *   Use **Off-Heap Memory** (managed by OS, requires explicit config) to reduce GC pressure for caching."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
