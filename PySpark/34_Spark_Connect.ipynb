{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "aa050bc5",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 34: What is Spark Connect?\n",
    "\n",
    "**Spark Connect** (introduced in Spark 3.4) is a client-server architecture that decouples the client (where you write code) from the Spark driver (where code is planned). It allows you to connect to a Spark cluster remotely from any language (Python, Go, Rust) or IDE (VS Code, PyCharm) using a thin client via the **gRPC** protocol.\n",
    "\n",
    "### Key Benefits:\n",
    "1.  **Decoupling:** Upgrading the Spark cluster doesn't require upgrading the client immediately.\n",
    "2.  **Remote Connectivity:** Run Spark code from your local laptop/IDE against a remote production cluster easily.\n",
    "3.  **Stability:** Client crashes don't take down the cluster; Cluster restarts don't necessarily crash the client (until execution).\n",
    "\n",
    "### Agenda:\n",
    "1.  **Architecture:** Understand how Spark Connect works (gRPC, Arrow).\n",
    "2.  **Setup:** Prerequisites for running Spark Connect (Server & Client).\n",
    "3.  **Execution:** Connecting to a remote Spark server using `remote()`.\n",
    "4.  **Comparison:** Spark Session vs. Spark Connect Session.\n",
    "5.  **Limitations:** Why RDDs don't work in Spark Connect."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f2c6a6",
   "metadata": {},
   "source": [
    "## 1. Architecture and Setup\n",
    "\n",
    "Traditionally, the Spark Driver and Client were tightly coupled in the same JVM process (or closely linked). With Spark Connect:\n",
    "*   **Client:** Translates DataFrame operations into logic plans and sends them over **gRPC**.\n",
    "*   **Server:** Receives plans, executes them, and streams results back as **Apache Arrow** batches.\n",
    "\n",
    "### Prerequisites (Based on Video Demo)\n",
    "To run this notebook successfully, you need a running **Spark Connect Server**. \n",
    "\n",
    "**Server Side (Docker):**\n",
    "If you are following the course Docker setup:\n",
    "1.  Use the image `pyspark-cluster-3.5.5` (Spark 3.5+ is recommended for Connect).\n",
    "2.  Start the container. The Spark Connect Server usually listens on port `15002`.\n",
    "\n",
    "**Client Side (Libraries):**\n",
    "You need to install specific dependencies to use the thin client:\n",
    "```bash\n",
    "pip install pyspark==3.5.5 pandas pyarrow grpcio grpcio-status protobuf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a12c7b44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Import SparkSession\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# In a standard local run, we usually do .master(\"local\").getOrCreate()\n",
    "# For Spark Connect, we use the .remote() option.\n",
    "\n",
    "# Connection String Format: sc://<host>:<port>\n",
    "# Default Spark Connect Port is 15002\n",
    "connection_string = \"sc://localhost:15002\"\n",
    "\n",
    "try:\n",
    "    spark = SparkSession.builder \\\n",
    "        .remote(connection_string) \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    print(\"Spark Connect Session created successfully!\")\n",
    "    print(spark)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(\"Could not connect to Spark Connect Server. Ensure the Docker container is running.\")\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "# Note: If you see 'pyspark.sql.connect.session.SparkSession', you are using the Client API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165c9272",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's verify the type of our session object.\n",
    "# Standard: pyspark.sql.session.SparkSession\n",
    "# Connect:  pyspark.sql.connect.session.SparkSession\n",
    "\n",
    "print(f\"Session Type: {type(spark)}\")\n",
    "\n",
    "# This object is a \"Thin Client\". It does not contain the heavy JVM driver logic locally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad9fbae0",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Spark Connect supports the standard DataFrame API.\n",
    "# The code looks exactly the same as standard PySpark.\n",
    "\n",
    "# 1. Create a Range DataFrame\n",
    "df = spark.range(10)\n",
    "\n",
    "# 2. Perform Transformations\n",
    "df_modified = df.withColumn(\"value_squared\", df[\"id\"] * df[\"id\"])\n",
    "\n",
    "# 3. Action (Trigger Execution on Remote Server)\n",
    "# The plan is sent via gRPC, executed remotely, and results streamed back.\n",
    "print(\"Executing Dataframe Action...\")\n",
    "df_modified.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "846cdb01",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Unlike standard Spark, the Client doesn't host the UI on localhost:4040 directly.\n",
    "# The UI lives on the Spark Server side.\n",
    "\n",
    "# If you check the Spark Master UI (usually localhost:8080 in the Docker setup),\n",
    "# you will see an application named \"Spark Connect Server\".\n",
    "# All queries run by this client appear under that application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4171f491",
   "metadata": {},
   "source": [
    "## 2. Limitations: The RDD API\n",
    "\n",
    "One major difference with Spark Connect is that it **does not support RDDs (Resilient Distributed Datasets)**.\n",
    "\n",
    "Because RDDs contain arbitrary Python/Java code (lambdas) that are hard to serialize and send over gRPC in a language-agnostic way, Spark Connect focuses purely on the **DataFrame/Dataset API**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee14d6b6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    # Attempting to access the underlying RDD will fail in Spark Connect\n",
    "    rdd = df.rdd\n",
    "    print(rdd.getNumPartitions())\n",
    "except Exception as e:\n",
    "    print(\"Caught Expected Error:\")\n",
    "    print(e)\n",
    "\n",
    "# You should see an error like \"NotImplementedError\" or \"AttributeError\".\n",
    "# This confirms we are using the decoupled architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b7776b2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# For comparison, here is how a standard session looks (if you wanted to run RDDs locally).\n",
    "# This creates a JVM process locally on your machine.\n",
    "\n",
    "from pyspark.sql import SparkSession as StandardSession\n",
    "\n",
    "spark_local = StandardSession.builder \\\n",
    "    .appName(\"Standard_Session\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Standard Session Type: {type(spark_local)}\")\n",
    "\n",
    "# RDDs work here\n",
    "print(f\"RDD Partition Count: {spark_local.range(10).rdd.getNumPartitions()}\")\n",
    "\n",
    "spark_local.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37dc093f",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Spark Connect** decouples Client and Server.\n",
    "2.  **Connectivity:** Use `SparkSession.builder.remote(\"sc://host:port\")`.\n",
    "3.  **Port:** Default is `15002`.\n",
    "4.  **API Support:** Full DataFrame/SQL support. **No RDD support**.\n",
    "5.  **Use Case:** Ideal for modern data stacks, connecting IDEs (VS Code/Jupyter) to remote clusters, and building lightweight data apps."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
