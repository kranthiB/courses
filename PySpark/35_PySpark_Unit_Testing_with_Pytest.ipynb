{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d17541ad",
   "metadata": {},
   "source": [
    "# PySpark: Zero to Hero\n",
    "## Module 35: Unit Testing with Pytest\n",
    "\n",
    "Writing unit tests is critical for production-grade Data Engineering. It ensures your transformation logic works as expected and prevents regressions when code changes.\n",
    "\n",
    "In this module, we will use the **`pytest`** framework to test PySpark transformations.\n",
    "\n",
    "### Agenda:\n",
    "1.  **Setup:** Install `pytest`.\n",
    "2.  **Project Structure:** Create a standard testing directory structure.\n",
    "3.  **Fixtures (`conftest.py`):** Create a reusable `SparkSession` for tests.\n",
    "4.  **Code Module (`common.py`):** Define the PySpark transformations to test.\n",
    "5.  **Test Cases:** Write positive and negative test assertions.\n",
    "6.  **Execution:** Run the tests and analyze the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44e4ae0b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Install pytest if not already installed\n",
    "%pip install pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b4277e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Create a directory named 'tests' to hold our code and tests\n",
    "os.makedirs(\"tests\", exist_ok=True)\n",
    "print(\"Created 'tests' directory.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bef3a95c",
   "metadata": {},
   "source": [
    "We use `conftest.py` to define **Fixtures**. Fixtures are setup functions that run before tests. \n",
    "Here, we define a `spark_session` fixture with `scope=\"session\"`, meaning the SparkSession is created once and reused across all tests, which saves time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f44fb6",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/conftest.py\n",
    "import pytest\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "@pytest.fixture(scope=\"session\")\n",
    "def spark_session():\n",
    "    \"\"\"\n",
    "    Fixture to create a SparkSession for testing.\n",
    "    The scope='session' ensures it is created once per test run.\n",
    "    \"\"\"\n",
    "    spark = SparkSession.builder \\\n",
    "        .master(\"local[1]\") \\\n",
    "        .appName(\"PySpark Unit Test\") \\\n",
    "        .getOrCreate()\n",
    "    \n",
    "    yield spark\n",
    "    \n",
    "    # Teardown (optional, but good practice)\n",
    "    spark.stop()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76e2e7b2",
   "metadata": {},
   "source": [
    "We create a file `common.py` that contains the actual transformation functions we want to test.\n",
    "1.  `remove_extra_spaces`: Cleans up whitespace in a column.\n",
    "2.  `filter_senior_citizen`: Filters rows where age >= 60."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8bd7411",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/common.py\n",
    "from pyspark.sql.functions import col, regexp_replace\n",
    "\n",
    "def remove_extra_spaces(df, column_name):\n",
    "    \"\"\"\n",
    "    Removes extra spaces from the specified column.\n",
    "    Replaces 2 or more spaces with a single space.\n",
    "    \"\"\"\n",
    "    # Regex pattern \"\\\\s+\" looks for one or more whitespace characters\n",
    "    # We want to normalize multiple spaces to single space, \n",
    "    # but here we strictly follow the video logic: replace multiple spaces with single space.\n",
    "    df_transformed = df.withColumn(column_name, regexp_replace(col(column_name), \"\\\\s+\", \" \"))\n",
    "    return df_transformed\n",
    "\n",
    "def filter_senior_citizen(df, column_name):\n",
    "    \"\"\"\n",
    "    Filters dataframe for rows where column_name >= 60.\n",
    "    \"\"\"\n",
    "    df_filtered = df.filter(col(column_name) >= 60)\n",
    "    return df_filtered"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b69db8f",
   "metadata": {},
   "source": [
    "Now we write the actual tests. Note that test files must start with `test_` for pytest to discover them.\n",
    "We inject the `spark_session` fixture into our test functions automatically by name."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e908bfaf",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/test_app.py\n",
    "import pytest\n",
    "from common import remove_extra_spaces, filter_senior_citizen\n",
    "# Note: In a real package structure, imports might look different. \n",
    "# Since we are running pytest from the parent dir or same dir, this simple import works.\n",
    "\n",
    "def test_single_space(spark_session):\n",
    "    \"\"\"Test Case 1: Verify extra spaces are removed.\"\"\"\n",
    "    \n",
    "    # 1. Prepare Sample Data\n",
    "    data = [(\"John  D.\", 30), (\"Alice   G.\", 25), (\"Bob T.\", 35)]\n",
    "    columns = [\"name\", \"age\"]\n",
    "    original_df = spark_session.createDataFrame(data, columns)\n",
    "    \n",
    "    # 2. Apply Transformation\n",
    "    transformed_df = remove_extra_spaces(original_df, \"name\")\n",
    "    \n",
    "    # 3. Prepare Expected Data\n",
    "    expected_data = [(\"John D.\", 30), (\"Alice G.\", 25), (\"Bob T.\", 35)]\n",
    "    expected_df = spark_session.createDataFrame(expected_data, columns)\n",
    "    \n",
    "    # 4. Assert\n",
    "    # Comparing schema and data\n",
    "    assert transformed_df.schema == expected_df.schema\n",
    "    # Collect data to compare lists of Row objects\n",
    "    assert transformed_df.collect() == expected_df.collect()\n",
    "\n",
    "def test_row_count(spark_session):\n",
    "    \"\"\"Test Case 2: Verify row count remains same after cleaning spaces.\"\"\"\n",
    "    data = [(\"John  D.\", 30), (\"Alice   G.\", 25)]\n",
    "    df = spark_session.createDataFrame(data, [\"name\", \"age\"])\n",
    "    \n",
    "    transformed_df = remove_extra_spaces(df, \"name\")\n",
    "    \n",
    "    assert transformed_df.count() == df.count()\n",
    "\n",
    "def test_senior_citizen_count(spark_session):\n",
    "    \"\"\"Test Case 3: Verify filtering logic.\"\"\"\n",
    "    data = [(\"A\", 60), (\"B\", 65), (\"C\", 55), (\"D\", 70)]\n",
    "    df = spark_session.createDataFrame(data, [\"name\", \"age\"])\n",
    "    \n",
    "    filtered_df = filter_senior_citizen(df, \"age\")\n",
    "    \n",
    "    # Expected: A(60), B(65), D(70) -> 3 records\n",
    "    assert filtered_df.count() == 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "873e3611",
   "metadata": {},
   "source": [
    "We purposefully write a failing test to see how pytest reports errors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62d2ad9b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "%%writefile tests/test_negative.py\n",
    "import pytest\n",
    "from common import filter_senior_citizen\n",
    "\n",
    "def test_senior_citizen_count_negative(spark_session):\n",
    "    \"\"\"\n",
    "    Test Case 4: Negative Scenario.\n",
    "    We assert an incorrect count to force a failure demonstration.\n",
    "    \"\"\"\n",
    "    data = [(\"A\", 60), (\"B\", 65), (\"C\", 55), (\"D\", 20)]\n",
    "    df = spark_session.createDataFrame(data, [\"name\", \"age\"])\n",
    "    \n",
    "    filtered_df = filter_senior_citizen(df, \"age\")\n",
    "    \n",
    "    # Actual count is 2 (60, 65). \n",
    "    # We assert 3 to make it fail.\n",
    "    expected_count = 3 \n",
    "    \n",
    "    assert filtered_df.count() == expected_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e809881",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We run pytest from the command line using the '!' magic command.\n",
    "# -v : Verbose mode (shows each test name and result)\n",
    "# We point it to the 'tests/' folder.\n",
    "\n",
    "!python -m pytest tests/ -v"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29f2625b",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "1.  **Pytest Integration:** Pytest works seamlessly with PySpark.\n",
    "2.  **Fixtures:** Use `conftest.py` to manage the `SparkSession` lifecycle. Use `scope=\"session\"` to avoid restarting Spark for every single test function.\n",
    "3.  **Assertions:** Use standard Python `assert` statements to compare DataFrames (via `.collect()`, `.count()`, or `.schema`).\n",
    "4.  **Best Practices:** \n",
    "    *   Separate test logic from business logic.\n",
    "    *   Test both data values and row counts.\n",
    "    *   Integrate these tests into CI/CD pipelines (Jenkins, GitHub Actions) to run automatically on code commits."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
