{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dab62f6",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 1: Course Introduction & Agenda\n",
    "\n",
    "Welcome to this comprehensive series on **Spark Streaming with PySpark**. Real-time data processing is one of the most in-demand skills in the data engineering landscape today.\n",
    "\n",
    "In this course, we will move beyond static batch processing and learn how to process data as it arrives, enabling real-time analytics, dashboards, and decision-making.\n",
    "\n",
    "### What is this course about?\n",
    "This series is designed to take you from the fundamental concepts of **Structured Streaming** to advanced, real-world implementations using industry-standard tools like Kafka, Redis, and Cosmos DB."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a00a2c",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the streaming concepts, ensure you have the following foundational knowledge. If you are new to these topics, it is highly recommended to brush up on them first.\n",
    "\n",
    "1.  **Basic Python:** Understanding of functions, libraries, and data structures.\n",
    "2.  **Apache Spark Core:** Knowledge of how Spark works (Drivers, Executors).\n",
    "3.  **PySpark DataFrames:** Familiarity with transformations, actions, and the DataFrame API.\n",
    "\n",
    "*Note: This course assumes you have a working PySpark environment set up (Local or Docker).*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3c2558e",
   "metadata": {},
   "source": [
    "## Course Agenda\n",
    "\n",
    "Here is the roadmap of what we will cover in the upcoming notebooks:\n",
    "\n",
    "1.  **Structured Streaming Basics:** Understanding the \"What, When, How, and Where\" of streaming.\n",
    "2.  **Code Migration:** How to convert standard Spark **Batch** code into **Streaming** code.\n",
    "3.  **Integrations:** Real-time coding examples connecting Spark with:\n",
    "    *   **Apache Kafka** (Message Broker)\n",
    "    *   **Redis** (In-memory Data Store)\n",
    "    *   **Cosmos DB** (NoSQL Database)\n",
    "4.  **File Formats:** Handling various file formats (JSON, CSV, Parquet) in a streaming context.\n",
    "5.  **Time Handling:**\n",
    "    *   Event-time processing.\n",
    "    *   Handling late data and watermarking.\n",
    "6.  **Optimization:** Performance tuning techniques specific to Spark Streaming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c9aefe",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Let's perform a quick sanity check to ensure your PySpark environment is ready.\n",
    "# We will initialize a SparkSession and check the version.\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming_Intro_Check\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(\"Spark Session Created Successfully!\")\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "\n",
    "# If this cell runs without errors, you are ready for the next module."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e30615a",
   "metadata": {},
   "source": [
    "## Up Next\n",
    "\n",
    "In the next notebook, we will cover the **Basics of Spark Streaming**. We will answer four fundamental questions to build our theoretical foundation:\n",
    "\n",
    "1.  **What** is Structured Streaming?\n",
    "2.  **When** should you use it?\n",
    "3.  **How** does it work under the hood?\n",
    "4.  **Where** does it fit in the data ecosystem?\n",
    "\n",
    "See you in **Notebook 02!**"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
