{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0bc53ac6",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 2: Streaming Basics & Architecture\n",
    "\n",
    "In this module, we dive into the fundamental concepts that power Spark Structured Streaming. We will understand how Spark handles real-time data and the core differences between Batch and Streaming architectures.\n",
    "\n",
    "### Agenda\n",
    "1.  **Batch vs. Streaming:** Understanding the key differences.\n",
    "2.  **Architecture:** How Structured Streaming works under the hood (Micro-batches).\n",
    "3.  **The \"Unbounded Table\" Concept.**\n",
    "4.  **The 4 Pillars of Streaming:** What, How, When, and Where.\n",
    "5.  **Practical Demo:** A real-time Word Count application using Netcat."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18e9bc5e",
   "metadata": {},
   "source": [
    "## 1. Batch vs. Streaming\n",
    "\n",
    "Before writing code, it is crucial to understand *when* to use streaming.\n",
    "\n",
    "| Feature | Batch Processing | Stream Processing |\n",
    "| :--- | :--- | :--- |\n",
    "| **Data Size** | Large, finite bulk of data. | Small, infinite sequence of data. |\n",
    "| **Schedule** | Fixed intervals (Daily, Weekly). | Continuous (24/7), Real-time. |\n",
    "| **Latency** | High (Minutes to Hours). | Low (Seconds to Milliseconds). |\n",
    "| **Example** | Nightly ETL, Monthly Reporting. | Fraud Detection, Sensor Monitoring. |\n",
    "\n",
    "### Why use Spark for Streaming?\n",
    "*   **Unified API:** You use the exact same DataFrame API for streaming as you do for batch. If you know Spark SQL, you know Streaming.\n",
    "*   **Scalability:** Handles high-volume throughput.\n",
    "*   **Fault Tolerance:** Built-in recovery mechanisms."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "650834b6",
   "metadata": {},
   "source": [
    "## 2. How Structured Streaming Works\n",
    "\n",
    "Spark Streaming doesn't process data one record at a time (like Storm or Flink might). Instead, it uses **Micro-Batch Architecture**.\n",
    "\n",
    "1.  **Input:** Data arrives continuously (e.g., from Kafka or a Socket).\n",
    "2.  **Micro-Batching:** Spark chops this continuous stream into small chunks called \"Micro-batches\" (e.g., every 1 second of data).\n",
    "3.  **Processing:** The Spark Engine processes each small batch using the standard Spark SQL engine.\n",
    "4.  **Output:** The results are appended to the output sink.\n",
    "\n",
    "### The \"Unbounded Table\"\n",
    "Think of your data stream not as a queue, but as an **Input Table that never stops growing**.\n",
    "*   Every new data item is just a new row appended to this table.\n",
    "*   Spark runs your query on this \"Unbounded Table\" continuously."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dac38d41",
   "metadata": {},
   "source": [
    "## 3. The 4 Pillars of a Streaming Query\n",
    "\n",
    "Every Structured Streaming application answers four basic questions:\n",
    "\n",
    "1.  **WHAT (Input Sources):** Where is the data coming from?\n",
    "    *   *Examples: Kafka, Files (CSV/JSON), Socket (for testing).*\n",
    "2.  **HOW (Transformations):** What logic are we applying?\n",
    "    *   *Examples: Filtering, Grouping, Mapping (Standard DataFrame operations).*\n",
    "3.  **WHEN (Triggers):** How often should we process the data?\n",
    "    *   *Examples: Every 1 second, \"AvailableNow\", or continuous.*\n",
    "4.  **WHERE (Output Sinks):** Where should the results go?\n",
    "    *   *Examples: Console, File, Kafka, Database.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7ae325f",
   "metadata": {},
   "source": [
    "## 4. Practical Demo: Socket Word Count\n",
    "\n",
    "We will build a classic **Word Count** streaming app. We will type words into a terminal, and Spark will count them in real-time.\n",
    "\n",
    "### **Step 1: Open a Terminal**\n",
    "We need a source to send data. We will use `netcat` (a utility to read/write network connections).\n",
    "1.  Open your command prompt or terminal.\n",
    "2.  Run the following command to start a server on port 9999:\n",
    "    ```bash\n",
    "    nc -lk 9999\n",
    "    ```\n",
    "    *(Note: On Windows, you may need to install nmap/netcat or use WSL).*\n",
    "\n",
    "### **Step 2: Run the Code Below**\n",
    "Once the terminal is listening, run the PySpark code below. Then, type words into your terminal (e.g., \"cat dog cat\") and hit Enter. Watch the Jupyter output update!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e09550ce",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import explode, split, col\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Streaming_Word_Count\") \\\n",
    "    .master(\"local[2]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 2. WHAT: Read from the Socket (Input Source)\n",
    "# We subscribe to the localhost port 9999 where we are typing words\n",
    "lines_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# 3. HOW: Transform the data\n",
    "# The input comes in a column named \"value\". We split it by space and explode it into words.\n",
    "words_df = lines_df.select(\n",
    "    explode(\n",
    "        split(col(\"value\"), \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# Perform the aggregation (Count occurrences)\n",
    "word_counts_df = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# 4. WHEN & WHERE: Write to Console (Trigger & Output)\n",
    "# OutputMode \"complete\" means we rewrite the entire table of counts every time.\n",
    "query = word_counts_df.writeStream \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime=\"2 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "# Wait for the stream to finish (or stop it manually)\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e25dac7c",
   "metadata": {},
   "source": [
    "### Analysis of the Code\n",
    "\n",
    "In the code above, you saw `.outputMode(\"complete\")`. There are three main output modes in Spark Streaming which determine **how** data is written to the sink:\n",
    "\n",
    "1.  **Complete Mode:** The *entire* updated Result Table is written to the sink. Useful for aggregations (like our Word Count).\n",
    "2.  **Append Mode:** Only *new* rows added to the Result Table since the last trigger are written. Useful for simple transformations (no aggregations).\n",
    "3.  **Update Mode:** Only the rows that were *updated* in the last trigger are written.\n",
    "\n",
    "In the next notebook, we will explore these modes in depth and start working with file sources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
