{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4229600a",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 4: Your First Streaming Application (Word Count)\n",
    "\n",
    "In this module, we will write our first real-time data processing application. To understand the ease of Spark Structured Streaming, we will use a unique approach:\n",
    "\n",
    "1.  **Solve the problem in Batch:** We will write the code to count words from a static text file.\n",
    "2.  **Convert to Streaming:** We will change just a few lines of code to make it process real-time data from a Socket.\n",
    "\n",
    "### The Objective: Word Count\n",
    "We want to read lines of text, split them into individual words, and count the occurrence of each word.\n",
    "\n",
    "### Prerequisites\n",
    "*   **Netcat (ncat):** A utility to create a data stream from your terminal.\n",
    "    *   *Linux/Mac:* Pre-installed or `sudo apt-get install netcat`\n",
    "    *   *Windows:* Included in the Docker container setup in Module 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59c41cbc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"WordCount_Socket_Stream\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session Created Successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93c1f42f",
   "metadata": {},
   "source": [
    "## Part 1: Solving in Batch Mode\n",
    "\n",
    "First, let's solve the problem using a static file. Imagine we have a file `input.txt` with the sentence:\n",
    "> *\"Hello world Hello Spark\"*\n",
    "\n",
    "**Logic:**\n",
    "1.  **Read:** Load the text file.\n",
    "2.  **Split:** Break the sentence into a list of words: `[\"Hello\", \"world\", \"Hello\", \"Spark\"]`.\n",
    "3.  **Explode:** Convert the list into separate rows.\n",
    "4.  **Count:** Group by the word and count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c0f4c6b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- 1. Create Dummy Data for Batch Testing ---\n",
    "data = [(\"Hello world Hello Spark\",), (\"Spark Streaming is easy\",)]\n",
    "schema = [\"value\"]\n",
    "df_batch = spark.createDataFrame(data, schema)\n",
    "\n",
    "print(\"Input Batch Data:\")\n",
    "df_batch.show(truncate=False)\n",
    "\n",
    "# --- 2. Transformation Logic ---\n",
    "\n",
    "# A. Split the lines into words (creates an array)\n",
    "df_split = df_batch.withColumn(\"words\", split(col(\"value\"), \" \"))\n",
    "\n",
    "# B. Explode the array into rows\n",
    "df_exploded = df_split.select(explode(col(\"words\")).alias(\"word\"))\n",
    "\n",
    "# C. Aggregation (Count)\n",
    "df_count = df_exploded.groupBy(\"word\").count()\n",
    "\n",
    "print(\"Batch Word Count Result:\")\n",
    "df_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e621e46b",
   "metadata": {},
   "source": [
    "## Part 2: Converting to Streaming\n",
    "\n",
    "Now, let's convert the code above to handle real-time data.\n",
    "\n",
    "### The Conversion Steps:\n",
    "1.  **Read:** Change `spark.read` → `spark.readStream`.\n",
    "2.  **Source:** Change format to `\"socket\"` and specify host/port.\n",
    "3.  **Logic:** **NO CHANGE!** The logic for Split, Explode, and Count remains exactly the same.\n",
    "4.  **Write:** Change `df.show()` → `df.writeStream`.\n",
    "5.  **Output:** Specify output mode (`complete`) and sink (`console`).\n",
    "\n",
    "### Before Running: Start Netcat\n",
    "Open your terminal (or the terminal inside your Docker container) and run:\n",
    "```bash\n",
    "ncat -l 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5b4dcf8",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "### **Cell 6 [Code]: Streaming Implementation**\n",
    "\n",
    "```python\n",
    "# --- 1. Read Stream from Socket ---\n",
    "# We connect to localhost:9999\n",
    "lines_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# --- 2. Transformation Logic (EXACTLY SAME AS BATCH) ---\n",
    "\n",
    "# A. Split lines into words\n",
    "words_df = lines_df.select(\n",
    "    explode(\n",
    "        split(col(\"value\"), \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# B. Aggregation\n",
    "word_counts_df = words_df.groupBy(\"word\").count()\n",
    "\n",
    "# --- 3. Write Stream to Console ---\n",
    "# We use OutputMode \"complete\" to see the total count updated every time\n",
    "query = word_counts_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "# Let the stream run until you stop it manually\n",
    "# Go to your terminal, type words like \"hello spark\", hit Enter, and check this notebook's output.\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25983836",
   "metadata": {},
   "source": [
    "## Understanding Output Mode: \"Complete\"\n",
    "\n",
    "In the code above, we used `.outputMode(\"complete\")`.\n",
    "\n",
    "*   **Scenario:** We are doing an aggregation (`count`).\n",
    "*   **Behavior:** Every time new data arrives (e.g., you type \"hello\"), Spark recalculates the count for *all* words it has seen so far and prints the *entire* table to the console.\n",
    "*   **Result:** You will see the counts for \"hello\" increase every time you type it.\n",
    "\n",
    "*We will explore other modes like `append` and `update` in future modules.*"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
