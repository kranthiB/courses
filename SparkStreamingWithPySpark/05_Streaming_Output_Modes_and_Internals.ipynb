{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "35a65b7e",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 5: Output Modes & Under the Hood\n",
    "\n",
    "In this module, we will dig deeper into how Spark Streaming executes jobs and explore the different ways we can write data to a sink.\n",
    "\n",
    "### Agenda\n",
    "1.  **Under the Hood:** How Micro-batches work.\n",
    "2.  **Optimization:** Tuning Shuffle Partitions for small data.\n",
    "3.  **Output Modes:**\n",
    "    *   **Complete:** Output the entire updated result table.\n",
    "    *   **Update:** Output only the rows that were updated.\n",
    "    *   **Append:** Output only new rows (and why this is tricky with aggregations)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1491144d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import split, explode, col\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"OutputModes_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. OPTIMIZATION: Shuffle Partitions\n",
    "# By default, Spark creates 200 partitions when shuffling data (e.g., during groupBy).\n",
    "# For small streaming data, this is overkill and slows down processing.\n",
    "# We set it to a smaller number (e.g., 3 or 8) to speed up micro-batches.\n",
    "spark.conf.set(\"spark.sql.shuffle.partitions\", 3)\n",
    "\n",
    "print(\"Spark Session Created with optimized partitions!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89f0180c",
   "metadata": {},
   "source": [
    "## Defining the Streaming Logic\n",
    "We will use the same **Word Count** logic as the previous module. The transformation logic remains constant; we will only change the **Output Mode** to see how it affects the result.\n",
    "\n",
    "**Prerequisite:**\n",
    "Make sure your Netcat server is running:\n",
    "```bash\n",
    "ncat -l 9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b55248",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Transformation Logic\n",
    "\n",
    "# 1. Read from Socket\n",
    "lines_df = spark.readStream \\\n",
    "    .format(\"socket\") \\\n",
    "    .option(\"host\", \"localhost\") \\\n",
    "    .option(\"port\", 9999) \\\n",
    "    .load()\n",
    "\n",
    "# 2. Transform: Split and Explode\n",
    "words_df = lines_df.select(\n",
    "    explode(\n",
    "        split(col(\"value\"), \" \")\n",
    "    ).alias(\"word\")\n",
    ")\n",
    "\n",
    "# 3. Aggregate: Count words\n",
    "# Note: Aggregations require maintaining \"State\" across micro-batches.\n",
    "word_counts_df = words_df.groupBy(\"word\").count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39d29a82",
   "metadata": {},
   "source": [
    "## Output Mode 1: Complete\n",
    "\n",
    "*   **Behavior:** Spark outputs the **entire Result Table** to the console after every trigger.\n",
    "*   **Use Case:** When you need the total counts of everything calculated so far.\n",
    "*   **Observation:** If you type \"cat\" in Batch 1, and \"cat\" in Batch 2, the output in Batch 2 will show \"cat: 2\" along with all other previous words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e0e04ca",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell, type words in Netcat, and observe the console.\n",
    "# Stop this cell manually before running the next one.\n",
    "\n",
    "query_complete = word_counts_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"complete\") \\\n",
    "    .start()\n",
    "\n",
    "query_complete.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b4a24d",
   "metadata": {},
   "source": [
    "## Output Mode 2: Update\n",
    "\n",
    "*   **Behavior:** Spark outputs **only the rows that were updated** in the current micro-batch.\n",
    "*   **Use Case:** When you only care about what changed right now (e.g., sending alerts).\n",
    "*   **Observation:**\n",
    "    1. Type \"cat dog\" -> Output: \"cat: 1, dog: 1\"\n",
    "    2. Type \"cat\" -> Output: \"cat: 2\" (You will NOT see \"dog\" in this output because \"dog\" count did not change)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b693c0f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Run this cell AFTER stopping the previous query.\n",
    "\n",
    "query_update = word_counts_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .start()\n",
    "\n",
    "query_update.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f07e07e3",
   "metadata": {},
   "source": [
    "## Output Mode 3: Append\n",
    "\n",
    "*   **Behavior:** Spark outputs only **new rows** that are finalized.\n",
    "*   **The Catch:** For aggregations (like Word Count), Spark **cannot** use Append mode without a **Watermark**.\n",
    "*   **Why?** Spark doesn't know if the count for \"cat\" is finished. Data for \"cat\" could arrive 1 hour later. Since it can't update previous outputs in Append mode, it waits forever and outputs nothing (or throws an error).\n",
    "\n",
    "*We will utilize Append mode in future modules when we handle non-aggregated data (like ETL) or when we implement Watermarking.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5a0661b",
   "metadata": {},
   "source": [
    "### Summary of Output Modes\n",
    "\n",
    "| Mode | Behavior | Best For |\n",
    "| :--- | :--- | :--- |\n",
    "| **Complete** | Rewrites the whole table. | Dashboards, Total Aggregates. |\n",
    "| **Update** | Writes only changed rows. | Alerts, DB Updates, Push Notifications. |\n",
    "| **Append** | Writes only new rows (immutable). | Storing raw data, Log files. |"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
