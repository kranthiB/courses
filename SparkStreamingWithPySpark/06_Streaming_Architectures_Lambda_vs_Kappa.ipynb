{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4a145f5",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 6: Data Processing Architectures\n",
    "\n",
    "Before building complex pipelines, it is essential to understand the architectural patterns used in the industry. Today, we look at the two giants of Big Data Architecture: **Lambda** and **Kappa**.\n",
    "\n",
    "### Why does this matter?\n",
    "Structuring your data pipeline correctly determines:\n",
    "1.  **Latency:** How fast data is available.\n",
    "2.  **Accuracy:** How reliable the data is.\n",
    "3.  **Maintenance:** How hard it is to update the code."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6724e751",
   "metadata": {},
   "source": [
    "## 1. Lambda Architecture\n",
    "The \"Classic\" approach to Big Data.\n",
    "\n",
    "### Concept\n",
    "Lambda architecture divides the system into two distinct layers to handle data:\n",
    "1.  **Batch Layer (Cold Path):**\n",
    "    *   Processes all historical data.\n",
    "    *   High latency, High accuracy.\n",
    "    *   *Engine Example:* Apache Spark (Batch), Hadoop MapReduce.\n",
    "2.  **Speed Layer (Hot Path):**\n",
    "    *   Processes real-time data only.\n",
    "    *   Low latency, potentially lower accuracy (approximation).\n",
    "    *   *Engine Example:* Spark Streaming, Storm.\n",
    "3.  **Serving Layer:** Merges views from both layers for the final query.\n",
    "\n",
    "### Pros & Cons\n",
    "| Pros | Cons |\n",
    "| :--- | :--- |\n",
    "| **Robust:** Batch layer corrects streaming errors eventually. | **Complexity:** You maintain **two** codebases (Batch logic + Streaming logic). |\n",
    "| **Reliable:** Good for legacy systems. | **Data Discrepancy:** Logic might differ slightly between layers. |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8edad828",
   "metadata": {},
   "source": [
    "## 2. Kappa Architecture\n",
    "The \"Modern\" approach (and the focus of this course).\n",
    "\n",
    "### Concept\n",
    "Kappa architecture simplifies the system by treating **everything as a stream**.\n",
    "1.  **Single Layer (Speed Layer):**\n",
    "    *   There is **no** separate batch layer.\n",
    "    *   Real-time data is processed as it arrives.\n",
    "    *   Historical data is treated as a \"bounded stream\" and processed through the *same* engine.\n",
    "\n",
    "### How Spark Enables Kappa\n",
    "Spark Structured Streaming uses a **Unified API**. The code to process a batch file is almost identical to the code for a real-time stream. This makes implementing Kappa architecture very natural in Spark.\n",
    "\n",
    "### Pros & Cons\n",
    "| Pros | Cons |\n",
    "| :--- | :--- |\n",
    "| **Simplicity:** One codebase to maintain. | **Complexity in Reprocessing:** Replaying history requires resetting the stream offset. |\n",
    "| **Consistency:** Same logic applied to historic and real-time data. | **Out-of-Order Data:** Requires handling \"Late Data\" (solved by Watermarking). |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db2725aa",
   "metadata": {},
   "source": [
    "## 3. Comparison & Strategy\n",
    "\n",
    "| Feature | Lambda | Kappa |\n",
    "| :--- | :--- | :--- |\n",
    "| **Pipelines** | Two (Batch + Stream) | One (Stream only) |\n",
    "| **Code Duplication** | High | None |\n",
    "| **Use Case** | Complex Historical corrections | Modern Streaming ETL |\n",
    "\n",
    "### Our Approach:\n",
    "In this course, we lean towards **Kappa Architecture**.\n",
    "*   We write code that handles data as a stream.\n",
    "*   We will learn to handle the challenges of Kappa (like **Out-of-Order data**) using **Watermarking** in upcoming modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "367ffc12",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# This cell demonstrates how Spark supports Kappa Architecture through Unified API.\n",
    "# The logic remains exactly the same, only the input method changes.\n",
    "\n",
    "spark = SparkSession.builder.appName(\"Architecture_Demo\").getOrCreate()\n",
    "\n",
    "# --- Hypothetical Code Structure ---\n",
    "\n",
    "def process_data(df):\n",
    "    \"\"\"\n",
    "    This transformation logic applies to BOTH Batch and Stream.\n",
    "    This is the core of Kappa Architecture: Write Once, Run Anywhere.\n",
    "    \"\"\"\n",
    "    return df.groupBy(\"value\").count()\n",
    "\n",
    "# 1. Batch Read (Lambda Cold Path)\n",
    "# df_batch = spark.read.text(\"path/to/history\")\n",
    "# final_batch = process_data(df_batch)\n",
    "\n",
    "# 2. Streaming Read (Kappa / Lambda Hot Path)\n",
    "# df_stream = spark.readStream.format(\"socket\")...\n",
    "# final_stream = process_data(df_stream)\n",
    "\n",
    "print(\"Spark's DataFrame API unifies Batch and Streaming, enabling Kappa Architecture.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
