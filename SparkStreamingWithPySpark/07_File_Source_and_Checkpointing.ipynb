{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8613c343",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 7: File Sources & Checkpointing\n",
    "\n",
    "In the previous modules, we used Sockets (Netcat) to simulate data. In the real world, data often arrives as files (JSON, CSV, Parquet) in a \"Landing Zone\" or \"Stage\" bucket (S3, ADLS).\n",
    "\n",
    "### Objectives:\n",
    "1.  **Read Stream from Files:** Monitor a directory for new JSON files.\n",
    "2.  **Schema Inference:** Configure Spark to automatically detect file structure.\n",
    "3.  **Complex Data Processing:** Flatten nested JSON arrays and structs into a tabular format (CSV).\n",
    "4.  **Source Cleaning:** Archive processed files automatically.\n",
    "5.  **Checkpointing:** Understand how Spark remembers what it has processed.\n",
    "\n",
    "### The Scenario\n",
    "We are receiving IoT device data in JSON format. Each file contains a batch of readings.\n",
    "*   **Input:** Nested JSON with arrays.\n",
    "*   **Output:** Flattened CSV files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38e87413",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, current_timestamp\n",
    "\n",
    "# 1. Initialize Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"File_Streaming_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. Enable Schema Inference\n",
    "# By default, Spark Streaming requires you to define a schema upfront.\n",
    "# For this demo, we enable inference to let Spark read the JSON structure automatically.\n",
    "spark.conf.set(\"spark.sql.streaming.schemaInference\", \"true\")\n",
    "\n",
    "print(\"Spark Session Created with Schema Inference Enabled.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b941ffc",
   "metadata": {},
   "source": [
    "## The Input Data (JSON)\n",
    "\n",
    "We expect JSON files to be dropped into `data/input`. Here is the structure of our sample data (`device_01.json`):\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"eventId\": \"e1\",\n",
    "  \"data\": {\n",
    "    \"devices\": [\n",
    "      {\n",
    "        \"deviceId\": \"d1\",\n",
    "        \"temperature\": 25,\n",
    "        \"measure\": \"C\"\n",
    "      },\n",
    "      {\n",
    "        \"deviceId\": \"d2\",\n",
    "        \"temperature\": 78,\n",
    "        \"measure\": \"F\"\n",
    "      }\n",
    "    ]\n",
    "  },\n",
    "  \"eventTime\": \"2024-01-01 10:00:00\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9e89d44",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Challenge: The core data is inside a nested array data.devices. We need to explode this array to get one row per device.\n",
    "\n",
    "# Step 1 - Read Stream\n",
    "\n",
    "\n",
    "# Define paths\n",
    "input_dir = \"data/input\"\n",
    "archive_dir = \"data/archive\"\n",
    "\n",
    "# Read Stream\n",
    "# maxFilesPerTrigger: Limits how many files are processed per batch (simulates flow).\n",
    "# cleanSource: \"archive\" moves processed files to a different folder so the input folder stays clean.\n",
    "raw_df = spark.readStream \\\n",
    "    .format(\"json\") \\\n",
    "    .option(\"maxFilesPerTrigger\", 1) \\\n",
    "    .option(\"cleanSource\", \"archive\") \\\n",
    "    .option(\"sourceArchiveDir\", archive_dir) \\\n",
    "    .load(input_dir)\n",
    "\n",
    "# Note: If you get a \"Path does not exist\" error, manually create the 'data/input' folder.\n",
    "print(\"Read Stream Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d2accf5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Explode the array to create multiple rows\n",
    "exploded_df = raw_df.select(\n",
    "    col(\"eventId\"),\n",
    "    col(\"eventTime\"),\n",
    "    explode(col(\"data.devices\")).alias(\"device_data\")\n",
    ")\n",
    "\n",
    "# 2. Flatten the struct columns using Dot Notation\n",
    "flattened_df = exploded_df.select(\n",
    "    col(\"eventId\"),\n",
    "    col(\"eventTime\"),\n",
    "    col(\"device_data.deviceId\").alias(\"device_id\"),\n",
    "    col(\"device_data.temperature\").alias(\"temp\"),\n",
    "    col(\"device_data.measure\").alias(\"unit\"),\n",
    "    current_timestamp().alias(\"processed_time\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c76cfdfb",
   "metadata": {},
   "source": [
    "## Step 3: Checkpointing & Writing\n",
    "\n",
    "Before we write, we must define a **Checkpoint Location**.\n",
    "\n",
    "### What is Checkpointing?\n",
    "It is a directory where Spark saves the **state** of the stream.\n",
    "1.  **Offsets:** Which files have I already processed?\n",
    "2.  **State:** (For aggregations) What are the current counts?\n",
    "\n",
    "**Why is it critical?**\n",
    "If your application crashes, Spark reads the checkpoint directory upon restart. It sees, *\"Ah, I already processed `device_01.json`, so I will ignore it and start looking for `device_02.json`.\"* without processing duplicates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5faf1bd",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "output_dir = \"data/output\"\n",
    "checkpoint_dir = \"data/checkpoint\"\n",
    "\n",
    "# Write Stream\n",
    "query = flattened_df.writeStream \\\n",
    "    .format(\"csv\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"path\", output_dir) \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .start()\n",
    "\n",
    "print(f\"Streaming to {output_dir}...\")\n",
    "print(f\"Tracking state in {checkpoint_dir}...\")\n",
    "\n",
    "# Keep the cell running\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da5519f",
   "metadata": {},
   "source": [
    "## How to Test This?\n",
    "\n",
    "Since this is a file-based trigger, nothing happens until you drop a file.\n",
    "\n",
    "1.  **Create** a file named `device_01.json` with the JSON content shown in Cell 3.\n",
    "2.  **Paste** it into the `data/input` folder.\n",
    "3.  **Observe:**\n",
    "    *   Spark will detect the file.\n",
    "    *   It will process it and write a CSV to `data/output`.\n",
    "    *   It will **move** the JSON file from `data/input` to `data/archive`.\n",
    "4.  **Test Checkpoint:** Paste the *exact same file* into `data/input` again.\n",
    "    *   **Result:** Nothing happens! Spark checks the `checkpoint` folder, realizes this file signature was already processed, and skips it."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
