{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "621e96ff",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 8: Checkpointing Deep Dive & Fault Tolerance\n",
    "\n",
    "In the previous module, we enabled **Checkpointing** to maintain state. Today, we will look \"under the hood\" of the Checkpoint directory to understand how Spark achieves **Fault Tolerance** and **Exactly-Once Semantics**.\n",
    "\n",
    "### Objectives:\n",
    "1.  **Analyze the Checkpoint Directory:** Understand `commits`, `offsets`, `sources`, and `metadata`.\n",
    "2.  **Idempotency Test:** Why does dropping the same file twice not work?\n",
    "3.  **Fault Tolerance Experiment:** Simulate a crash (delete a commit) and watch Spark recover.\n",
    "4.  **Production Best Practices:** How to handle re-processing safely."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46737170",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, explode, current_timestamp\n",
    "\n",
    "# 1. Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Checkpoint_Deep_Dive\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.sql.streaming.schemaInference\", \"true\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# 2. Define Paths\n",
    "base_dir = \"data\"\n",
    "input_dir = f\"{base_dir}/input\"\n",
    "checkpoint_dir = f\"{base_dir}/checkpoint\"\n",
    "\n",
    "# 3. Clean Start (Optional: Run this to reset experiments)\n",
    "# if os.path.exists(checkpoint_dir):\n",
    "#     shutil.rmtree(checkpoint_dir)\n",
    "\n",
    "# 4. Define Processing Logic (Same as Module 7)\n",
    "def get_streaming_df():\n",
    "    raw_df = spark.readStream \\\n",
    "        .format(\"json\") \\\n",
    "        .option(\"maxFilesPerTrigger\", 1) \\\n",
    "        .load(input_dir)\n",
    "\n",
    "    exploded_df = raw_df.select(\n",
    "        col(\"eventId\"),\n",
    "        explode(col(\"data.devices\")).alias(\"device_data\")\n",
    "    )\n",
    "\n",
    "    return exploded_df.select(\n",
    "        col(\"eventId\"),\n",
    "        col(\"device_data.deviceId\").alias(\"device_id\"),\n",
    "        col(\"device_data.temperature\").alias(\"temp\"),\n",
    "        current_timestamp().alias(\"processed_time\")\n",
    "    )\n",
    "\n",
    "print(\"Setup Complete. Processing logic defined.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b96aa62",
   "metadata": {},
   "source": [
    "## 1. The Checkpoint Structure\n",
    "\n",
    "When a query starts, Spark creates a folder structure at the `checkpointLocation`.\n",
    "\n",
    "*   **`metadata`**: Stores the unique ID of the streaming query. If you delete the checkpoint folder, a new ID is generated, and Spark treats it as a brand new query.\n",
    "*   **`sources`**: Tracks exactly which files/offsets have been read. This prevents processing the same file twice.\n",
    "*   **`offsets`**: Records the range of data (offset range) included in a specific batch ID.\n",
    "*   **`commits`**: The \"Stamp of Approval\". A file appears here ONLY when a batch is successfully processed and written to the sink.\n",
    "\n",
    "**The Flow:**\n",
    "1.  Spark reads new data -> Writes to **`offsets`** (Batch N).\n",
    "2.  Spark processes data -> Writes to Sink.\n",
    "3.  Spark finishes batch -> Writes to **`commits`** (Batch N)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a63180d9",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Start the stream to generate some checkpoint data\n",
    "# Make sure you have at least one JSON file in 'data/input'\n",
    "df = get_streaming_df()\n",
    "\n",
    "query = df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"checkpointLocation\", checkpoint_dir) \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()\n",
    "\n",
    "print(\"Batch processed. Checkpoint directory updated.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33c246fc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Helper function to list files in checkpoint sub-directories\n",
    "def inspect_checkpoint(sub_dir):\n",
    "    path = f\"{checkpoint_dir}/{sub_dir}\"\n",
    "    if os.path.exists(path):\n",
    "        print(f\"--- Content of {sub_dir} ---\")\n",
    "        files = os.listdir(path)\n",
    "        print(files)\n",
    "        # Optionally print content of the latest file\n",
    "        if files:\n",
    "            latest_file = max([f for f in files if not f.startswith('.')], key=lambda x: int(x) if x.isdigit() else 0)\n",
    "            with open(f\"{path}/{latest_file}\", 'r') as f:\n",
    "                print(f\"\\n[Content of {latest_file}]:\\n{f.read()[:200]}...\") # Print first 200 chars\n",
    "    else:\n",
    "        print(f\"{sub_dir} does not exist yet.\")\n",
    "\n",
    "# Inspect directories\n",
    "inspect_checkpoint(\"metadata\")\n",
    "inspect_checkpoint(\"sources/0\") # 0 is the source ID\n",
    "inspect_checkpoint(\"offsets\")\n",
    "inspect_checkpoint(\"commits\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49169110",
   "metadata": {},
   "source": [
    "## Experiment 1: Re-running the Same File\n",
    "\n",
    "**Scenario:** You accidentally drop `device_01.json` into the input folder again.\n",
    "**Result:** Spark ignores it.\n",
    "\n",
    "**Why?**\n",
    "Spark looks at `checkpoint/sources/0/`. It sees that `device_01.json` is already listed in the tracking log for a previous batch. Since the file signature hasn't changed, it skips it to ensure **Exactly-Once** processing.\n",
    "\n",
    "**How to force re-process?**\n",
    "1.  **Rename the file:** `device_01_v2.json`. Spark treats it as a new file.\n",
    "2.  **Delete Checkpoint:** Spark forgets everything and re-processes ALL files in the input directory."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed56b272",
   "metadata": {},
   "source": [
    "## Experiment 2: Simulating a Failure\n",
    "\n",
    "Let's simulate a crash where Spark **Read** the data but **Failed to Commit** (e.g., power failure before writing to `commits`).\n",
    "\n",
    "**Steps to Simulate:**\n",
    "1.  Place a NEW file (`device_02.json`) in `data/input`.\n",
    "2.  Run the stream (Cell 4).\n",
    "3.  Look at `checkpoint/commits` and identify the latest batch ID (e.g., `1`).\n",
    "4.  **Manually Delete** that file (`checkpoint/commits/1`).\n",
    "    *   *State:* Spark has `offsets/1` (knows data exists) but no `commits/1` (thinks job failed).\n",
    "5.  **Re-run the Stream** (Cell 4).\n",
    "\n",
    "**Observation:**\n",
    "Spark sees Offset 1 exists but Commit 1 is missing. It infers that Batch 1 failed. It will **Re-run Batch 1** automatically processing `device_02.json` again to ensure data integrity."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d787b989",
   "metadata": {},
   "source": [
    "## Production Best Practices\n",
    "\n",
    "1.  **Do NOT Touch Manually:** Never manually edit or delete files in the checkpoint directory in production. It can corrupt the stream state permanently.\n",
    "2.  **Changing Logic:** If you change your code logic (e.g., add a new column), you often cannot use the old checkpoint. You must restart with a fresh checkpoint directory.\n",
    "3.  **Re-processing:** To re-process data, it is safer to rename the input file than to tamper with the checkpoint sources."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
