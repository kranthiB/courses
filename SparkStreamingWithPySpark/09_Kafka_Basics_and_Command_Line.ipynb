{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8ce5d51",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 9: Apache Kafka Basics & CLI\n",
    "\n",
    "Before we integrate Kafka with Spark, we need to understand what Kafka is and how to interact with it.\n",
    "\n",
    "### What is Kafka?\n",
    "Apache Kafka is a distributed event streaming platform. It acts as a high-throughput, low-latency **message bus** that decouples data producers from data consumers.\n",
    "\n",
    "### Core Concepts:\n",
    "1.  **Producer:** The application sending data (e.g., IoT sensor, Web Server).\n",
    "2.  **Consumer:** The application reading data (e.g., Spark Streaming).\n",
    "3.  **Broker:** A single Kafka server. A group of brokers forms a **Cluster**.\n",
    "4.  **Topic:** A category or feed name to which records are published. Think of it as a \"Folder\" for messages.\n",
    "5.  **Partition:** Topics are split into partitions to allow parallel processing.\n",
    "6.  **Offset:** A unique ID assigned to every message within a partition."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b7ae9e0",
   "metadata": {},
   "source": [
    "## Kafka Architecture - Pub/Sub Model\n",
    "\n",
    "*   **Publish/Subscribe:** Producers publish messages to a Topic. Consumers subscribe to that Topic.\n",
    "*   **Retention:** Unlike a standard queue (RabbitMQ), Kafka stores messages for a configurable time (e.g., 7 days). This allows consumers to replay old data.\n",
    "\n",
    "```mermaid\n",
    "graph LR\n",
    "    P[Producer] -->|Writes| T(Topic: 'sensor-data')\n",
    "    subgraph Kafka Cluster\n",
    "        T --> Part0[Partition 0]\n",
    "        T --> Part1[Partition 1]\n",
    "    end\n",
    "    Part0 -->|Reads| C1[Consumer A]\n",
    "    Part1 -->|Reads| C2[Consumer B]\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99fc1723",
   "metadata": {},
   "source": [
    "### **Setup - Accessing Kafka CLI**\n",
    "\n",
    "\n",
    "#### Accessing Kafka CLI\n",
    "\n",
    "We will use the terminal inside our Docker container to run Kafka commands.\n",
    "\n",
    "**Step 1:** Open a Terminal.\n",
    "**Step 2:** Connect to the Kafka container:\n",
    "```bash\n",
    "docker exec -it ed-kafka /bin/bash\n",
    "```\n",
    "#### List Topics (Run via Python wrapper)\n",
    "\n",
    "Note: While you should run these in the terminal, we can use Python `os` module to execute them here for documentation purposes.\n",
    "\n",
    "```python\n",
    "import os\n",
    "\n",
    "# Command to list topics\n",
    "# We assume 'ed-kafka' is accessible via localhost:9092 from the host or mapped port.\n",
    "# NOTE: Running kafka-topics.sh requires the script to be in your PATH or specific location.\n",
    "# It is RECOMMENDED to run these in the Docker Terminal as shown in the video.\n",
    "\n",
    "print(\"Run this in your Docker Terminal:\")\n",
    "print(\"kafka-topics --list --bootstrap-server localhost:9092\")\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a3b7fbae",
   "metadata": {},
   "source": [
    "## Cheat Sheet: Kafka CLI Commands\n",
    "\n",
    "Run these inside the Kafka container.\n",
    "\n",
    "### 1. Create a Topic\n",
    "Create a topic named `test-topic` with 1 partition and replication factor of 1.\n",
    "```bash\n",
    "kafka-topics --create --topic test-topic --bootstrap-server localhost:9092 --partitions 1 --replication-factor 1\n",
    "```\n",
    "\n",
    "### 2. List Topics\n",
    "See all available topics.\n",
    "```bash\n",
    "kafka-topics --list --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "### 3. Describe Topic\n",
    "Get details about partitions, leaders, and replicas.\n",
    "```bash\n",
    "kafka-topics --describe --topic test-topic --bootstrap-server localhost:9092\n",
    "```\n",
    "\n",
    "### 4. Start a Producer (Write Data)\n",
    "Opens an interactive shell to type messages.\n",
    "```bash\n",
    "kafka-console-producer --topic test-topic --bootstrap-server localhost:9092\n",
    "> Hello Kafka\n",
    "> This is message 2\n",
    "```\n",
    "\n",
    "### 5. Start a Consumer (Read Data)\n",
    "Reads messages from the beginning.\n",
    "```bash\n",
    "kafka-console-consumer --topic test-topic --from-beginning --bootstrap-server localhost:9092\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9a30de7",
   "metadata": {},
   "source": [
    "## Partitions & Offsets\n",
    "\n",
    "When you created the topic, you saw the flag `--partitions`.\n",
    "\n",
    "*   **Scaling:** If you set partitions = 3, Kafka splits the data into 3 parts.\n",
    "*   **Parallelism:** Spark can run 3 tasks in parallel to read from these 3 partitions simultaneously.\n",
    "*   **Ordering:** Kafka guarantees order **only within a partition**, not across the whole topic.\n",
    "\n",
    "**Offset:**\n",
    "*   Message 1 in Partition 0 has Offset 0.\n",
    "*   Message 2 in Partition 0 has Offset 1.\n",
    "*   Spark tracks these offsets in the **Checkpoint Directory** to know exactly what it has read."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
