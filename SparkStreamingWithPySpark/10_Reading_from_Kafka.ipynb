{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7f232c97",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 10: Reading from Kafka & JSON Parsing\n",
    "\n",
    "This module combines the power of Kafka with Spark Structured Streaming. We will build a real-time pipeline that reads IoT device data from a Kafka topic, parses the JSON payload, and flattens nested arrays into a tabular format.\n",
    "\n",
    "### Objectives:\n",
    "1.  **Kafka Source:** Configure Spark to read from a Kafka topic.\n",
    "2.  **Jar Dependencies:** Load the `spark-sql-kafka` library.\n",
    "3.  **Data Extraction:** Convert binary Kafka values to Strings.\n",
    "4.  **JSON Parsing:** Use `from_json` with a defined schema.\n",
    "5.  **Flattening:** Explode nested arrays and promote struct fields to columns.\n",
    "\n",
    "### Prerequisites\n",
    "*   Kafka Cluster running (Module 3 Setup).\n",
    "*   `device-data` topic created.\n",
    "*   Data being produced to the topic (we will do this via terminal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a8486b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, expr\n",
    "\n",
    "# Define the Kafka package version. Ensure it matches your Spark & Scala version.\n",
    "# For Spark 3.x and Scala 2.12:\n",
    "kafka_jar_package = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka_Streaming_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session with Kafka support created successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c36b0078",
   "metadata": {},
   "source": [
    "## Defining the Schema\n",
    "\n",
    "Streaming DataFrames require a schema to parse JSON data. Our input data looks like this:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"eventId\": \"e1\",\n",
    "  \"eventTime\": \"2024-01-01...\",\n",
    "  \"data\": {\n",
    "    \"devices\": [\n",
    "      {\"deviceId\": \"d1\", \"temperature\": 25, \"measure\": \"C\", \"status\": \"SUCCESS\"},\n",
    "      {\"deviceId\": \"d2\", \"temperature\": 80, \"measure\": \"F\", \"status\": \"ERROR\"}\n",
    "    ]\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a06b8132",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Schema Definition\n",
    "\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Define schema for the inner device object\n",
    "device_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"measure\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "\n",
    "# Define schema for the main payload\n",
    "json_schema = StructType([\n",
    "    StructField(\"eventId\", StringType(), True),\n",
    "    StructField(\"eventTime\", StringType(), True),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"devices\", ArrayType(device_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "print(\"Schema Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8950363f",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Kafka Configuration\n",
    "kafka_topic = \"device-data\"\n",
    "kafka_bootstrap_servers = \"localhost:29092\" # Use \"ed-kafka:9092\" if running INSIDE Docker\n",
    "\n",
    "# Read Stream\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", kafka_bootstrap_servers) \\\n",
    "    .option(\"subscribe\", kafka_topic) \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()\n",
    "\n",
    "# Kafka sends data as binary (key, value). We need to cast 'value' to String.\n",
    "json_df = kafka_df.select(\n",
    "    col(\"value\").cast(\"string\").alias(\"json_string\")\n",
    ")\n",
    "\n",
    "print(\"Kafka Stream Initialized.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1e9fac",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# 1. Parse JSON string into a Struct\n",
    "parsed_df = json_df.select(\n",
    "    from_json(col(\"json_string\"), json_schema).alias(\"payload\")\n",
    ")\n",
    "\n",
    "# 2. Explode the array (One row per device)\n",
    "exploded_df = parsed_df.select(\n",
    "    col(\"payload.eventId\"),\n",
    "    col(\"payload.eventTime\"),\n",
    "    explode(col(\"payload.data.devices\")).alias(\"device\")\n",
    ")\n",
    "\n",
    "# 3. Flatten the columns\n",
    "flattened_df = exploded_df.select(\n",
    "    col(\"eventId\"),\n",
    "    col(\"eventTime\"),\n",
    "    col(\"device.deviceId\"),\n",
    "    col(\"device.temperature\"),\n",
    "    col(\"device.measure\"),\n",
    "    col(\"device.status\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f9753",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write the result to the console to verify\n",
    "query = flattened_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"append\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .start()\n",
    "\n",
    "print(\"Streaming query started...\")\n",
    "print(\"Go to your terminal and produce data to 'device-data' topic.\")\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf1949c8",
   "metadata": {},
   "source": [
    "## How to produce data?\n",
    "\n",
    "1.  Open your terminal.\n",
    "2.  Connect to the Kafka container:\n",
    "    `docker exec -it ed-kafka /bin/bash`\n",
    "3.  Start the console producer:\n",
    "    `kafka-console-producer --topic device-data --bootstrap-server localhost:9092`\n",
    "4.  **Paste this JSON:**\n",
    "    ```json\n",
    "    {\"eventId\": \"e100\", \"eventTime\": \"2024-01-01 10:00:00\", \"data\": {\"devices\": [{\"deviceId\": \"D1\", \"temperature\": 25, \"measure\": \"C\", \"status\": \"OK\"}, {\"deviceId\": \"D2\", \"temperature\": 90, \"measure\": \"F\", \"status\": \"WARN\"}]}}\n",
    "    ```\n",
    "5.  Watch the Jupyter output cell update with the flattened data rows!"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
