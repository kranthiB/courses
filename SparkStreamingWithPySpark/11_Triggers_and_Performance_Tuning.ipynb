{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c139c5c2",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 11: Triggers, Automation & Performance Tuning\n",
    "\n",
    "In the previous module, we manually pushed JSON data to Kafka. This is not practical for testing performance.\n",
    "\n",
    "### Objectives:\n",
    "1.  **Automate Data Production:** Use a Python script to generate thousands of fake IoT events and push them to Kafka automatically.\n",
    "2.  **Explore Triggers:** Control *when* Spark processes a batch.\n",
    "    *   **Default (Unspecified):** Run next batch as soon as previous one finishes.\n",
    "    *   **ProcessingTime:** Run at fixed intervals (e.g., every 10 seconds).\n",
    "    *   **AvailableNow (Once):** Process all available data then stop (great for cost saving/periodic jobs).\n",
    "    *   **Continuous (Experimental):** Low-latency processing (ms level).\n",
    "3.  **Performance Tuning:** Adjust shuffle partitions to speed up small-data processing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6aa29e",
   "metadata": {},
   "source": [
    "## Step 1: Automate Data Generation\n",
    "\n",
    "Instead of typing JSON manually, we will use a Python script to generate random data.\n",
    "\n",
    "**Action:**\n",
    "1.  Open your terminal.\n",
    "2.  Ensure you have the `kafka-python` library installed:\n",
    "    `pip install kafka-python`\n",
    "3.  Run the provided python generator script (let's assume you have `device_events.py` and `post_to_kafka.py` from the repo).\n",
    "    `python post_to_kafka.py`\n",
    "\n",
    "*This script will start flooding your 'device-data' topic with random events.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "167b219b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We reuse the exact same logic from Module 10, but we will change the .trigger() part.\n",
    "\n",
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "kafka_jar_package = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Kafka_Triggers_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate() # TUNING: Reduced partitions for faster local processing\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# --- Schema Definition (Same as before) ---\n",
    "device_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"measure\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "json_schema = StructType([\n",
    "    StructField(\"eventId\", StringType(), True),\n",
    "    StructField(\"eventTime\", StringType(), True),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"devices\", ArrayType(device_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# --- Read Stream ---\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n",
    "    .option(\"subscribe\", \"device-data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# --- Transformation Logic ---\n",
    "json_df = kafka_df.select(col(\"value\").cast(\"string\").alias(\"json_string\"))\n",
    "parsed_df = json_df.select(from_json(col(\"json_string\"), json_schema).alias(\"payload\"))\n",
    "flattened_df = parsed_df.select(\n",
    "    col(\"payload.eventId\"),\n",
    "    col(\"payload.eventTime\"),\n",
    "    explode(col(\"payload.data.devices\")).alias(\"device\")\n",
    ").select(\n",
    "    \"eventId\", \"eventTime\", \"device.deviceId\", \"device.temperature\", \"device.status\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b47638d",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scenario: Run a micro-batch every 10 seconds.\n",
    "# Even if data arrives at t=1s, Spark waits until t=10s to process it.\n",
    "# This increases latency but reduces overhead for small batches.\n",
    "\n",
    "print(\"Starting Stream with ProcessingTime='10 seconds'...\")\n",
    "\n",
    "query_processing_time = flattened_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(processingTime='10 seconds') \\\n",
    "    .start()\n",
    "\n",
    "# Let it run for 30 seconds then stop to test next trigger\n",
    "query_processing_time.awaitTermination(30)\n",
    "query_processing_time.stop()\n",
    "print(\"Stopped ProcessingTime Query.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a80cf200",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scenario: \"I want to run this as a nightly job, process everything since last run, and shut down.\"\n",
    "# This mimics Batch processing but uses Streaming architecture (Kappa).\n",
    "\n",
    "print(\"Starting Stream with AvailableNow=True...\")\n",
    "\n",
    "query_once = flattened_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .trigger(availableNow=True) \\\n",
    "    .start()\n",
    "\n",
    "query_once.awaitTermination()\n",
    "print(\"Job Finished! (It stopped automatically because availableNow=True)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2528d8ec",
   "metadata": {},
   "source": [
    "## Trigger 3: Continuous Processing\n",
    "\n",
    "*   **Concept:** Instead of micro-batches, Spark launches long-running tasks that process data row-by-row as it arrives.\n",
    "*   **Latency:** Milliseconds (vs Seconds for micro-batch).\n",
    "*   **Limitation:** Not all operations (like aggregations) are supported in this mode yet. Since we are only doing map/flatmap (parsing & flatten), it *might* work here, but requires specific support.\n",
    "\n",
    "*For this course, we stick to Micro-batch modes as they are the industry standard for robust pipelines.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59aa187c",
   "metadata": {},
   "source": [
    "## Tuning Summary\n",
    "\n",
    "1.  **Shuffle Partitions:**\n",
    "    *   `spark.sql.shuffle.partitions` defaults to 200.\n",
    "    *   For streaming, this often creates too many tiny tasks.\n",
    "    *   **Action:** Reduce it to 2, 4, or 8 (matching your core count) for low-volume streams.\n",
    "\n",
    "2.  **Max Offsets Per Trigger:**\n",
    "    *   `.option(\"maxOffsetsPerTrigger\", 1000)`\n",
    "    *   Prevents the stream from crashing if a huge burst of data arrives. It forces Spark to read chunks of 1000 messages at a time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
