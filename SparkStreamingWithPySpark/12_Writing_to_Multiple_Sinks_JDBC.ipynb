{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "80d986bc",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 12: Writing to Multiple Sinks\n",
    "\n",
    "A common requirement in data engineering is to write the same streaming data to multiple destinations. For example:\n",
    "1.  **Data Lake (Parquet):** For long-term storage and historical analysis.\n",
    "2.  **Database (Postgres):** For real-time dashboards and low-latency queries.\n",
    "\n",
    "Spark's `writeStream` only supports a single sink by default. To write to multiple sinks, we use the **`foreachBatch`** API.\n",
    "\n",
    "### Prerequisites\n",
    "*   **Postgres Database:** We have a Postgres container running in our Docker setup.\n",
    "*   **JDBC Driver:** We need to load the Postgres JDBC driver jar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a6af56",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, ArrayType\n",
    "\n",
    "# Define packages: Kafka + Postgres JDBC\n",
    "# Note: Ensure the postgres jar version is compatible.\n",
    "packages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\",\n",
    "    \"org.postgresql:postgresql:42.2.18\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Multiple_Sinks_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "print(\"Spark Session with Kafka & Postgres support created!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57dbe17b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Schema Definition ---\n",
    "device_schema = StructType([\n",
    "    StructField(\"deviceId\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"measure\", StringType(), True),\n",
    "    StructField(\"status\", StringType(), True)\n",
    "])\n",
    "json_schema = StructType([\n",
    "    StructField(\"eventId\", StringType(), True),\n",
    "    StructField(\"eventTime\", StringType(), True),\n",
    "    StructField(\"data\", StructType([\n",
    "        StructField(\"devices\", ArrayType(device_schema), True)\n",
    "    ]), True)\n",
    "])\n",
    "\n",
    "# --- Read Stream ---\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n",
    "    .option(\"subscribe\", \"device-data\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# --- Transform ---\n",
    "flattened_df = kafka_df.select(col(\"value\").cast(\"string\").alias(\"json_string\")) \\\n",
    "    .select(from_json(col(\"json_string\"), json_schema).alias(\"payload\")) \\\n",
    "    .select(\n",
    "        col(\"payload.eventId\"),\n",
    "        col(\"payload.eventTime\"),\n",
    "        explode(col(\"payload.data.devices\")).alias(\"device\")\n",
    "    ).select(\n",
    "        \"eventId\", \"eventTime\", \"device.deviceId\", \"device.temperature\", \"device.measure\", \"device.status\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09ba0d7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This function runs for every micro-batch.\n",
    "# 'batch_df' is a standard static DataFrame containing data for this specific batch.\n",
    "# 'batch_id' is a unique ID for the batch.\n",
    "\n",
    "def write_to_multiple_sinks(batch_df, batch_id):\n",
    "    print(f\"Processing Batch ID: {batch_id} with {batch_df.count()} records\")\n",
    "    \n",
    "    # 1. Write to PARQUET (Data Lake)\n",
    "    # We use standard batch write API here (.write, NOT .writeStream)\n",
    "    batch_df.write \\\n",
    "        .format(\"parquet\") \\\n",
    "        .mode(\"append\") \\\n",
    "        .save(\"data/output/device_data_parquet\")\n",
    "    \n",
    "    # 2. Write to POSTGRES (Database)\n",
    "    # Ensure you have created the table 'device_data' in Postgres or allow Spark to create it.\n",
    "    # JDBC URL: jdbc:postgresql://localhost:5432/postgres (mapped port from Docker)\n",
    "    \n",
    "    # Note: Replace with your actual Postgres credentials/details from Docker Compose\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "    jdbc_properties = {\n",
    "        \"user\": \"postgres\",\n",
    "        \"password\": \"password\", # Default password in many docker images\n",
    "        \"driver\": \"org.postgresql.Driver\"\n",
    "    }\n",
    "    \n",
    "    batch_df.write \\\n",
    "        .jdbc(url=jdbc_url, table=\"device_data\", mode=\"append\", properties=jdbc_properties)\n",
    "        \n",
    "    print(\"Batch written to Parquet and Postgres successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1625afc2",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "print(\"Starting Multiple Sink Stream...\")\n",
    "\n",
    "# Note: We pass the function NAME to foreachBatch (without parentheses)\n",
    "query = flattened_df.writeStream \\\n",
    "    .foreachBatch(write_to_multiple_sinks) \\\n",
    "    .option(\"checkpointLocation\", \"data/checkpoint_multi_sink\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa0fd5",
   "metadata": {},
   "source": [
    "## Verification\n",
    "\n",
    "1.  **Check Parquet:** Look into the folder `data/output/device_data_parquet`. You should see `.parquet` files appearing.\n",
    "2.  **Check Postgres:** Connect to your Postgres container/client and query the table:\n",
    "    ```sql\n",
    "    SELECT * FROM device_data;\n",
    "    ```\n",
    "3.  **Check Console:** The print statements inside the python function will show up in the Jupyter/Terminal logs indicating batch progress."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
