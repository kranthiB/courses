{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f625076d",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 13: Error Handling & Exception Management\n",
    "\n",
    "In a real-time pipeline, bad data is inevitable. If a corrupt JSON arrives, your entire stream could crash. We need a robust strategy to handle this.\n",
    "\n",
    "### Objectives:\n",
    "1.  **Malformed Records:** Identify data that doesn't match our schema.\n",
    "2.  **Data Quality Checks:** Filter out records with missing critical fields (e.g., null `customerId`).\n",
    "3.  **Segregation:** Split the stream into two paths:\n",
    "    *   **Valid Data:** Process and write to the main table.\n",
    "    *   **Error Data:** Write to an \"Error Table\" for debugging.\n",
    "4.  **Exception Handling:** Wrap custom logic in `try-except` blocks to prevent driver crashes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b6818e5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, from_json, explode, current_timestamp, size, lit, array\n",
    "\n",
    "# Define packages (Kafka + Postgres)\n",
    "packages = [\n",
    "    \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\",\n",
    "    \"org.postgresql:postgresql:42.2.18\"\n",
    "]\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Error_Handling_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", \",\".join(packages)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# --- Helper Function for Writing to Postgres ---\n",
    "def write_to_postgres(df, table_name):\n",
    "    jdbc_url = \"jdbc:postgresql://localhost:5432/postgres\"\n",
    "    jdbc_properties = {\"user\": \"postgres\", \"password\": \"password\", \"driver\": \"org.postgresql.Driver\"}\n",
    "    df.write.jdbc(url=jdbc_url, table=table_name, mode=\"append\", properties=jdbc_properties)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803e7b61",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# This function handles the micro-batch logic\n",
    "def process_microbatch(batch_df, batch_id):\n",
    "    print(f\"Processing Batch: {batch_id}\")\n",
    "    \n",
    "    # --- 1. Parse JSON & Handle Malformed Data ---\n",
    "    # We parse the raw string. If parsing fails, the struct fields will be null.\n",
    "    parsed_df = batch_df.select(\n",
    "        col(\"value\").cast(\"string\").alias(\"json_string\")\n",
    "    ).select(\n",
    "        from_json(col(\"json_string\"), json_schema).alias(\"payload\"), # using json_schema from previous modules\n",
    "        col(\"json_string\") # Keep original string for error logging\n",
    "    )\n",
    "    \n",
    "    # --- 2. Define Error Conditions ---\n",
    "    # A. Malformed JSON (payload is null)\n",
    "    # B. Missing Business Key (customerId is null)\n",
    "    # C. Empty Device Array (size of devices array is 0)\n",
    "    \n",
    "    # We add a column to flag errors\n",
    "    checked_df = parsed_df.withColumn(\"is_error\", \n",
    "        (col(\"payload\").isNull()) | \n",
    "        (col(\"payload.customerId\").isNull()) | \n",
    "        (size(col(\"payload.data.devices\")) == 0)\n",
    "    )\n",
    "    \n",
    "    # --- 3. Split the Stream ---\n",
    "    # VALID DATA\n",
    "    valid_df = checked_df.filter(\"is_error == false\").select(\n",
    "        col(\"payload.eventId\"),\n",
    "        col(\"payload.eventTime\"),\n",
    "        explode(col(\"payload.data.devices\")).alias(\"device\")\n",
    "    ).select(\n",
    "        \"eventId\", \"eventTime\", \"device.deviceId\", \"device.temperature\", \"device.status\"\n",
    "    )\n",
    "    \n",
    "    # ERROR DATA\n",
    "    error_df = checked_df.filter(\"is_error == true\").select(\n",
    "        col(\"json_string\").alias(\"raw_data\"),\n",
    "        current_timestamp().alias(\"error_time\"),\n",
    "        lit(batch_id).alias(\"batch_id\")\n",
    "    )\n",
    "    \n",
    "    # --- 4. Write to Sinks ---\n",
    "    try:\n",
    "        # Write Valid Data\n",
    "        if valid_df.count() > 0:\n",
    "            write_to_postgres(valid_df, \"device_data\")\n",
    "            print(f\"Written {valid_df.count()} valid records.\")\n",
    "            \n",
    "        # Write Error Data\n",
    "        if error_df.count() > 0:\n",
    "            write_to_postgres(error_df, \"device_data_error\")\n",
    "            print(f\"Captured {error_df.count()} bad records.\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"CRITICAL ERROR in Batch {batch_id}: {str(e)}\")\n",
    "        # In production, you might send an alert here (Slack/Email)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "359f7380",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Schema must be defined (copy from Module 10)\n",
    "# ... (Assume json_schema is defined) ...\n",
    "\n",
    "print(\"Starting Robust Stream...\")\n",
    "\n",
    "# Read from Kafka\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n",
    "    .option(\"subscribe\", \"device-data\") \\\n",
    "    .load()\n",
    "\n",
    "# Start\n",
    "query = kafka_df.writeStream \\\n",
    "    .foreachBatch(process_microbatch) \\\n",
    "    .option(\"checkpointLocation\", \"data/checkpoint_error_handling\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8eba031",
   "metadata": {},
   "source": [
    "## How to Test Failures\n",
    "\n",
    "1.  **Happy Path:** Send valid JSON via your Python generator.\n",
    "    *   Check `device_data` table in Postgres.\n",
    "2.  **Bad Data:** Use `kafka-console-producer` to send:\n",
    "    *   `\"This is just a string\"` (Malformed JSON)\n",
    "    *   `{\"eventId\": \"e1\", \"customerId\": null}` (Missing Key)\n",
    "    *   `{\"eventId\": \"e1\", \"data\": {\"devices\": []}}` (Empty Array)\n",
    "3.  **Verify:** Check `device_data_error` table in Postgres. You should see these raw strings logged with a timestamp."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
