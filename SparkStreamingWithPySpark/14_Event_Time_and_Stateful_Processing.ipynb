{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7181fcf6",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 15: Event Time, Processing Time & Stateful Processing\n",
    "\n",
    "In streaming, \"Time\" is not a single dimension. Data generated at 12:00 might arrive at 12:10 due to network lag. How do we handle this?\n",
    "\n",
    "### Objectives:\n",
    "1.  **Event Time vs. Processing Time:** Understanding the difference.\n",
    "2.  **The Late Data Problem:** What happens when data arrives out of order?\n",
    "3.  **Stateful Processing:** How Spark calculates aggregations (like averages) over time windows.\n",
    "4.  **The Memory Challenge:** Why we cannot keep state forever."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af91b89",
   "metadata": {},
   "source": [
    "## 1. Event Time vs. Processing Time\n",
    "\n",
    "| Type | Definition | Example |\n",
    "| :--- | :--- | :--- |\n",
    "| **Event Time** | The time the event actually occurred at the source. This is embedded in the data itself. | A sensor records temperature at **12:04 PM**. |\n",
    "| **Processing Time** | The time the system (Spark) receives and processes the data. | Spark reads that sensor record at **12:15 PM**. |\n",
    "\n",
    "### The Conflict\n",
    "If we calculate the \"Average Temperature for 12:00 - 12:10\", should we include the record that arrived at 12:15?\n",
    "*   **Yes**, because the *Event Time* (12:04) falls in that window.\n",
    "*   **Problem:** Spark has to remember (maintain state) that the 12:00 window is still \"open\" to accept late data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00843e99",
   "metadata": {},
   "source": [
    "## 2. The Late Data Scenario\n",
    "\n",
    "Imagine two devices sending data to a server in Bangalore:\n",
    "1.  **Device D1 (Delhi):** Fast network. Data generated at 12:04, Arrives at 12:04.\n",
    "2.  **Device D2 (Sydney):** Slow network. Data generated at 12:04, Arrives at 12:14.\n",
    "\n",
    "If Spark processes data in **10-minute windows** based on **Processing Time**:\n",
    "*   **Window 1 (12:00-12:10):** Includes D1.\n",
    "*   **Window 2 (12:10-12:20):** Includes D2 (Incorrectly!).\n",
    "\n",
    "**Solution:** We must process based on **Event Time**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "465fe4e3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, avg\n",
    "\n",
    "# Initialize Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"EventTime_Window_Demo\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# 1. Create dummy streaming data (Representing Flattened Device Data)\n",
    "# We assume schema: [eventTime, deviceId, temperature]\n",
    "# In a real scenario, this comes from Kafka.\n",
    "device_data_df = spark.readStream \\\n",
    "    .format(\"rate\") \\\n",
    "    .option(\"rowsPerSecond\", 1) \\\n",
    "    .load() \\\n",
    "    .selectExpr(\n",
    "        \"timestamp as eventTime\", \n",
    "        \"'device_1' as deviceId\", \n",
    "        \"cast(value as int) as temperature\" \n",
    "    )\n",
    "\n",
    "# 2. Define Windowed Aggregation\n",
    "# We group by a 10-minute window based on 'eventTime'\n",
    "# Logic: \"Calculate average temperature for every 10 minutes per device\"\n",
    "\n",
    "windowed_counts = device_data_df \\\n",
    "    .groupBy(\n",
    "        window(col(\"eventTime\"), \"10 minutes\"), # The Window\n",
    "        col(\"deviceId\")\n",
    "    ) \\\n",
    "    .agg(avg(\"temperature\").alias(\"avg_temp\"))\n",
    "\n",
    "# 3. Print the Schema to understand the structure\n",
    "windowed_counts.printSchema()\n",
    "\n",
    "# Note: The 'window' column is a struct containing {start, end}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4078d4",
   "metadata": {},
   "source": [
    "## 3. Stateful Processing & The Problem\n",
    "\n",
    "When we run the code above:\n",
    "1.  Spark creates a \"Bucket\" (State) in memory for the window `12:00 - 12:10`.\n",
    "2.  As data arrives, it updates the average in that bucket.\n",
    "\n",
    "### The Critical Question\n",
    "**When does Spark drop the bucket?**\n",
    "If data can arrive late, Spark theoretically has to keep the `12:00 - 12:10` bucket open **forever** just in case a record from 12:04 arrives 5 years later.\n",
    "\n",
    "**The Consequence:**\n",
    "*   Infinite State accumulation.\n",
    "*   Memory Overflow (OOM Error).\n",
    "*   System Crash.\n",
    "\n",
    "### The Solution: Watermarking\n",
    "To solve this, we need a mechanism to tell Spark: *\"Hey, if data is older than 30 minutes, just ignore it and drop the old state.\"*\n",
    "\n",
    "This mechanism is called **Watermarking**, which we will implement in the next module."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
