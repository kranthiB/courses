{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db13237a",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 17: Window Operations & Late Data Handling\n",
    "\n",
    "In this module, we put theory into practice. We will process streaming data using:\n",
    "1.  **Tumbling Windows:** Non-overlapping aggregation (e.g., every 10 mins).\n",
    "2.  **Sliding Windows:** Overlapping aggregation (e.g., every 10 mins, sliding every 5 mins).\n",
    "3.  **Watermarking:** Handling late data by defining a cutoff threshold.\n",
    "\n",
    "### The Scenario\n",
    "We receive a stream of words (Animals/Birds) with an `event_time`. We want to count the occurrences of each word within specific time windows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45dec360",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, window, count, from_json, explode, expr\n",
    "from pyspark.sql.types import StructType, StructField, StringType, ArrayType\n",
    "\n",
    "kafka_jar_package = \"org.apache.spark:spark-sql-kafka-0-10_2.12:3.3.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Window_Operations_Lab\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"4\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "# --- Schema for Input Data ---\n",
    "# {\"event_time\": \"timestamp\", \"data\": \"owl dog cat\"}\n",
    "json_schema = StructType([\n",
    "    StructField(\"event_time\", StringType(), True),\n",
    "    StructField(\"data\", StringType(), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "021e5f82",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Read from Kafka (Topic: wildlife)\n",
    "kafka_df = spark.readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:29092\") \\\n",
    "    .option(\"subscribe\", \"wildlife\") \\\n",
    "    .option(\"startingOffsets\", \"latest\") \\\n",
    "    .load()\n",
    "\n",
    "# Parse JSON & Explode Words\n",
    "# Note: We must cast event_time string to TimestampType for windowing!\n",
    "words_df = kafka_df.select(col(\"value\").cast(\"string\").alias(\"json_string\")) \\\n",
    "    .select(from_json(col(\"json_string\"), json_schema).alias(\"payload\")) \\\n",
    "    .select(\n",
    "        col(\"payload.event_time\").cast(\"timestamp\").alias(\"eventTime\"),\n",
    "        explode(expr(\"split(payload.data, ' ')\")).alias(\"word\")\n",
    "    )\n",
    "\n",
    "# Check Schema (Important: eventTime must be Timestamp)\n",
    "words_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f240d319",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Scenario: Count words every 10 minutes based on event time.\n",
    "# Watermark: 10 minutes (Allow data up to 10 mins late)\n",
    "\n",
    "windowed_counts = words_df \\\n",
    "    .withWatermark(\"eventTime\", \"10 minutes\") \\\n",
    "    .groupBy(\n",
    "        window(col(\"eventTime\"), \"10 minutes\"),\n",
    "        col(\"word\")\n",
    "    ) \\\n",
    "    .count()\n",
    "\n",
    "# Flatten output for readable console print\n",
    "final_df = windowed_counts.select(\n",
    "    col(\"window.start\").alias(\"start_time\"),\n",
    "    col(\"window.end\").alias(\"end_time\"),\n",
    "    col(\"word\"),\n",
    "    col(\"count\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6fe88f5",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We use Update mode to see changes to windows as late data arrives.\n",
    "query = final_df.writeStream \\\n",
    "    .format(\"console\") \\\n",
    "    .outputMode(\"update\") \\\n",
    "    .option(\"truncate\", \"false\") \\\n",
    "    .trigger(processingTime=\"10 seconds\") \\\n",
    "    .start()\n",
    "\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50453ddf",
   "metadata": {},
   "source": [
    "## How to Test Late Data\n",
    "\n",
    "1.  **Start Producer:** Use `kafka-console-producer` for topic `wildlife`.\n",
    "2.  **Send On-Time Data:**\n",
    "    `{\"event_time\": \"2024-01-01 12:05:00\", \"data\": \"owl\"}`\n",
    "    *   *Result:* Window 12:00-12:10 -> owl: 1\n",
    "3.  **Send Another On-Time (Same Window):**\n",
    "    `{\"event_time\": \"2024-01-01 12:08:00\", \"data\": \"dog\"}`\n",
    "    *   *Result:* Window 12:00-12:10 -> owl: 1, dog: 1\n",
    "4.  **Send Late Data (Within Watermark):**\n",
    "    `{\"event_time\": \"2024-01-01 12:04:00\", \"data\": \"cat\"}` (Arriving at 12:15 real time)\n",
    "    *   *Result:* Window 12:00-12:10 updates! -> owl: 1, dog: 1, cat: 1\n",
    "5.  **Send Very Late Data (Outside Watermark):**\n",
    "    `{\"event_time\": \"2024-01-01 11:00:00\", \"data\": \"shark\"}`\n",
    "    *   *Result:* Ignored. No update."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
