{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2cfac555",
   "metadata": {},
   "source": [
    "# Spark Streaming with PySpark\n",
    "## Module 18: Azure Cosmos DB Integration\n",
    "\n",
    "In this module, we explore **NoSQL** databases and integrate **Azure Cosmos DB** with Apache Spark.\n",
    "\n",
    "### Learning Objectives:\n",
    "1.  **Understand NoSQL vs. SQL:** The difference between scaling out (horizontal) vs. scaling up (vertical) and schema flexibility.\n",
    "2.  **Azure Cosmos DB Architecture:**\n",
    "    *   **Database:** Logical container for data.\n",
    "    *   **Container:** Equivalent to a table, partitioned physically and logically.\n",
    "    *   **Items:** The actual data documents (e.g., JSON).\n",
    "    *   **Partition Key:** Crucial for distributing data across the cluster.\n",
    "3.  **Spark Connector:** Configuring `azure-cosmos-spark` to read/write data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acad3f7b",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, lit\n",
    "from pyspark.sql.types import StringType\n",
    "\n",
    "# Define the Cosmos DB Spark Connector package\n",
    "# Ensure this version matches your Spark/Scala version (Spark 3.3.x / Scala 2.12)\n",
    "cosmos_connector_package = \"com.azure.cosmos.spark:azure-cosmos-spark_3-3_2-12:4.15.0\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Azure_Cosmos_DB_Integration\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .config(\"spark.jars.packages\", cosmos_connector_package) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "print(\"Spark Session Created successfully with Cosmos DB Connector\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01b90052",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# --- Configuration Configuration ---\n",
    "# In a production environment, use spark-defaults.conf or a secret manager.\n",
    "# For this lab, we define them here (Replace with your actual credentials).\n",
    "\n",
    "# Endpoint: Found in Azure Portal -> Cosmos DB Account -> Keys\n",
    "cosmos_endpoint = \"https://<your-account-name>.documents.azure.com:443/\"\n",
    "\n",
    "# Key: Found in Azure Portal -> Cosmos DB Account -> Keys (Primary or Secondary Key)\n",
    "cosmos_master_key = \"<your-primary-key>\"\n",
    "\n",
    "database_name = \"self\"\n",
    "container_name = \"device-data\"\n",
    "\n",
    "# Create a dictionary for write configuration\n",
    "write_config = {\n",
    "    \"spark.cosmos.accountEndpoint\": cosmos_endpoint,\n",
    "    \"spark.cosmos.accountKey\": cosmos_master_key,\n",
    "    \"spark.cosmos.database\": database_name,\n",
    "    \"spark.cosmos.container\": container_name,\n",
    "    \"spark.cosmos.write.strategy\": \"ItemAppend\", # Strategies: ItemAppend, ItemOverwrite, ItemDelete, etc.\n",
    "    \"spark.cosmos.write.bulk.enabled\": \"true\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94105909",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# We will use a sample JSON file representing device data to write to Cosmos DB.\n",
    "# Make sure the file exists in your datasets folder.\n",
    "\n",
    "input_path = \"datasets/device_03.json\"\n",
    "\n",
    "# Read JSON data\n",
    "source_df = spark.read.json(input_path)\n",
    "\n",
    "# --- Important: Handling ID and Partition Key ---\n",
    "# Cosmos DB requires an 'id' field for uniqueness and a Partition Key for distribution.\n",
    "# Our container was created with Partition Key: /customerId\n",
    "# Our source data has 'eventId'. We will create the 'id' column from 'eventId'.\n",
    "\n",
    "df_to_write = source_df.withColumn(\"id\", col(\"eventId\"))\n",
    "\n",
    "print(\"Source Data Schema:\")\n",
    "df_to_write.printSchema()\n",
    "df_to_write.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1683e7cc",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Write the dataframe to Cosmos DB using the OLTP format\n",
    "try:\n",
    "    df_to_write.write \\\n",
    "        .format(\"cosmos.oltp\") \\\n",
    "        .options(**write_config) \\\n",
    "        .mode(\"append\") \\\n",
    "        .save()\n",
    "    print(\"Data successfully written to Cosmos DB!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error writing to Cosmos DB: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6811a89",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Configure read settings\n",
    "read_config = {\n",
    "    \"spark.cosmos.accountEndpoint\": cosmos_endpoint,\n",
    "    \"spark.cosmos.accountKey\": cosmos_master_key,\n",
    "    \"spark.cosmos.database\": database_name,\n",
    "    \"spark.cosmos.container\": container_name,\n",
    "    \"spark.cosmos.read.inferSchema.enabled\": \"true\" # Automatically detect data types\n",
    "}\n",
    "\n",
    "# Read from Cosmos DB\n",
    "cosmos_df = spark.read \\\n",
    "    .format(\"cosmos.oltp\") \\\n",
    "    .options(**read_config) \\\n",
    "    .load()\n",
    "\n",
    "print(\"Data read from Cosmos DB:\")\n",
    "cosmos_df.printSchema()\n",
    "cosmos_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e9a305a",
   "metadata": {},
   "source": [
    "## Security Note: Managing Secrets\n",
    "\n",
    "Hardcoding keys in your notebook (as done in **Cell 3**) is strictly for learning purposes. In a real-world scenario, you should decouple secrets from your code.\n",
    "\n",
    "**Method: Using `spark-defaults.conf`**\n",
    "1.  Navigate to your Spark installation's `conf` folder.\n",
    "2.  Open (or create) `spark-defaults.conf`.\n",
    "3.  Add your configurations there:\n",
    "    ```properties\n",
    "    spark.cosmos.accountEndpoint https://<your-account>.documents.azure.com:443/\n",
    "    spark.cosmos.accountKey <your-secret-key>\n",
    "    ```\n",
    "4.  Restart your Spark Session.\n",
    "5.  In your code, you can omit the endpoint and key from the `options` dictionary, as Spark will pick them up from the environment configuration."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
